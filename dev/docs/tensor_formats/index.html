<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensor Formats · Finch.jl</title><meta name="title" content="Tensor Formats · Finch.jl"/><meta property="og:title" content="Tensor Formats · Finch.jl"/><meta property="twitter:title" content="Tensor Formats · Finch.jl"/><meta name="description" content="Documentation for Finch.jl."/><meta property="og:description" content="Documentation for Finch.jl."/><meta property="twitter:description" content="Documentation for Finch.jl."/><meta property="og:url" content="https://finch-tensor.github.io/Finch.jl/docs/tensor_formats/"/><meta property="twitter:url" content="https://finch-tensor.github.io/Finch.jl/docs/tensor_formats/"/><link rel="canonical" href="https://finch-tensor.github.io/Finch.jl/docs/tensor_formats/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Finch.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Finch.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><span class="tocitem">Documentation</span><ul><li class="is-active"><a class="tocitem" href>Tensor Formats</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Custom-Storage-Tree-Level-Formats"><span>Custom Storage Tree Level Formats</span></a></li><li class="toplevel"><a class="tocitem" href="#Types-of-Level-Storage"><span>Types of Level Storage</span></a></li><li class="toplevel"><a class="tocitem" href="#Examples-of-Popular-Formats-in-Finch"><span>Examples of Popular Formats in Finch</span></a></li><li class="toplevel"><a class="tocitem" href="#Level-Constructors"><span>Level Constructors</span></a></li><li><a class="tocitem" href="#Core-Levels"><span>Core Levels</span></a></li><li><a class="tocitem" href="#Advanced-Levels"><span>Advanced Levels</span></a></li><li><a class="tocitem" href="#Legacy-Levels"><span>Legacy Levels</span></a></li></ul></li><li><a class="tocitem" href="../array_api/">High-Level Array API</a></li><li><a class="tocitem" href="../sparse_utils/">Sparse and Structured Utilities</a></li><li><a class="tocitem" href="../user-defined_functions/">User-Defined Functions</a></li><li><a class="tocitem" href="../fileio/">FileIO</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox"/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Advanced: Finch Language</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../language/calling_finch/">Calling Finch</a></li><li><a class="tocitem" href="../language/finch_language/">The Finch Language</a></li><li><a class="tocitem" href="../language/dimensionalization/">Dimensionalization</a></li><li><a class="tocitem" href="../language/index_sugar/">Index Sugar</a></li><li><a class="tocitem" href="../language/mask_sugar/">Mask Sugar</a></li><li><a class="tocitem" href="../language/iteration_protocols/">Iteration Protocols</a></li><li><a class="tocitem" href="../language/parallelization/">Parallelization</a></li><li><a class="tocitem" href="../language/interoperability/">Interoperability</a></li><li><a class="tocitem" href="../language/optimization_tips/">Optimization Tips</a></li><li><a class="tocitem" href="../language/benchmarking_tips/">Benchmarking Tips</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-7" type="checkbox"/><label class="tocitem" for="menuitem-3-7"><span class="docs-label">Developers: Internal Details</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../internals/virtualization/">Virtualization</a></li><li><a class="tocitem" href="../internals/tensor_interface/">Tensor Interface</a></li><li><a class="tocitem" href="../internals/compiler_interface/">Compiler Interfaces</a></li><li><a class="tocitem" href="../internals/finch_notation/">Finch Notation</a></li><li><a class="tocitem" href="../internals/finch_logic/">Finch Logic</a></li></ul></li></ul></li><li><a class="tocitem" href="../../CONTRIBUTING/">Community and Contributions</a></li><li><span class="tocitem">Appendices and Additional Resources</span><ul><li><a class="tocitem" href="../../appendices/directory_structure/">Directory Structure</a></li><li><a class="tocitem" href="../../appendices/directory_structure/">Code Listing</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Documentation</a></li><li class="is-active"><a href>Tensor Formats</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensor Formats</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/finch-tensor/Finch.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/finch-tensor/Finch.jl/blob/main/docs/src/docs/tensor_formats.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Constructing-Tensors"><a class="docs-heading-anchor" href="#Constructing-Tensors">Constructing Tensors</a><a id="Constructing-Tensors-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-Tensors" title="Permalink"></a></h1><p>You can build a finch tensor with the <code>Tensor</code> constructor. In general, the <code>Tensor</code> constructor mirrors Julia&#39;s <a href="https://docs.julialang.org/en/v1/base/arrays/#Core.Array"><code>Array</code></a> constructor, but with an additional prefixed argument which specifies the formatted storage for the tensor.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor" href="#Finch.Tensor"><code>Finch.Tensor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor{Lvl} &lt;: AbstractFiber{Lvl}</code></pre><p>The multidimensional array type used by <code>Finch</code>. <code>Tensor</code> is a thin wrapper around the hierarchical level storage of type <code>Lvl</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L4-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor-Tuple{Finch.AbstractLevel}" href="#Finch.Tensor-Tuple{Finch.AbstractLevel}"><code>Finch.Tensor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor(lvl)</code></pre><p>Construct a <code>Tensor</code> using the tensor level storage <code>lvl</code>. No initialization of storage is performed, it is assumed that position 1 of <code>lvl</code> corresponds to a valid tensor, and <code>lvl</code> will be wrapped as-is. Call a different constructor to initialize the storage.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L14-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}" href="#Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}"><code>Finch.Tensor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor(lvl, [undef], dims...)</code></pre><p>Construct a <code>Tensor</code> of size <code>dims</code>, and initialize to <code>undef</code>, potentially allocating memory.  Here <code>undef</code> is the <code>UndefInitializer</code> singleton type. <code>dims...</code> may be a variable number of dimensions or a tuple of dimensions, but it must correspond to the number of dimensions in <code>lvl</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L24-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor-Tuple{Finch.AbstractLevel, UndefInitializer}" href="#Finch.Tensor-Tuple{Finch.AbstractLevel, UndefInitializer}"><code>Finch.Tensor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor(lvl, arr)</code></pre><p>Construct a <code>Tensor</code> and initialize it to the contents of <code>arr</code>. To explicitly copy into a tensor, use @ref[<code>copyto!</code>]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L41-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor-Tuple{Finch.AbstractLevel, Any}" href="#Finch.Tensor-Tuple{Finch.AbstractLevel, Any}"><code>Finch.Tensor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor(lvl, arr)</code></pre><p>Construct a <code>Tensor</code> and initialize it to the contents of <code>arr</code>. To explicitly copy into a tensor, use @ref[<code>copyto!</code>]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L41-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Tensor-Tuple{Any}" href="#Finch.Tensor-Tuple{Any}"><code>Finch.Tensor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Tensor(arr, [init = zero(eltype(arr))])</code></pre><p>Copy an array-like object <code>arr</code> into a corresponding, similar <code>Tensor</code> datastructure. Uses <code>init</code> as an initial value. May reuse memory when possible. To explicitly copy into a tensor, use @ref[<code>copyto!</code>].</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; println(summary(Tensor(sparse([1 0; 0 1]))))
2×2 Tensor(Dense(SparseList(Element(0))))

julia&gt; println(summary(Tensor(ones(3, 2, 4))))
3×2×4 Tensor(Dense(Dense(Dense(Element(0.0)))))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/tensors.jl#L50-L66">source</a></section></article><p>A few predefined formats are available for use in the first argument to the <code>Tensor</code> constructor:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.DenseFormat" href="#Finch.DenseFormat"><code>Finch.DenseFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DenseFormat(N, z = 0.0, T = typeof(z))</code></pre><p>A dense format with a fill value of <code>z</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L163-L167">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.CSCFormat" href="#Finch.CSCFormat"><code>Finch.CSCFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CSCFormat(z = 0.0, T = typeof(z))</code></pre><p>A CSC format with a fill value of <code>z</code>. CSC stores a sparse matrix as a dense array of lists.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L191-L196">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.CSFFormat" href="#Finch.CSFFormat"><code>Finch.CSFFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CSFFormat(N, z = 0.0, T = typeof(z))</code></pre><p>An <code>N</code>-dimensional CSC format with a fill value of <code>z</code>. CSF supports random access in the rightmost index, and uses a tree structure to store the rest of the data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L176-L182">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.DCSCFormat" href="#Finch.DCSCFormat"><code>Finch.DCSCFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DCSCFormat(z = 0.0, T = typeof(z))</code></pre><p>A DCSC format with a fill value of <code>z</code>. DCSC stores a sparse matrix as a list of lists.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L213-L218">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.DCSFFormat" href="#Finch.DCSFFormat"><code>Finch.DCSFFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DCSFFormat(z = 0.0, T = typeof(z))</code></pre><p>A DCSF format with a fill value of <code>z</code>. DCSF stores a sparse tensor as a list of lists of lists.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L199-L204">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.COOFormat" href="#Finch.COOFormat"><code>Finch.COOFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">COOFormat(N, z = 0.0, T = typeof(z))</code></pre><p>An <code>N</code>-dimensional COO format with a fill value of <code>z</code>. COO stores a sparse tensor as a list of coordinates.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L247-L252">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.HashFormat" href="#Finch.HashFormat"><code>Finch.HashFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HashFormat(N, z = 0.0, T = typeof(z))</code></pre><p>A hash-table based format with a fill value of <code>z</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L221-L225">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.ByteMapFormat" href="#Finch.ByteMapFormat"><code>Finch.ByteMapFormat</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ByteMapFormat(N, z = 0.0, T = typeof(z))</code></pre><p>A byte-map based format with a fill value of <code>z</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Finch.jl#L234-L238">source</a></section></article><p>For example, to construct an empty sparse matrix:</p><pre><code class="language-julia-repl hljs">julia&gt; A_fbr = Tensor(Dense(SparseList(Element(0.0))), 4, 3)
4×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  0.0  0.0
 0.0  0.0  0.0
 0.0  0.0  0.0
 0.0  0.0  0.0

julia&gt; tensor_tree(A_fbr)
4×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseList (0.0) [1:4]
   ├─ [:, 2]: SparseList (0.0) [1:4]
   └─ [:, 3]: SparseList (0.0) [1:4]</code></pre><p>To initialize a sparse matrix with some values:</p><pre><code class="language-julia-repl hljs">julia&gt; A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]
4×3 Matrix{Float64}:
 0.0  0.0  4.4
 1.1  0.0  0.0
 2.2  0.0  5.5
 3.3  0.0  0.0

julia&gt; A_fbr = Tensor(Dense(SparseList(Element(0.0))), A)
4×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  0.0  4.4
 1.1  0.0  0.0
 2.2  0.0  5.5
 3.3  0.0  0.0

julia&gt; tensor_tree(A_fbr)
4×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseList (0.0) [1:4]
   │  ├─ [2]: 1.1
   │  ├─ [3]: 2.2
   │  └─ [4]: 3.3
   ├─ [:, 2]: SparseList (0.0) [1:4]
   └─ [:, 3]: SparseList (0.0) [1:4]
      ├─ [1]: 4.4
      └─ [3]: 5.5</code></pre><h1 id="Custom-Storage-Tree-Level-Formats"><a class="docs-heading-anchor" href="#Custom-Storage-Tree-Level-Formats">Custom Storage Tree Level Formats</a><a id="Custom-Storage-Tree-Level-Formats-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Storage-Tree-Level-Formats" title="Permalink"></a></h1><p>This section describes the formatted storage for Finch tensors, the first argument to the <a href="#Finch.Tensor"><code>Tensor</code></a> constructor. Level storage types holds all of the tensor data, and can be nested hierarchichally.</p><p>Finch represents tensors hierarchically in a tree, where each node in the tree is a vector of subtensors and the leaves are the elements.  Thus, a matrix is analogous to a vector of vectors, and a 3-tensor is analogous to a vector of vectors of vectors.  The vectors at each level of the tensor all have the same structure, which can be selected by the user. You can visualize the tree using the <a href="../../appendices/listing/#Finch.tensor_tree-Tuple{Finch.AbstractTensor}-appendices-listing"><code>tensor_tree</code></a> function.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.tensor_tree-Tuple{Finch.AbstractTensor}" href="#Finch.tensor_tree-Tuple{Finch.AbstractTensor}"><code>Finch.tensor_tree</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">tensor_tree(tns; nmax = 2)</code></pre><p>Print a tree representation of the tensor <code>tns</code> to the standard output. <code>nmax</code> is half the maximum number of children to show before truncating.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/abstract_tensor.jl#L171-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.tensor_tree-Tuple{IO, Finch.AbstractTensor}" href="#Finch.tensor_tree-Tuple{IO, Finch.AbstractTensor}"><code>Finch.tensor_tree</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">tensor_tree(io::IO, tns; nmax = 2)</code></pre><p>Print a tree representation of the tensor <code>tns</code> to <code>io</code>. <code>nmax</code> is half the maximum number of children to show before truncating.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/abstract_tensor.jl#L179-L184">source</a></section></article><p>In a Finch tensor tree, the child of each node is selected by an array index. All of the children at the same level will use the same format and share the same storage. Finch is column major, so in an expression <code>A[i_1, ..., i_N]</code>, the rightmost dimension <code>i_N</code> corresponds to the root level of the tree, and the leftmost dimension <code>i_1</code> corresponds to the leaf level.</p><p>Our example could be visualized as follows:</p><p><img src="../../assets/LevelsVsFibers-matrix.png" alt="CSC Format Index Tree"/></p><h1 id="Types-of-Level-Storage"><a class="docs-heading-anchor" href="#Types-of-Level-Storage">Types of Level Storage</a><a id="Types-of-Level-Storage-1"></a><a class="docs-heading-anchor-permalink" href="#Types-of-Level-Storage" title="Permalink"></a></h1><p>Finch supports a variety of storage formats for each level of the tensor tree, each with advantages and disadvantages. Some storage formats support in-order access, while others support random access. Some storage formats must be written to in column-major order, while others support out-of-order writes. The capabilities of each level are summarized in the following tables along with some general descriptions.</p><table><tr><th style="text-align: left">Level Format Name</th><th style="text-align: left">Group</th><th style="text-align: left">Data Characteristic</th><th style="text-align: center">Column-Major Reads</th><th style="text-align: center">Random Reads</th><th style="text-align: center">Column-Major Bulk Update</th><th style="text-align: center">Random Bulk Update</th><th style="text-align: center">Random Updates</th><th style="text-align: center">Status</th></tr><tr><td style="text-align: left">Dense</td><td style="text-align: left">Core</td><td style="text-align: left">Dense</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseTree</td><td style="text-align: left">Core</td><td style="text-align: left">Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">SparseRunListTree</td><td style="text-align: left">Core</td><td style="text-align: left">Sparse Runs</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">Element</td><td style="text-align: left">Core</td><td style="text-align: left">Leaf</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">Pattern</td><td style="text-align: left">Core</td><td style="text-align: left">Leaf</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseList</td><td style="text-align: left">Advanced</td><td style="text-align: left">Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseRunList</td><td style="text-align: left">Advanced</td><td style="text-align: left">Sparse Runs</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseBlockList</td><td style="text-align: left">Advanced</td><td style="text-align: left">Sparse Blocks</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparsePoint</td><td style="text-align: left">Advanced</td><td style="text-align: left">Single Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseInterval</td><td style="text-align: left">Advanced</td><td style="text-align: left">Single Sparse Run</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseBand</td><td style="text-align: left">Advanced</td><td style="text-align: left">Single Sparse Block</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">RunList</td><td style="text-align: left">Advanced</td><td style="text-align: left">Dense Runs</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">❌</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">SparseBytemap</td><td style="text-align: left">Advanced</td><td style="text-align: left">Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: left">SparseDict</td><td style="text-align: left">Advanced</td><td style="text-align: left">Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅️</td></tr><tr><td style="text-align: left">MutexLevel</td><td style="text-align: left">Modifier</td><td style="text-align: left">No Data</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">SeperationLevel</td><td style="text-align: left">Modifier</td><td style="text-align: left">No Data</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">⚙️</td></tr><tr><td style="text-align: left">SparseCOO</td><td style="text-align: left">Legacy</td><td style="text-align: left">Sparse</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">❌</td><td style="text-align: center">✅</td><td style="text-align: center">✅️</td></tr></table><p>The &quot;Level Format Name&quot; is the name of the level datatype. Other columns have descriptions below.</p><h3 id="Status"><a class="docs-heading-anchor" href="#Status">Status</a><a id="Status-1"></a><a class="docs-heading-anchor-permalink" href="#Status" title="Permalink"></a></h3><table><tr><th style="text-align: left">Symbol</th><th style="text-align: left">Meaning</th></tr><tr><td style="text-align: left">✅</td><td style="text-align: left">Indicates the level is ready for serious use.</td></tr><tr><td style="text-align: left">⚙️</td><td style="text-align: left">Indicates the level is experimental and under development.</td></tr><tr><td style="text-align: left">🕸️</td><td style="text-align: left">Indicates the level is deprecated, and may be removed in a future release.</td></tr></table><h3 id="Groups"><a class="docs-heading-anchor" href="#Groups">Groups</a><a id="Groups-1"></a><a class="docs-heading-anchor-permalink" href="#Groups" title="Permalink"></a></h3><h4 id="Core-Group"><a class="docs-heading-anchor" href="#Core-Group">Core Group</a><a id="Core-Group-1"></a><a class="docs-heading-anchor-permalink" href="#Core-Group" title="Permalink"></a></h4><p>Contains the basic, minimal set of levels one should use to build and manipulate tensors.  These levels can be efficiently read and written to in any order.</p><h4 id="Advanced-Group"><a class="docs-heading-anchor" href="#Advanced-Group">Advanced Group</a><a id="Advanced-Group-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Group" title="Permalink"></a></h4><p>Contains levels which are more specialized, and geared towards bulk updates. These levels may be more efficient in certain cases, but are also more restrictive about access orders and intended for more advanced usage.</p><h4 id="Modifier-Group"><a class="docs-heading-anchor" href="#Modifier-Group">Modifier Group</a><a id="Modifier-Group-1"></a><a class="docs-heading-anchor-permalink" href="#Modifier-Group" title="Permalink"></a></h4><p>Contains levels which are also more specialized, but not towards a sparsity pattern. These levels modify other levels in a variety of ways, but don&#39;t store novel sparsity patterns. Typically, they modify how levels are stored or attach data to levels to support the utilization of various hardware features.</p><h4 id="Legacy-Group"><a class="docs-heading-anchor" href="#Legacy-Group">Legacy Group</a><a id="Legacy-Group-1"></a><a class="docs-heading-anchor-permalink" href="#Legacy-Group" title="Permalink"></a></h4><p>Contains levels which are not recommended for new code, but are included for compatibility with older code.</p><h3 id="Data-Characteristics"><a class="docs-heading-anchor" href="#Data-Characteristics">Data Characteristics</a><a id="Data-Characteristics-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Characteristics" title="Permalink"></a></h3><table><tr><th style="text-align: left">Level Type</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><strong>Dense</strong></td><td style="text-align: left">Levels which store every subtensor.</td></tr><tr><td style="text-align: left"><strong>Leaf</strong></td><td style="text-align: left">Levels which store only scalars, used for the leaf level of the tree.</td></tr><tr><td style="text-align: left"><strong>Sparse</strong></td><td style="text-align: left">Levels which store only non-fill values, used for levels with few nonzeros.</td></tr><tr><td style="text-align: left"><strong>Sparse Runs</strong></td><td style="text-align: left">Levels which store runs of repeated non-fill values.</td></tr><tr><td style="text-align: left"><strong>Sparse Blocks</strong></td><td style="text-align: left">Levels which store Blocks of repeated non-fill values.</td></tr><tr><td style="text-align: left"><strong>Dense Runs</strong></td><td style="text-align: left">Levels which store runs of repeated values, and no compile-time zero annihilation.</td></tr><tr><td style="text-align: left"><strong>No Data</strong></td><td style="text-align: left">Levels which don&#39;t store data but which alter the storage pattern or attach additional meta-data.</td></tr></table><p>Note that the <code>Single</code> sparse levels store a single instance of each nonzero, run, or block. These are useful with a parent level to represent IDs.</p><h3 id="Access-Characteristics"><a class="docs-heading-anchor" href="#Access-Characteristics">Access Characteristics</a><a id="Access-Characteristics-1"></a><a class="docs-heading-anchor-permalink" href="#Access-Characteristics" title="Permalink"></a></h3><table><tr><th style="text-align: left">Operation Type</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><strong>Column-Major Reads</strong></td><td style="text-align: left">Indicates efficient reading of data in column-major order.</td></tr><tr><td style="text-align: left"><strong>Random Reads</strong></td><td style="text-align: left">Indicates efficient reading of data in random-access order.</td></tr><tr><td style="text-align: left"><strong>Column-Major Bulk Update</strong></td><td style="text-align: left">Indicates efficient writing of data in column-major order, the total time roughly linear to the size of the tensor.</td></tr><tr><td style="text-align: left"><strong>Column-Major Random Update</strong></td><td style="text-align: left">Indicates efficient writing of data in random-access order, the total time roughly linear to the size of the tensor.</td></tr><tr><td style="text-align: left"><strong>Random Update</strong></td><td style="text-align: left">Indicates efficient writing of data in random-access order, the total time roughly linear to the number of updates.</td></tr></table><h3 id="Diagrams"><a class="docs-heading-anchor" href="#Diagrams">Diagrams</a><a id="Diagrams-1"></a><a class="docs-heading-anchor-permalink" href="#Diagrams" title="Permalink"></a></h3><p>The following diagrams illustrate the structure of the levels individually.</p><p><img src="../../assets/Structures-levels.png" alt="Diagram of Core Level Structures"/></p><p>The following diagrams illustrate the way that levels can be combined to form a tensor tree.</p><p><img src="../../assets/Structures-structures.png" alt="Diagram of Core Level Structures"/></p><h1 id="Examples-of-Popular-Formats-in-Finch"><a class="docs-heading-anchor" href="#Examples-of-Popular-Formats-in-Finch">Examples of Popular Formats in Finch</a><a id="Examples-of-Popular-Formats-in-Finch-1"></a><a class="docs-heading-anchor-permalink" href="#Examples-of-Popular-Formats-in-Finch" title="Permalink"></a></h1><p>Finch levels can be used to construct a variety of popular sparse formats. A few examples follow:</p><table><tr><th style="text-align: left">Format Type</th><th style="text-align: left">Syntax</th></tr><tr><td style="text-align: left">Sparse Vector</td><td style="text-align: left"><code>Tensor(SparseList(Element(0.0)), args...)</code></td></tr><tr><td style="text-align: left">CSC Matrix</td><td style="text-align: left"><code>Tensor(Dense(SparseList(Element(0.0))), args...)</code></td></tr><tr><td style="text-align: left">CSF 3-Tensor</td><td style="text-align: left"><code>Tensor(Dense(SparseList(SparseList(Element(0.0)))), args...)</code></td></tr><tr><td style="text-align: left">DCSC (Hypersparse) Matrix</td><td style="text-align: left"><code>Tensor(SparseList(SparseList(Element(0.0))), args...)</code></td></tr><tr><td style="text-align: left">COO Matrix</td><td style="text-align: left"><code>Tensor(SparseCOO{2}(Element(0.0)), args...)</code></td></tr><tr><td style="text-align: left">COO 3-Tensor</td><td style="text-align: left"><code>Tensor(SparseCOO{3}(Element(0.0)), args...)</code></td></tr><tr><td style="text-align: left">Run-Length-Encoded Image</td><td style="text-align: left"><code>Tensor(Dense(RunList(Element(0.0))), args...)</code></td></tr></table><h1 id="Level-Constructors"><a class="docs-heading-anchor" href="#Level-Constructors">Level Constructors</a><a id="Level-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Level-Constructors" title="Permalink"></a></h1><h2 id="Core-Levels"><a class="docs-heading-anchor" href="#Core-Levels">Core Levels</a><a id="Core-Levels-1"></a><a class="docs-heading-anchor-permalink" href="#Core-Levels" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.DenseLevel" href="#Finch.DenseLevel"><code>Finch.DenseLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DenseLevel{[Ti=Int]}(lvl, [dim])</code></pre><p>A subfiber of a dense level is an array which stores every slice <code>A[:, ..., :, i]</code> as a distinct subfiber in <code>lvl</code>. Optionally, <code>dim</code> is the size of the last dimension. <code>Ti</code> is the type of the indices used to index the level.</p><pre><code class="language-julia-repl hljs">julia&gt; ndims(Tensor(Dense(Element(0.0))))
1

julia&gt; ndims(Tensor(Dense(Dense(Element(0.0)))))
2

julia&gt; tensor_tree(Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4]))
2×2-Tensor
└─ Dense [:,1:2]
   ├─ [:, 1]: Dense [1:2]
   │  ├─ [1]: 1.0
   │  └─ [2]: 3.0
   └─ [:, 2]: Dense [1:2]
      ├─ [1]: 2.0
      └─ [2]: 4.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/dense_levels.jl#L1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.ElementLevel" href="#Finch.ElementLevel"><code>Finch.ElementLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()</code></pre><p>A subfiber of an element level is a scalar of type <code>Tv</code>, initialized to <code>Vf</code>. <code>Vf</code> may optionally be given as the first argument.</p><p>The data is stored in a vector of type <code>Val</code> with <code>eltype(Val) = Tv</code>. The type <code>Tp</code> is the index type used to access Val.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(Element(0.0)), [1, 2, 3]))
3-Tensor
└─ Dense [1:3]
   ├─ [1]: 1.0
   ├─ [2]: 2.0
   └─ [3]: 3.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/element_levels.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.PatternLevel" href="#Finch.PatternLevel"><code>Finch.PatternLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PatternLevel{[Tp=Int]}()</code></pre><p>A subfiber of a pattern level is the Boolean value true, but it&#39;s <code>fill_value</code> is false. PatternLevels are used to create tensors that represent which values are stored by other fibers. See <a href="../sparse_utils/#Finch.pattern!"><code>pattern!</code></a> for usage examples.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(Pattern()), 3))
3-Tensor
└─ Dense [1:3]
   ├─ [1]: true
   ├─ [2]: true
   └─ [3]: true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/pattern_levels.jl#L1-L16">source</a></section></article><h2 id="Advanced-Levels"><a class="docs-heading-anchor" href="#Advanced-Levels">Advanced Levels</a><a id="Advanced-Levels-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Levels" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseListLevel" href="#Finch.SparseListLevel"><code>Finch.SparseListLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseListLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])</code></pre><p>A subfiber of a sparse level does not need to represent slices <code>A[:, ..., :, i]</code> which are entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. Instead, only potentially non-fill slices are stored as subfibers in <code>lvl</code>.  A sorted list is used to record which slices are stored. Optionally, <code>dim</code> is the size of the last dimension.</p><p><code>Ti</code> is the type of the last tensor index, and <code>Tp</code> is the type used for positions in the level. The types <code>Ptr</code> and <code>Idx</code> are the types of the arrays used to store positions and indicies.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseList (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   ├─ [:, 2]: SparseList (0.0) [1:3]
   └─ [:, 3]: SparseList (0.0) [1:3]
      ├─ [1]: 20.0
      └─ [3]: 40.0

julia&gt; tensor_tree(Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ SparseList (0.0) [:,1:3]
   ├─ [:, 1]: SparseList (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   └─ [:, 3]: SparseList (0.0) [1:3]
      ├─ [1]: 20.0
      └─ [3]: 40.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_list_levels.jl#L1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.RunListLevel" href="#Finch.RunListLevel"><code>Finch.RunListLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RunListLevel{[Ti=Int], [Ptr, Right]}(lvl, [dim], [merge = true])</code></pre><p>The RunListLevel represent runs of equivalent slices <code>A[:, ..., :, i]</code>. A sorted list is used to record the right endpoint of each run. Optionally, <code>dim</code> is the size of the last dimension.</p><p><code>Ti</code> is the type of the last tensor index, and <code>Tp</code> is the type used for positions in the level. The types <code>Ptr</code> and <code>Right</code> are the types of the arrays used to store positions and endpoints.</p><p>The <code>merge</code> keyword argument is used to specify whether the level should merge duplicate consecutive runs.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(RunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: RunList (0.0) [1:3]
   │  ├─ [1:1]: 10.0
   │  ├─ [2:2]: 30.0
   │  └─ [3:3]: 0.0
   ├─ [:, 2]: RunList (0.0) [1:3]
   │  └─ [1:3]: 0.0
   └─ [:, 3]: RunList (0.0) [1:3]
      ├─ [1:1]: 20.0
      ├─ [2:2]: 0.0
      └─ [3:3]: 40.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/dense_rle_levels.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseRunListLevel" href="#Finch.SparseRunListLevel"><code>Finch.SparseRunListLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseRunListLevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim]; [merge = true])</code></pre><p>The SparseRunListLevel represent runs of equivalent slices <code>A[:, ..., :, i]</code> which are not entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. A sorted list is used to record the left and right endpoints of each run. Optionally, <code>dim</code> is the size of the last dimension.</p><p><code>Ti</code> is the type of the last tensor index, and <code>Tp</code> is the type used for positions in the level. The types <code>Ptr</code>, <code>Left</code>, and <code>Right</code> are the types of the arrays used to store positions and endpoints.</p><p>The <code>merge</code> keyword argument is used to specify whether the level should merge duplicate consecutive runs.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparseRunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseRunList (0.0) [1:3]
   │  ├─ [1:1]: 10.0
   │  └─ [2:2]: 30.0
   ├─ [:, 2]: SparseRunList (0.0) [1:3]
   └─ [:, 3]: SparseRunList (0.0) [1:3]
      ├─ [1:1]: 20.0
      └─ [3:3]: 40.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_rle_levels.jl#L1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseBlockListLevel" href="#Finch.SparseBlockListLevel"><code>Finch.SparseBlockListLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>SparseBlockListLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])</p><p>Like the <a href="#Finch.SparseListLevel"><code>SparseListLevel</code></a>, but contiguous subfibers are stored together in blocks.</p><p>```jldoctest julia&gt; Tensor(Dense(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0</p><p>julia&gt; Tensor(SparseBlockList(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) SparseList (0.0) [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_vbl_levels.jl#L1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseBandLevel" href="#Finch.SparseBandLevel"><code>Finch.SparseBandLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>SparseBandLevel{[Ti=Int], [Idx, Ofs]}(lvl, [dim])</p><p>Like the <a href="#Finch.SparseBlockListLevel"><code>SparseBlockListLevel</code></a>, but stores only a single block, and fills in zeros.</p><p>```jldoctest julia&gt; Tensor(Dense(SparseBand(Element(0.0))), [10 0 20; 30 40 0; 0 0 50]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_band_levels.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparsePointLevel" href="#Finch.SparsePointLevel"><code>Finch.SparsePointLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparsePointLevel{[Ti=Int], [Idx]}(lvl, [dim])</code></pre><p>A subfiber of a SparsePoint level does not need to represent slices <code>A[:, ..., :, i]</code> which are entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. Instead, only potentially non-fill slices are stored as subfibers in <code>lvl</code>. A main difference compared to SparseList level is that SparsePoint level only stores a &#39;single&#39; non-fill slice. It emits an error if the program tries to write multiple (&gt;=2) coordinates into SparsePoint.</p><p><code>Ti</code> is the type of the last tensor index. The types <code>Ptr</code> and <code>Idx</code> are the types of the arrays used to store positions and indicies.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparsePoint(Element(0.0))), [10 0 0; 0 20 0; 0 0 30]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparsePoint (0.0) [1:3]
   │  └─ [1]: 10.0
   ├─ [:, 2]: SparsePoint (0.0) [1:3]
   │  └─ [2]: 20.0
   └─ [:, 3]: SparsePoint (0.0) [1:3]
      └─ [3]: 30.0

julia&gt; tensor_tree(Tensor(SparsePoint(Dense(Element(0.0))), [0 0 0; 0 0 30; 0 0 30]))
3×3-Tensor
└─ SparsePoint (0.0) [:,1:3]
   └─ [:, 3]: Dense [1:3]
      ├─ [1]: 0.0
      ├─ [2]: 30.0
      └─ [3]: 30.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_point_levels.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseIntervalLevel" href="#Finch.SparseIntervalLevel"><code>Finch.SparseIntervalLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseIntervalLevel{[Ti=Int], [Left, Right]}(lvl, [dim])</code></pre><p>The SparseIntervalLevel represent runs of equivalent slices <code>A[:, ..., :, i]</code> which are not entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. A main difference compared to SparseRunList level is that SparseInterval level only stores a &#39;single&#39; non-fill run. It emits an error if the program tries to write multiple (&gt;=2) runs into SparseInterval.</p><p><code>Ti</code> is the type of the last tensor index. The types <code>Left</code>, and &#39;Right&#39; are the types of the arrays used to store positions and endpoints.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(SparseInterval(Element(0)), [0, 10, 0]))
3-Tensor
└─ SparseInterval (0) [1:3]
   └─ [2:2]: 10

julia&gt; x = Tensor(SparseInterval(Element(0)), 10);

julia&gt; @finch begin for i = extent(3,6); x[~i] = 1 end end;

julia&gt; tensor_tree(x)
10-Tensor
└─ SparseInterval (0) [1:10]
   └─ [3:6]: 1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_interval_levels.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseByteMapLevel" href="#Finch.SparseByteMapLevel"><code>Finch.SparseByteMapLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseByteMapLevel{[Ti=Int], [Ptr, Tbl]}(lvl, [dims])</code></pre><p>Like the <a href="#Finch.SparseListLevel"><code>SparseListLevel</code></a>, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.</p><p><code>Ti</code> is the type of the last tensor index, and <code>Tp</code> is the type used for positions in the level.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseByteMap (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   ├─ [:, 2]: SparseByteMap (0.0) [1:3]
   └─ [:, 3]: SparseByteMap (0.0) [1:3]
      ├─ [1]: 0.0
      └─ [3]: 0.0

julia&gt; tensor_tree(Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ SparseByteMap (0.0) [:,1:3]
   ├─ [:, 1]: SparseByteMap (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   └─ [:, 3]: SparseByteMap (0.0) [1:3]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_bytemap_levels.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseDictLevel" href="#Finch.SparseDictLevel"><code>Finch.SparseDictLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseDictLevel{[Ti=Int], [Tp=Int], [Ptr, Idx, Val, Tbl, Pool=Dict]}(lvl, [dim])</code></pre><p>A subfiber of a sparse level does not need to represent slices <code>A[:, ..., :, i]</code> which are entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. Instead, only potentially non-fill slices are stored as subfibers in <code>lvl</code>.  A datastructure specified by Tbl is used to record which slices are stored. Optionally, <code>dim</code> is the size of the last dimension.</p><p><code>Ti</code> is the type of the last fiber index, and <code>Tp</code> is the type used for positions in the level. The types <code>Ptr</code> and <code>Idx</code> are the types of the arrays used to store positions and indicies.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseDict (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   ├─ [:, 2]: SparseDict (0.0) [1:3]
   └─ [:, 3]: SparseDict (0.0) [1:3]
      ├─ [1]: 20.0
      └─ [3]: 40.0

julia&gt; tensor_tree(Tensor(SparseDict(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ SparseDict (0.0) [:,1:3]
   ├─ [:, 1]: SparseDict (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   └─ [:, 3]: SparseDict (0.0) [1:3]
      ├─ [1]: 20.0
      └─ [3]: 40.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_dict_levels.jl#L1-L36">source</a></section></article><h2 id="Legacy-Levels"><a class="docs-heading-anchor" href="#Legacy-Levels">Legacy Levels</a><a id="Legacy-Levels-1"></a><a class="docs-heading-anchor-permalink" href="#Legacy-Levels" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.SparseCOOLevel" href="#Finch.SparseCOOLevel"><code>Finch.SparseCOOLevel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr, Tbl]}(lvl, [dims])</code></pre><p>A subfiber of a sparse level does not need to represent slices which are entirely <a href="../sparse_utils/#Finch.fill_value"><code>fill_value</code></a>. Instead, only potentially non-fill slices are stored as subfibers in <code>lvl</code>. The sparse coo level corresponds to <code>N</code> indices in the subfiber, so fibers in the sublevel are the slices <code>A[:, ..., :, i_1, ..., i_n]</code>.  A set of <code>N</code> lists (one for each index) are used to record which slices are stored. The coordinates (sets of <code>N</code> indices) are sorted in column major order.  Optionally, <code>dims</code> are the sizes of the last dimensions.</p><p><code>TI</code> is the type of the last <code>N</code> tensor indices, and <code>Tp</code> is the type used for positions in the level.</p><p>The type <code>Tbl</code> is an NTuple type where each entry k is a subtype <code>AbstractVector{TI[k]}</code>.</p><p>The type <code>Ptr</code> is the type for the pointer array.</p><pre><code class="language-julia-repl hljs">julia&gt; tensor_tree(Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ Dense [:,1:3]
   ├─ [:, 1]: SparseCOO{1} (0.0) [1:3]
   │  ├─ [1]: 10.0
   │  └─ [2]: 30.0
   ├─ [:, 2]: SparseCOO{1} (0.0) [1:3]
   └─ [:, 3]: SparseCOO{1} (0.0) [1:3]
      ├─ [1]: 20.0
      └─ [3]: 40.0

julia&gt; tensor_tree(Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40]))
3×3-Tensor
└─ SparseCOO{2} (0.0) [:,1:3]
   ├─ [1, 1]: 10.0
   ├─ [2, 1]: 30.0
   ├─ [1, 3]: 20.0
   └─ [3, 3]: 40.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/tensors/levels/sparse_coo_levels.jl#L1-L39">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../array_api/">High-Level Array API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.10.1 on <span class="colophon-date" title="Friday 18 April 2025 14:32">Friday 18 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
