var documenterSearchIndex = {"docs":
[{"location":"docs/language/interoperability/#Using-Finch-with-Other-Languages","page":"Interoperability","title":"Using Finch with Other Languages","text":"","category":"section"},{"location":"docs/language/interoperability/#Python","page":"Interoperability","title":"Python","text":"","category":"section"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"Finch has a dedicated Python frontend which can be installed with pip install finch-tensor. The frontend is Array-API compliant. The code for the python wrapper is available at the finch-tensor-python repo. Finch is also available as a backend to pydata/sparse.","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"You can also use juliacall to access more advanced Finch features from Python.","category":"page"},{"location":"docs/language/interoperability/#Other-languages","page":"Interoperability","title":"Other languages","text":"","category":"section"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"You can use Finch in other languages using Julia interfaces such as julia.h, making considerations for converting between 0-indexed and 1-indexed arrays.","category":"page"},{"location":"docs/language/interoperability/#0-Index-Compatibility","page":"Interoperability","title":"0-Index Compatibility","text":"","category":"section"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"Julia, Matlab, Fortran, etc. index arrays starting at 1. C, python, etc. index starting at 0. In a dense array, we can simply subtract one from the index, and in fact, this is what Julia will does under the hood when you pass a vector between C to Julia.","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"However, for sparse array formats, it's not just a matter of subtracting one from the index, as the internal lists of indices, positions, etc all start from zero as well. To remedy the situation, Finch leverages PlusOneVector and CIndex.","category":"page"},{"location":"docs/language/interoperability/#PlusOneVector","page":"Interoperability","title":"PlusOneVector","text":"","category":"section"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"PlusOneVector is a view that adds 1 on access to an underlying 0-Index vector. This allows to use Python/NumPy vector, without copying, as a mutable index array.","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"julia> v = Vector([1, 0, 2, 3])\n4-element Vector{Int64}:\n 1\n 0\n 2\n 3\n\njulia> obov = PlusOneVector(v)\n4-element PlusOneVector{Int64, Vector{Int64}}:\n 2\n 1\n 3\n 4\n\njulia> obov[1] += 8\n10\n\njulia> obov\n4-element PlusOneVector{Int64, Vector{Int64}}:\n 10\n  1\n  3\n  4\n\njulia> obov.data\n4-element Vector{Int64}:\n 9\n 0\n 2\n 3","category":"page"},{"location":"docs/language/interoperability/#CIndex","page":"Interoperability","title":"CIndex","text":"","category":"section"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"warning: Warning\nCIndex is no longer recommended - use PlusOneVector instead.","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"Finch also interoperates with the CIndices package, which exports a type called CIndex. The internal representation of CIndex is one less than the value it represents, and we can use CIndex as the index or position type of a Finch array to represent arrays in other languages.","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"For example, if idx_c, ptr_c, and val_c are the internal arrays of a CSC matrix in a zero-indexed language, we can represent that matrix as a one-indexed Finch array without copying by calling","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"DocTestSetup = quote\n    using Finch\n    using CIndices\nend","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"julia> m = 4;\n       n = 3;\n       ptr_c = [0, 3, 3, 5];\n       idx_c = [1, 2, 3, 0, 2];\n       val_c = [1.1, 2.2, 3.3, 4.4, 5.5];\n\njulia> ptr_jl = reinterpret(CIndex{Int}, ptr_c)\n4-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 1\n 4\n 4\n 6\n\njulia> idx_jl = reinterpret(CIndex{Int}, idx_c)\n5-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 2\n 3\n 4\n 1\n 3\n\njulia> A = Tensor(\n           Dense(\n               SparseList{CIndex{Int}}(Element{0.0,Float64,CIndex{Int}}(val_c), m, ptr_jl, idx_jl),\n               n,\n           ),\n       )\nCIndex{Int64}(4)×3 Tensor{DenseLevel{Int64, SparseListLevel{CIndex{Int64}, Base.ReinterpretArray{CIndex{Int64}, 1, Int64, Vector{Int64}, false}, Base.ReinterpretArray{CIndex{Int64}, 1, Int64, Vector{Int64}, false}, ElementLevel{0.0, Float64, CIndex{Int64}, Vector{Float64}}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A)\nCIndex{Int64}(4)×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:CIndex{Int64}(4)]\n   │  ├─ [CIndex{Int64}(2)]: 1.1\n   │  ├─ [CIndex{Int64}(3)]: 2.2\n   │  └─ [CIndex{Int64}(4)]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:CIndex{Int64}(4)]\n   └─ [:, 3]: SparseList (0.0) [1:CIndex{Int64}(4)]\n      ├─ [CIndex{Int64}(1)]: 4.4\n      └─ [CIndex{Int64}(3)]: 5.5","category":"page"},{"location":"docs/language/interoperability/","page":"Interoperability","title":"Interoperability","text":"We can also convert between representations by copying to or from CIndex fibers.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"CurrentModule = Finch","category":"page"},{"location":"docs/language/optimization_tips/#Optimization-Tips-for-Finch","page":"Optimization Tips","title":"Optimization Tips for Finch","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"It's easy to ask Finch to run the same operation in different ways. However, different approaches have different performance. The right approach really depends on your particular situation. Here's a collection of general approaches that help Finch generate faster code in most cases.","category":"page"},{"location":"docs/language/optimization_tips/#Concordant-Iteration","page":"Optimization Tips","title":"Concordant Iteration","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"By default, Finch stores arrays in column major order (first index fast). When the storage order of an array in a Finch expression corresponds to the loop order, we call this concordant iteration. For example, the following expression represents a concordant traversal of a sparse matrix, as the outer loops access the higher levels of the tensor tree:","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(\n    Dense(SparseList(Element(0.0))),\n    fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)),\n)\ns = Scalar(0.0)\n@finch for j in _, i in _\n    s[] += A[i, j]\nend\n\n# output\n\nNamedTuple()","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"We can investigate the generated code with @finch_code.  This code iterates over only the nonzeros in order. If our matrix is m × n with nnz nonzeros, this takes O(n + nnz) time.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code for j in _, i in _\n    s[] += A[i, j]\nend\n\n# output\n\nquote\n    s_data = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s_data.val\n    A_lvl = (ex.bodies[1]).body.body.rhs.tns.bind.lvl\n    A_lvl_stop = A_lvl.shape\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_2_ptr = A_lvl_2.ptr\n    A_lvl_2_idx = A_lvl_2.idx\n    A_lvl_2_stop = A_lvl_2.shape\n    A_lvl_3 = A_lvl_2.lvl\n    A_lvl_3_val = A_lvl_3.val\n    for j_3 = 1:A_lvl_stop\n        A_lvl_q = (1 - 1) * A_lvl_stop + j_3\n        A_lvl_2_q = A_lvl_2_ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2_ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2_idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(A_lvl_2_i1, A_lvl_2_stop)\n        if phase_stop >= 1\n            if A_lvl_2_idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_2_idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while true\n                A_lvl_2_i = A_lvl_2_idx[A_lvl_2_q]\n                if A_lvl_2_i < phase_stop\n                    A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                    s_val = A_lvl_3_val_2 + s_val\n                    A_lvl_2_q += 1\n                else\n                    phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_3\n                        A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                        s_val += A_lvl_3_val_2\n                        A_lvl_2_q += 1\n                    end\n                    break\n                end\n            end\n        end\n    end\n    result = ()\n    s_data.val = s_val\n    result\nend","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"When the loop order does not correspond to storage order, we call this discordant iteration. For example, if we swap the loop order in the above example, then Finch needs to randomly access each sparse column for each row i. We end up needing to find each (i, j) pair because we don't know whether it will be zero until we search for it. In all, this takes time O(n * m * log(nnz)), much less efficient! We shouldn't randomly access sparse arrays unless we really need to and they support it efficiently!","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Note the double for loop in the following code","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code for i in _, j in _\n    s[] += A[i, j]\nend # DISCORDANT, DO NOT DO THIS\n\n# output\n\nquote\n    s_data = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s_data.val\n    A_lvl = (ex.bodies[1]).body.body.rhs.tns.bind.lvl\n    A_lvl_stop = A_lvl.shape\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_2_ptr = A_lvl_2.ptr\n    A_lvl_2_idx = A_lvl_2.idx\n    A_lvl_2_stop = A_lvl_2.shape\n    A_lvl_3 = A_lvl_2.lvl\n    A_lvl_3_val = A_lvl_3.val\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_lvl_2_stop\n        for j_3 = 1:A_lvl_stop\n            A_lvl_q = (1 - 1) * A_lvl_stop + j_3\n            A_lvl_2_q = A_lvl_2_ptr[A_lvl_q]\n            A_lvl_2_q_stop = A_lvl_2_ptr[A_lvl_q + 1]\n            if A_lvl_2_q < A_lvl_2_q_stop\n                A_lvl_2_i1 = A_lvl_2_idx[A_lvl_2_q_stop - 1]\n            else\n                A_lvl_2_i1 = 0\n            end\n            phase_stop = min(i_3, A_lvl_2_i1)\n            if phase_stop >= i_3\n                if A_lvl_2_idx[A_lvl_2_q] < i_3\n                    A_lvl_2_q = Finch.scansearch(A_lvl_2_idx, i_3, A_lvl_2_q, A_lvl_2_q_stop - 1)\n                end\n                while true\n                    A_lvl_2_i = A_lvl_2_idx[A_lvl_2_q]\n                    if A_lvl_2_i < phase_stop\n                        A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                        s_val = A_lvl_3_val_2 + s_val\n                        A_lvl_2_q += 1\n                    else\n                        phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                        if A_lvl_2_i == phase_stop_3\n                            A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                            s_val += A_lvl_3_val_2\n                            A_lvl_2_q += 1\n                        end\n                        break\n                    end\n                end\n            end\n        end\n    end\n    result = ()\n    s_data.val = s_val\n    result\nend","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"TL;DR: As a quick heuristic, if your array indices are all in alphabetical order, then the loop indices should be reverse alphabetical.","category":"page"},{"location":"docs/language/optimization_tips/#Appropriate-Fill-Values","page":"Optimization Tips","title":"Appropriate Fill Values","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"The @finch macro requires the user to specify an output format. This is the most flexibile approach, but can sometimes lead to densification unless the output fill value is appropriate for the computation.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"For example, if A is m × n with nnz nonzeros, the following Finch kernel will densify B, filling it with m * n stored values:","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(\n    Dense(SparseList(Element(0.0))),\n    fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)),\n)\nB = Tensor(Dense(SparseList(Element(0.0)))) #DO NOT DO THIS, B has the wrong fill value\n@finch (B .= 0;\nfor j in _, i in _\n    B[i, j] = A[i, j] + 1\nend;\nreturn B)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Since A is filled with 0.0, adding 1 to the fill value produces 1.0. However, B can only represent a fill value of 0.0. Instead, we should specify 1.0 for the fill.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(\n    Dense(SparseList(Element(0.0))),\n    fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)),\n)\nB = Tensor(Dense(SparseList(Element(1.0))))\n@finch (B .= 1;\nfor j in _, i in _\n    B[i, j] = A[i, j] + 1\nend;\nreturn B)\ncountstored(B)\n\n# output\n\n5","category":"page"},{"location":"docs/language/optimization_tips/#Static-Versus-Dynamic-Values","page":"Optimization Tips","title":"Static Versus Dynamic Values","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"In order to skip some computations, Finch must be able to determine the value of program variables. Continuing our above example, if we obscure the value of 1 behind a variable x, Finch can only determine that x has type Int, not that it is 1.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(\n    Dense(SparseList(Element(0.0))),\n    fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)),\n)\nB = Tensor(Dense(SparseList(Element(1.0))))\nx = 1 #DO NOT DO THIS, Finch cannot see the value of x anymore\n@finch (B .= 1;\nfor j in _, i in _\n    B[i, j] = A[i, j] + x\nend;\nreturn B)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"However, there are some situations where you may want a value to be dynamic. For example, consider the function saxpy(x, a, y) = x .* a .+ y. Because we do not know the value of a until we run the function, we should treat it as dynamic, and the following implementation is reasonable:","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"function saxpy(x, a, y)\n    z = Tensor(SparseList(Element(0.0)))\n    @finch (z .= 0;\n    for i in _\n        z[i] = a * x[i] + y[i]\n    end;\n    return z)\nend","category":"page"},{"location":"docs/language/optimization_tips/#Use-Known-Functions","page":"Optimization Tips","title":"Use Known Functions","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Unless you declare the properties of your functions using Finch's User-Defined Functions interface, Finch doesn't know how they work. For example, using a lambda obscures the meaning of *.","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(\n    Dense(SparseList(Element(0.0))),\n    fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)),\n)\nB = ones(4, 3)\nC = Scalar(0.0)\nf(x, y) = x * y # DO NOT DO THIS, Obscures *\n@finch (C .= 0;\nfor j in _, i in _\n    C[] += f(A[i, j], B[i, j])\nend;\nreturn C)\n\n# output\n\n(C = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Checking the generated code, we see that this code is indeed densifying (notice the for-loop which repeatedly evaluates f(B[i, j], 0.0)).","category":"page"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code (C .= 0;\nfor j in _, i in _\n    C[] += f(A[i, j], B[i, j])\nend;\nreturn C)\n\n# output\n\nquote\n    C_data = ((ex.bodies[1]).bodies[1]).tns.bind\n    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.rhs.args[1]).tns.bind.lvl\n    A_lvl_stop = A_lvl.shape\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_2_ptr = A_lvl_2.ptr\n    A_lvl_2_idx = A_lvl_2.idx\n    A_lvl_2_stop = A_lvl_2.shape\n    A_lvl_3 = A_lvl_2.lvl\n    A_lvl_3_val = A_lvl_3.val\n    B_data = (((ex.bodies[1]).bodies[2]).body.body.rhs.args[2]).tns.bind\n    sugar_1 = size((((ex.bodies[1]).bodies[2]).body.body.rhs.args[2]).tns.bind)\n    B_mode1_stop = sugar_1[1]\n    B_mode2_stop = sugar_1[2]\n    B_mode1_stop == A_lvl_2_stop || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode1_stop) != $(A_lvl_2_stop))\"))\n    B_mode2_stop == A_lvl_stop || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode2_stop) != $(A_lvl_stop))\"))\n    C_val = 0\n    for j_4 = 1:B_mode2_stop\n        A_lvl_q = (1 - 1) * A_lvl_stop + j_4\n        A_lvl_2_q = A_lvl_2_ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2_ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2_idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(B_mode1_stop, A_lvl_2_i1)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_2_idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_2_idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while true\n                A_lvl_2_i = A_lvl_2_idx[A_lvl_2_q]\n                if A_lvl_2_i < phase_stop\n                    for i_6 = i:-1 + A_lvl_2_i\n                        val = B_data[i_6, j_4]\n                        C_val = (Main).f(0.0, val) + C_val\n                    end\n                    A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                    val_2 = B_data[A_lvl_2_i, j_4]\n                    C_val += (Main).f(A_lvl_3_val_2, val_2)\n                    A_lvl_2_q += 1\n                    i = A_lvl_2_i + 1\n                else\n                    phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_3\n                        for i_8 = i:-1 + phase_stop_3\n                            val_3 = B_data[i_8, j_4]\n                            C_val += (Main).f(0.0, val_3)\n                        end\n                        A_lvl_3_val_2 = A_lvl_3_val[A_lvl_2_q]\n                        val_4 = B_data[phase_stop_3, j_4]\n                        C_val += (Main).f(A_lvl_3_val_2, val_4)\n                        A_lvl_2_q += 1\n                    else\n                        for i_10 = i:phase_stop_3\n                            val_5 = B_data[i_10, j_4]\n                            C_val += (Main).f(0.0, val_5)\n                        end\n                    end\n                    i = phase_stop_3 + 1\n                    break\n                end\n            end\n        end\n        phase_start_3 = max(1, 1 + A_lvl_2_i1)\n        if B_mode1_stop >= phase_start_3\n            for i_12 = phase_start_3:B_mode1_stop\n                val_6 = B_data[i_12, j_4]\n                C_val += (Main).f(0.0, val_6)\n            end\n        end\n    end\n    C_data.val = C_val\n    (C = C_data,)\nend\n","category":"page"},{"location":"docs/language/optimization_tips/#Type-Stability","page":"Optimization Tips","title":"Type Stability","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Julia code runs fastest when the compiler can infer the types of all intermediate values.  Finch does not check that the generated code is type-stable. In situations where tensors have nonuniform index or element types, or the computation itself might involve multiple types, one should check that the output of @finch_kernel code is type-stable with @code_warntype.","category":"page"},{"location":"docs/language/optimization_tips/#Dense-Arrays","page":"Optimization Tips","title":"Dense Arrays","text":"","category":"section"},{"location":"docs/language/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Finch is currently optimized for sparse code and does not implement traditional dense optimizations. We are currently adding these features, but if you need dense performance, you may want to look at JuliaGPU instead.","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"CurrentModule = Finch","category":"page"},{"location":"docs/language/benchmarking_tips/#Benchmarking-Tips","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"","category":"section"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Julia code is notoriously fussy to benchmark. We'll use BenchmarkTools.jl to automatically follow best practices for getting reliable julia benchmarks. We'll also follow the Julia Performance Tips.","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Finch is even trickier to benchmark, for a few reasons:","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"The first time an @finch function is called, it is compiled, which takes an extra long time. @finch can also incur dynamic dispatch costs if the array types are not type stable. We can remedy this by using @finch_kernel, which simplifies benchmarking by compiling the function ahead of time, so it behaves like a normal Julia function. If you must use @finch, try to ensure that the code is type-stable.\nFinch tensors reuse memory from previous calls, so the first time a tensor is used in a Finch function, it will allocate memory, but subsequent times not so much. If we want to benchmark the memory allocation, we can reconstruct the tensor each time. Otherwise, we can let the repeated executions of the kernel measure the non-allocating runtime.\nRuntime for sparse code often depends on the sparsity pattern, so it's important to benchmark with representative data. Using standard matrices or tensors from MatrixDepot.jl or TensorDepot.jl is a good way to do this.","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"using Finch\nusing BenchmarkTools\nusing SparseArrays\nusing MatrixDepot","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Load a sparse matrix from MatrixDepot.jl and convert it to a Finch tensor","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), matrixdepot(\"HB/west0067\"))\n(m, n) = size(A)\n\nx = Tensor(Dense(Element(0.0)), rand(n))\ny = Tensor(Dense(Element(0.0)))","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Dense [1:0]","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Construct a Finch kernel for sparse matrix-vector multiply","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"eval(@finch_kernel function spmv(y, A, x)\n    y .= 0\n    for j in _, i in _\n        y[i] += A[i, j] * x[j]\n    end\nend)","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"spmv (generic function with 1 method)","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Benchmark the kernel, ignoring allocation costs for y","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"@benchmark spmv($y, $A, $x)","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"BenchmarkTools.Trial: 10000 samples with 211 evaluations.\n Range (min … max):  355.450 ns … 517.379 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     358.014 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   359.839 ns ±   9.413 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▇▃▇█▃               ▁▂▃                                       ▂\n  █████▁▁▁▁▁▁▁▃▃▁▁▄▃▃████▄▆▄▅▆▄▅▅▄▅▆▅▆▆▇▇▇▆▆▅▆▆▄▆▅▆▅▅▄▆▅▅▆▃▆▄▃▆ █\n  355 ns        Histogram: log(frequency) by time        407 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"The @benchmark macro will benchmark a function in local scope, and it will run the function a few times to estimate the runtime. It will also try to avoid first-time compilation costs by running the function once before benchmarking it. Note the use of $ to interpolate the arrays into the benchmark, bringing them into the local scope.","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"We can also benchmark the memory allocation of the kernel by constructing y in the benchmark kernel","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"@benchmark begin\n    y = Tensor(Dense(Element(0.0)))\n    y = spmv(y, $A, $x).y\nend","category":"page"},{"location":"docs/language/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"BenchmarkTools.Trial: 10000 samples with 202 evaluations.\n Range (min … max):  387.168 ns …   3.504 μs  ┊ GC (min … max): 0.00% … 86.82%\n Time  (median):     391.297 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   403.645 ns ± 136.241 ns  ┊ GC (mean ± σ):  1.68% ±  4.34%\n\n   ▂▇█▇▅▂           ▂▃▂▂▂▂▁▁▁                                   ▂\n  ▆███████▅▄▄▃▅▄▄▃▄▇████████████▇▆▆▆▇█▇███▇▇▇▆▇█▇█▇▇▆▅▅▄▄▆▅▅▅▄▅ █\n  387 ns        Histogram: log(frequency) by time        452 ns <\n\n Memory estimate: 608 bytes, allocs estimate: 2.","category":"page"},{"location":"docs/language/index_sugar/#Index-Sugar-and-Tensor-Modifiers","page":"Index Sugar","title":"Index Sugar and Tensor Modifiers","text":"","category":"section"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"In Finch, expressions like x[i + 1] are compiled using tensor modifiers, like offset(x, 1)[i]. The user can construct tensor modifiers directly, e.g. offset(x, 1), or implicitly using the syntax x[i + 1]. Recognizable index expressions are converted to tensor modifiers before dimensionalization, so that the modified tensor will participate in dimensionalization.","category":"page"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"While tensor modifiers may change the behavior of a tensor, they reference their parent tensor as the root tensor. Modified tensors are not understoond as distinct from their roots. For example, all accesses to the root tensor must obey lifecycle and dimensionalization rules. Additionally, root tensors which are themselves modifiers are unwrapped at the beginning of the program, so that modifiers are not obscured and the new root tensor is not a modifier.","category":"page"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"The following table lists the recognized index expressions and their equivalent tensor expressions, where i is an index, a, b are constants, p is an iteration protocol, and x is an expression:","category":"page"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"Original Expression Transformed Expression\nA[i + a] offset(A, 1)[i]\nA[i + x] toeplitz(A, 1)[i, x]\nA[(a:b)(i)] window(A, a:b)[i]\nA[a * i] scale(A, (3,))[i]\nA[i * x] products(A, 1)[i, j]\nA[~i] permissive(A)[i]\nA[p(i)] protocolize(A, p)[i]","category":"page"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"Each of these tensor modifiers is described below:","category":"page"},{"location":"docs/language/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"offset\ntoeplitz\nwindow\nscale\nproducts\npermissive\nprotocolize","category":"page"},{"location":"docs/language/index_sugar/#Finch.offset","page":"Index Sugar","title":"Finch.offset","text":"offset(tns, delta...)\n\nCreate an OffsetArray such that offset(tns, delta...)[i...] == tns[i .+ delta...]. The dimensions declared by an OffsetArray are shifted, so that size(offset(tns, delta...)) == size(tns) .+ delta.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.toeplitz","page":"Index Sugar","title":"Finch.toeplitz","text":"toeplitz(tns, dim)\n\nCreate a ToeplitzArray such that\n\n    Toeplitz(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] + i[dim + 1], i[dim + 2:end]...]\n\nThe ToplitzArray can be thought of as adding a dimension that shifts another dimension of the original tensor.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.window","page":"Index Sugar","title":"Finch.window","text":"window(tns, dims)\n\nCreate a WindowedArray which represents a view into another tensor\n\n    window(tns, dims)[i...] == tns[dim[1][i], dim[2][i], ...]\n\nThe windowed array restricts the new dimension to the dimension of valid indices of each dim. The dims may also be nothing to represent a full view of the underlying dimension.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.scale","page":"Index Sugar","title":"Finch.scale","text":"scale(tns, delta...)\n\nCreate a ScaleArray such that scale(tns, delta...)[i...] == tns[i .* delta...].  The dimensions declared by an OffsetArray are shifted, so that size(scale(tns, delta...)) == size(tns) .* delta.  This is only supported on tensors with real-valued dimensions.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.products","page":"Index Sugar","title":"Finch.products","text":"products(tns, dim)\n\nCreate a ProductArray such that\n\n    products(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] * i[dim + 1], i[dim + 2:end]...]\n\nThis is like toeplitz but with times instead of plus.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.permissive","page":"Index Sugar","title":"Finch.permissive","text":"permissive(tns, dims...)\n\nCreate an PermissiveArray where permissive(tns, dims...)[i...] is missing if i[n] is not in the bounds of tns when dims[n] is true.  This wrapper allows all permissive dimensions to be exempt from dimension checks, and is useful when we need to access an array out of bounds, or for padding. More formally,\n\n    permissive(tns, dims...)[i...] =\n        if any(n -> dims[n] && !(i[n] in axes(tns)[n]))\n            missing\n        else\n            tns[i...]\n        end\n\n\n\n\n\n","category":"function"},{"location":"docs/language/index_sugar/#Finch.protocolize","page":"Index Sugar","title":"Finch.protocolize","text":"protocolize(tns, protos...)\n\nCreate a ProtocolizedArray that accesses dimension n with protocol protos[n], if protos[n] is not nothing. See the documention for Iteration Protocols for more information. For example, to gallop along the inner dimension of a matrix A, we write A[gallop(i), j], which becomes protocolize(A, gallop, nothing)[i, j].\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/virtualization/#Program-Instances","page":"Virtualization","title":"Program Instances","text":"","category":"section"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Finch relies heavily on Julia's metaprogramming capabilities ( macros and generated functions in particular) to produce code. To review briefly, a macro allows us to inspect the syntax of it's arguments and generate replacement syntax. A generated function allows us to inspect the type of the function arguments and produce code for a function body.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In normal Finch usage, we might call Finch as follows:","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> C = Tensor(SparseList(Element(0)));\n\njulia> A = Tensor(SparseList(Element(0)), [0, 2, 0, 0, 3]);\n\njulia> B = Tensor(Dense(Element(0)), [11, 12, 13, 14, 15]);\n\njulia> @finch (C .= 0;\n       for i in _\n           C[i] = A[i] * B[i]\n       end);\n\njulia> C\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}:\n  0\n 24\n  0\n  0\n 45\n\njulia> tensor_tree(C)\n5-Tensor\n└─ SparseList (0) [1:5]\n   ├─ [2]: 24\n   └─ [5]: 45","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"The @macroexpand macro allows us to see the result of applying a macro. Let's examine what happens when we use the @finch macro (we've stripped line numbers from the result to clean it up):","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> Finch.regensym(Finch.striplines((@macroexpand @finch (C .= 0;\n       for i in _\n           C[i] = A[i] * B[i]\n       end))))\nquote\n    _res_1 = (Finch.execute)((Finch.FinchNotation.block_instance)((Finch.FinchNotation.block_instance)((Finch.FinchNotation.declare_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), literal_instance(0), (Finch.FinchNotation.literal_instance)(Finch.auto)), begin\n                        let i = index_instance(i)\n                            (Finch.FinchNotation.loop_instance)(i, Finch.FinchNotation.Auto(), (Finch.FinchNotation.assign_instance)((Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), (Finch.FinchNotation.updater_instance)((Finch.FinchNotation.literal_instance)((Finch.FinchNotation.initwrite)((Finch.fill_value)(C)))), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.literal_instance)((Finch.FinchNotation.initwrite)((Finch.fill_value)(C))), (Finch.FinchNotation.call_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:*), (Finch.FinchNotation.finch_leaf_instance)(*)), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:A), (Finch.FinchNotation.finch_leaf_instance)(A)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:B), (Finch.FinchNotation.finch_leaf_instance)(B)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))))))\n                        end\n                    end), (Finch.FinchNotation.yieldbind_instance)(variable_instance(:C))); )\n    begin\n        C = _res_1[:C]\n    end\n    begin\n        _res_1\n    end\nend","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In the above output, @finch creates an AST of program instances, then calls Finch.execute on it. A program instance is a struct that contains the program to be executed along with its arguments. Although we can use the above constructors (e.g. loop_instance) to make our own program instance, it is most convenient to use the unexported macro Finch.finch_program_instance:","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> using Finch: @finch_program_instance\n\njulia> prgm = Finch.@finch_program_instance (C .= 0;\n       for i in _\n           C[i] = A[i] * B[i]\n       end;\n       return C)\nFinch program instance: begin\n  tag(C, Tensor(SparseList(Element(0)))) .= 0\n  for i = Auto()\n    tag(C, Tensor(SparseList(Element(0))))[tag(i, i)] <<Finch.FinchNotation.InitWriter{0}()>>= tag(*, *)(tag(A, Tensor(SparseList(Element(0))))[tag(i, i)], tag(B, Tensor(Dense(Element(0))))[tag(i, i)])\n  end\n  return (tag(C, Tensor(SparseList(Element(0)))))\nend","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"As we can see, our program instance contains not only the AST to be executed, but also the data to execute the program with. The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. We can run our program using Finch.execute, which returns a NamedTuple of outputs.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> typeof(prgm)\nFinch.FinchNotation.BlockInstance{Tuple{Finch.FinchNotation.DeclareInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{0}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Auto()}}, Finch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.Auto, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.UpdaterInstance{Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.InitWriter{0}()}}, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.InitWriter{0}()}, Finch.FinchNotation.CallInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:*}, Finch.FinchNotation.LiteralInstance{*}}, Tuple{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:A}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:B}, Tensor{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}}}}}, Finch.FinchNotation.YieldBindInstance{Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}}}}}\n\njulia> C = Finch.execute(prgm).C\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}:\n  0\n 24\n  0\n  0\n 45","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"This functionality is sufficient for building finch kernels programatically. For example, if we wish to define a function pointwise_sum() that takes the pointwise sum of a variable number of vector inputs, we might implement it as follows:","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> function pointwise_sum(As...)\n           B = Tensor(Dense(Element(0)))\n           isempty(As) && return B\n           i = Finch.FinchNotation.index_instance(:i)\n           A_vars = [\n               Finch.FinchNotation.tag_instance(\n                   Finch.FinchNotation.variable_instance(Symbol(:A, n)), As[n]\n               ) for n in 1:length(As)\n           ]\n           #create a list of variable instances with different names to hold the input tensors\n           ex = @finch_program_instance 0\n           for A_var in A_vars\n               ex = @finch_program_instance $A_var[i] + $ex\n           end\n           prgm = @finch_program_instance (B .= 0;\n           for i in _\n               B[i] = $ex\n           end;\n           return B)\n           return Finch.execute(prgm).B\n       end\npointwise_sum (generic function with 1 method)\n\njulia> pointwise_sum([1, 2], [3, 4])\n2 Tensor{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}:\n 4\n 6","category":"page"},{"location":"docs/internals/virtualization/#Virtualization","page":"Virtualization","title":"Virtualization","text":"","category":"section"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Finch generates different code depending on the types of the arguments to the program. For example, in the following program, Finch generates different code depending on the types of A and B. In order to execute a program, Finch builds a typed AST (Abstract Syntax Tree), then calls Finch.execute on it. The AST object is just an instance of a program to execute, and contains the program to execute along with the data to execute it.  The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. During compilation, Finch uses the type of the program to construct a more ergonomic representation, which is then used to generate code. This process is called \"virtualization\".  All of the Finch AST nodes have both instance and virtual representations. For example, the literal 42 is represented as Finch.FinchNotation.LiteralInstance(42) and then virtualized to literal(42).  The virtualization process is implemented by the virtualize function.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> A = Tensor(SparseList(Element(0)), [0, 2, 0, 0, 3]);\n\njulia> B = Tensor(Dense(Element(0)), [11, 12, 13, 14, 15]);\n\njulia> s = Scalar(0);\n\njulia> typeof(A)\nTensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}\n\njulia> typeof(B)\nTensor{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}\n\njulia> inst = Finch.@finch_program_instance begin\n           for i in _\n               s[] += A[i]\n           end\n       end\nFinch program instance: for i = Auto()\n  tag(s, Scalar{0, Int64})[] <<tag(+, +)>>= tag(A, Tensor(SparseList(Element(0))))[tag(i, i)]\nend\n\njulia> typeof(inst)\nFinch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.Auto, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:s}, Scalar{0, Int64}}, Finch.FinchNotation.UpdaterInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:+}, Finch.FinchNotation.LiteralInstance{+}}}, Tuple{}}, Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:+}, Finch.FinchNotation.LiteralInstance{+}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:A}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}}}\n\njulia> Finch.virtualize(Finch.JuliaContext(), :inst, typeof(inst))\nFinch program: for i = virtual(Finch.FinchNotation.Auto)\n  tag(s, virtual(Finch.VirtualScalar))[] <<tag(+, +)>>= tag(A, virtual(Finch.VirtualFiber{Finch.VirtualSparseListLevel}))[tag(i, i)]\nend\n\njulia> @finch_code begin\n           for i in _\n               s[] += A[i]\n           end\n       end\nquote\n    s_data = (ex.bodies[1]).body.lhs.tns.bind\n    s_val = s_data.val\n    A_lvl = (ex.bodies[1]).body.rhs.tns.bind.lvl\n    A_lvl_ptr = A_lvl.ptr\n    A_lvl_idx = A_lvl.idx\n    A_lvl_stop = A_lvl.shape\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_2_val = A_lvl_2.val\n    A_lvl_q = A_lvl_ptr[1]\n    A_lvl_q_stop = A_lvl_ptr[1 + 1]\n    if A_lvl_q < A_lvl_q_stop\n        A_lvl_i1 = A_lvl_idx[A_lvl_q_stop - 1]\n    else\n        A_lvl_i1 = 0\n    end\n    phase_stop = min(A_lvl_i1, A_lvl_stop)\n    if phase_stop >= 1\n        if A_lvl_idx[A_lvl_q] < 1\n            A_lvl_q = Finch.scansearch(A_lvl_idx, 1, A_lvl_q, A_lvl_q_stop - 1)\n        end\n        while true\n            A_lvl_i = A_lvl_idx[A_lvl_q]\n            if A_lvl_i < phase_stop\n                A_lvl_2_val_2 = A_lvl_2_val[A_lvl_q]\n                s_val = A_lvl_2_val_2 + s_val\n                A_lvl_q += 1\n            else\n                phase_stop_3 = min(phase_stop, A_lvl_i)\n                if A_lvl_i == phase_stop_3\n                    A_lvl_2_val_2 = A_lvl_2_val[A_lvl_q]\n                    s_val += A_lvl_2_val_2\n                    A_lvl_q += 1\n                end\n                break\n            end\n        end\n    end\n    result = ()\n    s_data.val = s_val\n    result\nend\n\njulia> @finch_code begin\n           for i in _\n               s[] += B[i]\n           end\n       end\nquote\n    s_data = (ex.bodies[1]).body.lhs.tns.bind\n    s_val = s_data.val\n    B_lvl = (ex.bodies[1]).body.rhs.tns.bind.lvl\n    B_lvl_stop = B_lvl.shape\n    B_lvl_2 = B_lvl.lvl\n    B_lvl_2_val = B_lvl_2.val\n    for i_3 = 1:B_lvl_stop\n        B_lvl_q = (1 - 1) * B_lvl_stop + i_3\n        B_lvl_2_val_2 = B_lvl_2_val[B_lvl_q]\n        s_val = B_lvl_2_val_2 + s_val\n    end\n    result = ()\n    s_data.val = s_val\n    result\nend","category":"page"},{"location":"docs/internals/virtualization/#The-\"virtual\"-IR-Node","page":"Virtualization","title":"The \"virtual\" IR Node","text":"","category":"section"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Users can also create their own virtual nodes to represent their custom types. While most calls to virtualize result in a Finch IR Node, some objects, such as tensors and dimensions, are virtualized to a virtual object, which holds the custom virtual type.  These types may contain constants and other virtuals, as well as reference variables in the scope of the executing context. Any aspect of virtuals visible to Finch should be considered immutable, but virtuals may reference mutable variables in the scope of the executing context.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"virtualize\nFinchNotation.virtual","category":"page"},{"location":"docs/internals/virtualization/#Finch.virtualize","page":"Virtualization","title":"Finch.virtualize","text":"virtualize(ctx, ex, T, [tag])\n\nReturn the virtual program corresponding to the Julia expression ex of type T in the JuliaContext ctx. Implementaters may support the optional tag argument is used to name the resulting virtual variable.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/virtualization/#Finch.FinchNotation.virtual","page":"Virtualization","title":"Finch.FinchNotation.virtual","text":"virtual(val)\n\nFinch AST expression for an object val which has special meaning to the compiler. This type is typically used for tensors, as it allows users to specify the tensor's shape and data type.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/virtualization/#Virtual-Methods","page":"Virtualization","title":"Virtual Methods","text":"","category":"section"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Many methods have analogues we can call on the virtual version of the object. For example, we can call size an an array, and virtual_size on a virtual array. The virtual methods are used to generate code, so if they are pure they may return an expression which computes the results, and if they have side effects they may accept a context argument into which they can emit their side-effecting code.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In addition to the special compiler methods which are prefixed virtual_, there is also a function virtual_call, which is used to evaluate function calls on Finch IR when it would result in a virtual object. The behavior should mirror the concrete behavior of the corresponding function.","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"virtual_call","category":"page"},{"location":"docs/internals/virtualization/#Finch.virtual_call","page":"Virtualization","title":"Finch.virtual_call","text":"virtual_call(ctx, f, args...)\n\nGiven the virtual arguments args..., and a literal function f, return a virtual object representing the result of the function call. If the function is not foldable, return nothing. This function is used so that we can call e.g. tensor wrapper constructors and dimension constructors in finch code. Implementations should overload virtual_call_def to provide the actual implementation.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/virtualization/#Working-with-Finch-IR","page":"Virtualization","title":"Working with Finch IR","text":"","category":"section"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Calling print on a finch program or program instance will print the structure of the program as one would call constructors to build it. For example,","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> prgm_inst = Finch.@finch_program_instance for i in _\n           s[] += A[i]\n       end;\n\njulia> println(prgm_inst)\nloop_instance(index_instance(i), Finch.FinchNotation.Auto(), assign_instance(access_instance(tag_instance(variable_instance(:s), Scalar{0, Int64}(0)), updater_instance(tag_instance(variable_instance(:+), literal_instance(+)))), tag_instance(variable_instance(:+), literal_instance(+)), access_instance(tag_instance(variable_instance(:A), Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), reader_instance(), tag_instance(variable_instance(:i), index_instance(i)))))\n\njulia> prgm_inst\nFinch program instance: for i = Auto()\n  tag(s, Scalar{0, Int64})[] <<tag(+, +)>>= tag(A, Tensor(SparseList(Element(0))))[tag(i, i)]\nend\n\njulia> prgm = Finch.@finch_program for i in _\n           s[] += A[i]\n       end;\n\njulia> println(prgm)\nloop(index(i), virtual(Finch.FinchNotation.Auto()), assign(access(literal(Scalar{0, Int64}(0)), updater(literal(+))), literal(+), access(literal(Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), reader(), index(i))))\n\njulia> prgm\nFinch program: for i = virtual(Finch.FinchNotation.Auto)\n  Scalar{0, Int64}(0)[] <<+>>= Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))[i]\nend","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Both the virtual and instance representations of Finch IR define SyntaxInterface.jl and AbstractTrees.jl representations, so you can use the standard operation, arguments, istree, and children functions to inspect the structure of the program, as well as the rewriters defined by RewriteTools.jl","category":"page"},{"location":"docs/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> using Finch.FinchNotation;\n\njulia> PostOrderDFS(prgm)\nPostOrderDFS{FinchNode}(loop(index(i), virtual(Auto()), assign(access(literal(Scalar{0, Int64}(0)), updater(literal(+))), literal(+), access(literal(Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), reader(), index(i)))))\n\njulia> (@capture prgm loop(~idx, ~ext, ~val))\ntrue\n\njulia> idx\nFinch program: i","category":"page"},{"location":"docs/language/concordization/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"appendices/faqs/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"CurrentModule = Finch","category":"page"},{"location":"docs/array_api/#High-Level-Array-API","page":"High-Level Array API","title":"High-Level Array API","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch tensors also support many of the basic array operations one might expect, including indexing, slicing, and elementwise maps, broadcast, and reduce. For example:","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  1.0  0.0  2.0  0.0  0.0\n 0.0  0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> A + 0\n3×6 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  1.0  0.0  2.0  0.0  0.0\n 0.0  0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> A + 1\n3×6 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}}:\n 1.0  2.0  1.0  3.0  1.0  1.0\n 1.0  1.0  1.0  1.0  4.0  1.0\n 1.0  1.0  1.0  1.0  1.0  1.0\n\njulia> B = A .* 2\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  2.0  0.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  6.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> B[1:2, 1:2]\n2×2 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  2.0\n 0.0  0.0\n\njulia> map(x -> x^2, B)\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  4.0  0.0  16.0   0.0  0.0\n 0.0  0.0  0.0   0.0  36.0  0.0\n 0.0  0.0  0.0   0.0   0.0  0.0","category":"page"},{"location":"docs/array_api/#Einsum","page":"High-Level Array API","title":"Einsum","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch also supports a highly general @einsum macro which supports any reduction over any simple pointwise array expression.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"@einsum","category":"page"},{"location":"docs/array_api/#Finch.@einsum","page":"High-Level Array API","title":"Finch.@einsum","text":"@einsum tns[idxs...] <<op>>= ex...\n\nConstruct an einsum expression that computes the result of applying op to the tensor tns with the indices idxs and the tensors in the expression ex. The result is stored in the variable tns.\n\nex may be any pointwise expression consisting of function calls and tensor references of the form tns[idxs...], where tns and idxs are symbols.\n\nThe <<op>> operator can be any binary operator that is defined on the element type of the expression ex.\n\nThe einsum will evaluate the pointwise expression tns[idxs...] <<op>>= ex... over all combinations of index values in tns and the tensors in ex.\n\nHere are a few examples:\n\n@einsum C[i, j] += A[i, k] * B[k, j]\n@einsum C[i, j, k] += A[i, j] * B[j, k]\n@einsum D[i, k] += X[i, j] * Y[j, k]\n@einsum J[i, j] = H[i, j] * I[i, j]\n@einsum N[i, j] = K[i, k] * L[k, j] - M[i, j]\n@einsum R[i, j] <<max>>= P[i, k] + Q[k, j]\n@einsum x[i] = A[i, j] * x[j]\n\n\n\n\n\n","category":"macro"},{"location":"docs/array_api/#Array-Fusion","page":"High-Level Array API","title":"Array Fusion","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch supports array fusion, which allows you to compose multiple array operations into a single kernel. This can be a significant performance optimization, as it allows the compiler to optimize the entire operation at once. The two functions the user needs to know about are lazy and compute. You can use lazy to mark an array as an input to a fused operation, and call compute to execute the entire operation at once. For example:","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> C = lazy(A);\n\njulia> D = lazy(B);\n\njulia> E = (C .+ D) / 2;\n\njulia> compute(E)\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  1.5  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  4.5  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"In the above example, E is a fused operation that adds C and D together and then divides the result by 2. The compute function examines the entire operation and decides how to execute it in the most efficient way possible. In this case, it would likely generate a single kernel that adds the elements of A and B together and divides each result by 2, without materializing an intermediate.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"lazy\ncompute\nfused","category":"page"},{"location":"docs/array_api/#Finch.lazy","page":"High-Level Array API","title":"Finch.lazy","text":"lazy(arg)\n\nCreate a lazy tensor from an argument. All operations on lazy tensors are lazy, and will not be executed until compute is called on their result.\n\nfor example,\n\nx = lazy(rand(10))\ny = lazy(rand(10))\nz = x + y\nz = z + 1\nz = compute(z)\n\nwill not actually compute z until compute(z) is called, so the execution of x + y is fused with the execution of z + 1.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/#Finch.compute","page":"High-Level Array API","title":"Finch.compute","text":"compute(args...; ctx=default_scheduler(), kwargs...) -> Any\n\nCompute the value of a lazy tensor. The result is the argument itself, or a tuple of arguments if multiple arguments are passed. Some keyword arguments can be passed to control the execution of the program:     - verbose=false: Print the generated code before execution     - tag=:global: A tag to distinguish between different classes of inputs for the same program.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/#Finch.fused","page":"High-Level Array API","title":"Finch.fused","text":"fused(f, args...; kwargs...)\n\nThis function decorator modifies f to fuse the contained array operations and optimize the resulting program. The function must return a single array or tuple of arrays.  Some keyword arguments can be passed to control the execution of the program:     - verbose=false: Print the generated code before execution     - tag=:global: A tag to distinguish between different classes of inputs for the same program.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"The lazy and compute functions allow the compiler to fuse operations together, resulting in asymptotically more efficient code.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> using BenchmarkTools\n\njulia> A = fsprand(1000, 1000, 100);\n       B = Tensor(rand(1000, 1000));\n       C = Tensor(rand(1000, 1000));\n\njulia> @btime A .* (B * C);\n  145.940 ms (859 allocations: 7.69 MiB)\n\njulia> @btime compute(lazy(A) .* (lazy(B) * lazy(C)));\n  694.666 μs (712 allocations: 60.86 KiB)","category":"page"},{"location":"docs/array_api/#Optimizers","page":"High-Level Array API","title":"Optimizers","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Different optimizers can be used with compute, such as the state-of-the-art Galley optimizer, which can adapt to the sparsity patterns of the inputs. The optimizer can be set as an argument ctx to the compute function, or using set_scheduler or with_scheduler.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"set_scheduler!\nwith_scheduler\ndefault_scheduler","category":"page"},{"location":"docs/array_api/#Finch.set_scheduler!","page":"High-Level Array API","title":"Finch.set_scheduler!","text":"set_scheduler!(scheduler)\n\nSet the current scheduler to scheduler. The scheduler is used by compute to execute lazy tensor programs.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/#Finch.with_scheduler","page":"High-Level Array API","title":"Finch.with_scheduler","text":"with_scheduler(f, scheduler)\n\nExecute f with the current scheduler set to scheduler.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/#Finch.default_scheduler","page":"High-Level Array API","title":"Finch.default_scheduler","text":"default_scheduler(;verbose=false)\n\nThe default scheduler used by compute to execute lazy tensor programs. Fuses all pointwise expresions into reductions. Only fuses reductions into pointwise expressions when they are the only usage of the reduction.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/#The-Galley-Optimizer","page":"High-Level Array API","title":"The Galley Optimizer","text":"","category":"section"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Galley is a cost-based optimizer for Finch's lazy evaluation interface based on techniques from database query optimization. To use Galley, you just add the parameter ctx=galley_optimizer() to the compute function. While the default optimizer (ctx=default_scheduler()) makes decisions entirely based on the types of the inputs, Galley gathers statistics on their sparsity to make cost-based based optimization decisions.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"galley_scheduler","category":"page"},{"location":"docs/array_api/#Finch.Galley.galley_scheduler","page":"High-Level Array API","title":"Finch.Galley.galley_scheduler","text":"galley_scheduler(verbose = false, estimator=DCStats)\n\nThe galley scheduler uses the sparsity patterns of the inputs to optimize the computation. The first set of inputs given to galley is used to optimize, and the estimator is used to estimate the sparsity of intermediate computations during optimization.\n\n\n\n\n\n","category":"function"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> A = fsprand(1000, 1000, 0.1);\n       B = fsprand(1000, 1000, 0.1);\n       C = fsprand(1000, 1000, 0.0001);\n\njulia> A = lazy(A);\n       B = lazy(B);\n       C = lazy(C);\n\njulia> @btime compute(sum(A * B * C));\n  282.503 ms (1018 allocations: 184.43 MiB)\n\njulia> @btime compute(sum(A * B * C), ctx=galley_scheduler());\n  152.792 μs (672 allocations: 28.81 KiB)","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"By taking advantage of the fact that C is highly sparse, Galley can better structure the computation. In the matrix chain multiplication, it always starts with the C,B matmul before multiplying with A. In the summation, it takes advantage of distributivity to pushing the reduction down to the inputs. It first sums over A and C, then multiplies those vectors with B.","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Because Galley adapts to the sparsity patterns of the first input tensor, it can be useful to distinguish between different uses of the same function using the tag keyword argument to compute or fuse.  For example, we may wish to distinguish one spmv from another, as follows:","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> A = rand(1000, 1000);\n       B = rand(1000, 1000);\n       C = fsprand(1000, 1000, 0.0001);\n\njulia> fused((A, B, C) -> C .* (A * B), A, B, C; tag=:very_sparse_sddmm);\n\njulia> C = fsprand(1000, 1000, 0.9);\n\njulia> fused((A, B, C) -> C .* (A * B), A, B, C; tag=:very_dense_sddmm);\n","category":"page"},{"location":"docs/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"By distinguishing between the two uses of the same function, Galley can make better decisions about how to optimize each computation separately.","category":"page"},{"location":"appendices/publications_articles/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"docs/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/finch_logic/#Finch-Logic-(High-Level-IR)","page":"Finch Logic","title":"Finch Logic (High-Level IR)","text":"","category":"section"},{"location":"docs/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"Finch Logic is an internal high-level intermediate representation (IR) that allows us to fuse and optimize successive calls to array operations such as map, reduce, and broadcast. It is reminiscent to database query notation, representing the a sequence of tensor expressions bound to variables. Values in the program are tensors, with named indices. The order of indices is semantically meaningful.","category":"page"},{"location":"docs/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"The nodes are as follows:","category":"page"},{"location":"docs/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"immediate\ndeferred\nfield\nalias\ntable\nmapjoin\naggregate\nreorder\nrelabel\nreformat\nsubquery\nquery\nproduces\nplan","category":"page"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.immediate","page":"Finch Logic","title":"Finch.FinchLogic.immediate","text":"immediate(val)\n\nLogical AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.deferred","page":"Finch Logic","title":"Finch.FinchLogic.deferred","text":"deferred(ex, [type])\n\nLogical AST expression for an expression ex of type type, yet to be evaluated.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.field","page":"Finch Logic","title":"Finch.FinchLogic.field","text":"field(name)\n\nLogical AST expression for an field named name.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.alias","page":"Finch Logic","title":"Finch.FinchLogic.alias","text":"alias(name)\n\nLogical AST expression for an alias named name.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.table","page":"Finch Logic","title":"Finch.FinchLogic.table","text":"table(tns, idxs...)\n\nLogical AST expression for a tensor object tns, indexed by fields idxs....\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.mapjoin","page":"Finch Logic","title":"Finch.FinchLogic.mapjoin","text":"mapjoin(op, args...)\n\nLogical AST expression for mapping the function op across args.... The order of fields in the mapjoin is unique(vcat(map(getfields, args)...))\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.aggregate","page":"Finch Logic","title":"Finch.FinchLogic.aggregate","text":"aggregate(op, init, arg, idxs...)\n\nLogical AST statement that reduces arg using op, starting with init. idxs are the dimensions to reduce. May happen in any order.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.reorder","page":"Finch Logic","title":"Finch.FinchLogic.reorder","text":"reorder(arg, idxs...)\n\nLogical AST statement that reorders the dimensions of arg to be idxs.... Dimensions known to be length 1 may be dropped. Dimensions that do not exist in arg may be added.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.relabel","page":"Finch Logic","title":"Finch.FinchLogic.relabel","text":"relabel(arg, idxs...)\n\nLogical AST statement that relabels the dimensions of arg to be idxs...\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.reformat","page":"Finch Logic","title":"Finch.FinchLogic.reformat","text":"reformat(tns, arg)\n\nLogical AST statement that reformats arg into the tensor tns.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.subquery","page":"Finch Logic","title":"Finch.FinchLogic.subquery","text":"subquery(lhs, arg)\n\nLogical AST statement that evaluates arg, binding the result to lhs, and returns arg.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.query","page":"Finch Logic","title":"Finch.FinchLogic.query","text":"query(lhs, rhs)\n\nLogical AST statement that evaluates rhs, binding the result to lhs.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.produces","page":"Finch Logic","title":"Finch.FinchLogic.produces","text":"produces(args...)\n\nLogical AST statement that returns args... from the current plan. Halts execution of the program.\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.plan","page":"Finch Logic","title":"Finch.FinchLogic.plan","text":"plan(bodies...)\n\nLogical AST statement that executes a sequence of statements bodies....\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_logic/#Finch-Logic-Internals","page":"Finch Logic","title":"Finch Logic Internals","text":"","category":"section"},{"location":"docs/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"FinchLogic.LogicNode\nFinchLogic.logic_leaf\nisimmediate\nisdeferred\nisalias\nisfield","category":"page"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.LogicNode","page":"Finch Logic","title":"Finch.FinchLogic.LogicNode","text":"LogicNode\n\nA Finch Logic IR node. Finch uses a variant of Concrete Field Notation as an intermediate representation.\n\nThe LogicNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchLogic.LogicNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.logic_leaf","page":"Finch Logic","title":"Finch.FinchLogic.logic_leaf","text":"logic_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a immediate or value.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.isimmediate","page":"Finch Logic","title":"Finch.FinchLogic.isimmediate","text":"isimmediate(node)\n\nReturns true if the node is a finch immediate\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.isdeferred","page":"Finch Logic","title":"Finch.FinchLogic.isdeferred","text":"isdeferred(node)\n\nReturns true if the node is a finch immediate\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.isalias","page":"Finch Logic","title":"Finch.FinchLogic.isalias","text":"isalias(node)\n\nReturns true if the node is a finch alias\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_logic/#Finch.FinchLogic.isfield","page":"Finch Logic","title":"Finch.FinchLogic.isfield","text":"isfield(node)\n\nReturns true if the node is a finch field\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_logic/#Executing-FinchLogic","page":"Finch Logic","title":"Executing FinchLogic","text":"","category":"section"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In Finch, all tensors accessed by a particular index must have the same dimension along the corresponding mode. Finch determines the dimension of a loop index i from all of the tensors using i in an access, as well as the bounds in the loop itself.","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"For example, consider the following code","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"A = fsprand(3, 4, 0.5)\nB = fsprand(4, 5, 0.5)\nC = Tensor(Dense(SparseList(Element(0.0))))\n@finch begin\n    C .= 0\n    for i in 1:3\n        for j in _\n            for k in _\n                C[i, j] += A[i, k] * B[k, j]\n            end\n        end\n    end\nend","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In the above code, the second dimension of A must match the first dimension of B.  Also, the first dimension of A must match the i loop dimension, 1:3. Finch will also resize declared tensors to match indices used in writes, so C is resized to (1:3, 1:5). If no dimensions are specified elsewhere, then Finch will use the dimension of the declared tensor.","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Dimensionalization occurs after wrapper arrays are de-sugared. You can therefore exempt a tensor from dimensionalization by wrapping the corresponding index in ~. For example,","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"@finch begin\n    y .= 0\n    for i in 1:3\n        y[~i] += x[i]\n    end\nend","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"does not set the dimension of y, and y does not participate in dimensionalization.","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In summary, the rules of index dimensionalization are as follows:","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Indices have dimensions\nUsing an index in an access “hints” that the index should have the corresponding dimension\nLoop dimensions are equal to the “meet” of all hints in the loop body and the loop bounds\nThe meet usually asserts that dimensions match, but may also e.g. propagate info about parallelization","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"The rules of declaration dimensionalization are as follows:","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Declarations have dimensions\nLeft hand side (updating) tensor access “hint” the size of that tensor\nThe dimensions of a declaration are the “meet” of all hints from the declaration to the first read\nThe new dimensions of the declared tensor are used when the tensor is on the right hand side (reading) access.","category":"page"},{"location":"docs/language/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Finch.FinchNotation.Auto","category":"page"},{"location":"docs/language/dimensionalization/#Finch.FinchNotation.Auto","page":"Dimensionalization","title":"Finch.FinchNotation.Auto","text":"Auto()\n\nA singleton type representing the lack of a dimension.  This is used in place of a dimension when we want to avoid dimensionality checks. In the @finch macro, you can write Auto() with an underscore as for i = _, allowing finch to pick up the loop bounds from the tensors automatically.\n\n\n\n\n\n","category":"type"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"CurrentModule = Finch","category":"page"},{"location":"docs/language/finch_language/#Finch-Notation","page":"The Finch Language","title":"Finch Notation","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are written in Julia, but they are not Julia programs. Instead, they are an abstraction description of a tensor computation.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Several examples are given in the examples directory.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are blocks of tensor operations, joined by control flow. Finch is an imperative language. The AST is separated into statements and expressions, where statements can modify the state of the program but expressions cannot.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"The core Finch expressions are:","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"literal e.g. 1, 1.0, nothing\nvalue e.g. x, y\nindex e.g. i, inside of for i = _; ... end\nvariable e.g. x, inside of (x = y; ...)\ncall e.g. op(args...)\naccess e.g. tns[idxs...]","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"And the core Finch statements are:","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"declare e.g. tns .= init\nassign e.g. lhs[idxs...] <<op>>= rhs\nloop e.g. for i = _; ... end\ndefine e.g. let var = val; ... end\nsieve e.g. if cond; ... end\nblock e.g. begin ... end","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"literal\nvalue\nindex\nvariable\ncall\naccess\ndefine\nassign\nloop\nsieve\nblock","category":"page"},{"location":"docs/language/finch_language/#Finch.FinchNotation.literal","page":"The Finch Language","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.value","page":"The Finch Language","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.index","page":"The Finch Language","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.variable","page":"The Finch Language","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.call","page":"The Finch Language","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.access","page":"The Finch Language","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.define","page":"The Finch Language","title":"Finch.FinchNotation.define","text":"define(lhs, rhs, body)\n\nFinch AST statement that defines lhs as having the value rhs in body. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.assign","page":"The Finch Language","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). Overwriting is accomplished with the function overwrite(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.loop","page":"The Finch Language","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.sieve","page":"The Finch Language","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.block","page":"The Finch Language","title":"Finch.FinchNotation.block","text":"block(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Scoping","page":"The Finch Language","title":"Scoping","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are scoped. Scopes contain variable definitions and tensor declarations.  Loops and sieves introduce new scopes. The following program has four scopes, each of which is numbered to the left of the statements it contains.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"@finch begin\n1   y .= 0\n1   for j = _\n1   2   t .= 0\n1   2   for i = _\n1   2   3   t[] += A[i, j] * x[i]\n1   2   end\n1   2   for i = _\n1   2   4   y[i] += A[i, j] * t[]\n1   2   end\n1   end\nend","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Variables refer to their defined values in the innermost containing scope. If variables are undefined, they are assumed to have global scope (they may come from the surrounding program).","category":"page"},{"location":"docs/language/finch_language/#Tensor-Lifecycle","page":"The Finch Language","title":"Tensor Lifecycle","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensors have two modes: Read and Update. Tensors in read mode may be read, but not updated. Tensors in update mode may be updated, but not read. A tensor declaration initializes and possibly resizes the tensor, setting it to update mode. Also, Finch will automatically change the mode of tensors as they are used. However, tensors may only change their mode within scopes that contain their declaration. If a tensor has not been declared, it is assumed to have global scope.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensor declaration is different than variable definition. Declaring a tensor initializes the memory (usually to zero) and sets the tensor to update mode. Defining a tensor simply gives a name to that memory. A tensor may be declared multiple times, but it may only be defined once.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensors are assumed to be in read mode when they are defined. Tensors must enter and exit scope in read mode. Finch inserts freeze and thaw statements to ensure that tensors are in the correct mode. Freezing a tensor prevents further updates and allows reads. Thawing a tensor allows further updates and prevents reads.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensor lifecycle statements consist of:","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"reader\nupdater\ndeclare\nfreeze\nthaw","category":"page"},{"location":"docs/language/finch_language/#Finch.FinchNotation.reader","page":"The Finch Language","title":"Finch.FinchNotation.reader","text":"reader()\n\nFinch AST expression representing a read-only mode for a tensor access. Declare, freeze, and thaw statements can change the mode of a tensor.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.updater","page":"The Finch Language","title":"Finch.FinchNotation.updater","text":"updater(op)\n\nFinch AST expression representing an update-only mode for a tensor access, using the reduction operator op.  Declare, freeze, and thaw statements can change the mode of a tensor.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.declare","page":"The Finch Language","title":"Finch.FinchNotation.declare","text":"declare(tns, init, op)\n\nFinch AST statement that declares tns with an initial value init reduced with op in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.freeze","page":"The Finch Language","title":"Finch.FinchNotation.freeze","text":"freeze(tns, op)\n\nFinch AST statement that freezes tns in the current scope after modifications with op, moving the tensor from update-only mode to read-only mode.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Finch.FinchNotation.thaw","page":"The Finch Language","title":"Finch.FinchNotation.thaw","text":"thaw(tns, op)\n\nFinch AST statement that thaws tns in the current scope, moving the tensor from read-only mode to update-only mode with a reduction operator op.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/finch_language/#Dimensionalization","page":"The Finch Language","title":"Dimensionalization","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch loops have dimensions. Accessing a tensor with an unmodified loop index \"hints\" that the loop should have the same dimension as the corresponding axis of the tensor. Finch will automatically dimensionalize loops that are hinted by tensor accesses. One may refer to the automatically determined dimension using a variable named _ or :.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Similarly, tensor declarations also set the dimensions of a tensor. Accessing a tensor with an unmodified loop index \"hints\" that the tensor axis should have the same dimension as the corresponding loop. Finch will automatically dimensionalize declarations based on all updates up to the first read.","category":"page"},{"location":"docs/language/finch_language/#Array-Combinators","page":"The Finch Language","title":"Array Combinators","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch includes several array combinators that modify the behavior of arrays. For example, the OffsetArray type wraps an existing array, but shifts its indices. The PermissiveArray type wraps an existing array, but allows out-of-bounds reads and writes. When an array is accessed out of bounds, it produces Missing.","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Array combinators introduce some complexity to the tensor lifecycle, as wrappers may contain multiple or different arrays that could potentially be in different modes. Any array combinators used in a tensor access must reference a single global variable which holds the root array. The root array is the single array that gets declared, and changes modes from read to update, or vice versa.","category":"page"},{"location":"docs/language/finch_language/#Fancy-Indexing","page":"The Finch Language","title":"Fancy Indexing","text":"","category":"section"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch supports arbitrary indexing of arrays, but certain indexing operations have first class support through array combinators. Before dimensionalization, the following transformations are performed:","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"    A[i + c] =>        OffsetArray(A, c)[i]\n    A[i + j] =>      ToeplitzArray(A, 1)[i, j]\n       A[~i] => PermissiveArray(A, true)[i]","category":"page"},{"location":"docs/language/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Note that these transformations may change the behavior of dimensionalization, since they often result in unmodified loop indices (the index i will participate in dimensionalization, but an index expression like i + 1 will not).","category":"page"},{"location":"docs/language/parallelization/","page":"Parallelization","title":"Parallelization","text":"CurrentModule = Finch","category":"page"},{"location":"docs/language/parallelization/#(Experimental)-Parallelization-in-Finch","page":"Parallelization","title":"(Experimental) Parallelization in Finch","text":"","category":"section"},{"location":"docs/language/parallelization/#Formats","page":"Parallelization","title":"Formats","text":"","category":"section"},{"location":"docs/language/parallelization/","page":"Parallelization","title":"Parallelization","text":"Finch levels usually cannot be updated concurrently from multiple threads. Sparse and structured formats typically store their data buffers contiguously across different columns, making parallel updates difficult to implement correctly and efficiently. However, Finch provides a few specialized concurrent level types, and a few levels which can reinterpret other level formats in a concurrent way.","category":"page"},{"location":"docs/language/parallelization/","page":"Parallelization","title":"Parallelization","text":"Finch.AtomicElementLevel\nFinch.MutexLevel\nFinch.SeparateLevel","category":"page"},{"location":"docs/language/parallelization/#Finch.AtomicElementLevel","page":"Parallelization","title":"Finch.AtomicElementLevel","text":"AtomicElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nLike an ElementLevel, but updates to the level are performed atomically.\n\njulia> tensor_tree(Tensor(Dense(AtomicElement(0.0)), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"docs/language/parallelization/#Finch.MutexLevel","page":"Parallelization","title":"Finch.MutexLevel","text":"MutexLevel{Val, Lvl}()\n\nMutex Level Protects the level directly below it with atomics\n\nEach position in the level below the Mutex level is protected by a lock.\n\njulia> tensor_tree(Tensor(Dense(Mutex(Element(0.0))), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Mutex ->\n   │  └─ 1.0\n   ├─ [2]: Mutex ->\n   │  └─ 2.0\n   └─ [3]: Mutex ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"docs/language/parallelization/#Finch.SeparateLevel","page":"Parallelization","title":"Finch.SeparateLevel","text":"SeparateLevel{Lvl, [Val]}()\n\nA subfiber of a Separate level is a separate tensor of type Lvl, in it's own memory space.\n\nEach sublevel is stored in a vector of type Val with eltype(Val) = Lvl.\n\njulia> tensor_tree(Tensor(Dense(Separate(Element(0.0))), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Pointer ->\n   │  └─ 1.0\n   ├─ [2]: Pointer ->\n   │  └─ 2.0\n   └─ [3]: Pointer ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"docs/language/parallelization/#Parallel-Loops","page":"Parallelization","title":"Parallel Loops","text":"","category":"section"},{"location":"docs/language/parallelization/","page":"Parallelization","title":"Parallelization","text":"A loop can be run in parallel with a parallel dimension. A dimension can be wrapped in the parallel() modifier to indicate that it should run in parallel.","category":"page"},{"location":"docs/language/parallelization/","page":"Parallelization","title":"Parallelization","text":"Finch.parallel\nFinch.CPU\nFinch.Serial","category":"page"},{"location":"docs/language/parallelization/#Finch.parallel","page":"Parallelization","title":"Finch.parallel","text":"parallel(ext, device=CPU(nthreads()))\n\nA dimension ext that is parallelized over device. The ext field is usually _, or dimensionless, but can be any standard dimension argument.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/parallelization/#Finch.CPU","page":"Parallelization","title":"Finch.CPU","text":"CPU(n)\n\nA device that represents a CPU with n threads.\n\n\n\n\n\n","category":"type"},{"location":"docs/language/parallelization/#Finch.Serial","page":"Parallelization","title":"Finch.Serial","text":"Serial()\n\nA device that represents a serial CPU execution.\n\n\n\n\n\n","category":"type"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"We welcome contributions to Finch, and follow the Julia contributing guidelines.  If you use or want to use Finch and have a question or bug, please do file a Github issue!  If you want to contribute to Finch, please first file an issue to double check that there is interest from a contributor in the feature.","category":"page"},{"location":"CONTRIBUTING/#Versions","page":"Community and Contributions","title":"Versions","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The Finch API is stable and we follow semantic versioning conventions. The main branch of the Finch repo is the most up-to-date development branch. While it is not stable, it should always pass tests.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Contributors will develop and test Finch from a local directory. Please see the Package documentation for more info, particularly the section on developing.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"To determine which version of Finch you have, run Pkg.status(\"Finch\") in the Julia REPL. If the installed version of Finch tracks a local path, the output will include the path like so:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Status `~/.julia/environments/v1.9/Project.toml`\n  [9177782c] Finch v0.5.4 `~/Projects/Finch.jl`","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If the installed version of Finch tracks a particular version (probably not what you want since it will not reflect local changes), the output will look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Status `~/.julia/environments/v1.8/Project.toml`\n  [9177782c] Finch v0.5.4","category":"page"},{"location":"CONTRIBUTING/#Utilities","page":"Community and Contributions","title":"Utilities","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Finch include several scripts that can be executed directly, e.g. runtests.jl. These scripts are all have local Pkg environments. The scripts include convenience headers to automatically use their respective environments, so you won't need to worry about --project=. flags, etc.","category":"page"},{"location":"CONTRIBUTING/#Testing","page":"Community and Contributions","title":"Testing","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"All pull requests should pass continuous integration testing before merging. The test suite has a few options, which are accessible through running the test suite directly as ./test/runtests.jl.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Finch compares compiler output against reference versions.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If you have the appropriate permissions, you can run the FixBot github action on your PR branch to automatically generate output for both 32-bit and 64-bit builds.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If you run the test suite directly you can pass the --overwrite flag to tell the test suite to overwrite the reference.  Because the reference output depends on the system word size, you'll need to generate reference output for 32-bit and 64-bit builds of Julia to get Finch to pass tests. The easiest way to do this is to run each 32-bit or 64-bit build of Julia on a system that supports it. You can Download multiple builds yourself or use juliaup to manage multiple versions. Using juliaup, it might look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"julia +release~x86 test/runtests.jl --overwrite\njulia +release~x64 test/runtests.jl --overwrite","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The test suite takes a while to run. You can parallelize by setting the -p option to configure the number of processors.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"./test/runtests.jl -p 22","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"You can also filter to only run a selection of test suitesusing the --include or --exclude arguments, or","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"./test/runtests.jl --include constructors interface_einsum interface_asmd","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"This information is summarized with ./test/runtests.jl --help","category":"page"},{"location":"CONTRIBUTING/#Python-test-suite","page":"Community and Contributions","title":"Python test suite","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"finch-tensor-python contains a separate Array API compatible test suite written in Python. It requires Python 3.10 or later and Poetry installed.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"It can be run with:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"git clone https://github.com/finch-tensor/finch-tensor-python.git\ncd finch-tensor-python\npoetry install --with test\nFINCH_REPO_PATH=<PATH_TO_FINCH_REPO> poetry run pytest tests/","category":"page"},{"location":"CONTRIBUTING/#Benchmarking","page":"Community and Contributions","title":"Benchmarking","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The Finch test suite includes a benchmarking script that measures Finch performance on a variety of kernels. It also includes some scripts to help compare Finch performance on the feature branch to the main branch. To run the benchmarking script, run ./benchmarks/runbenchmarks.jl. To run the comparison script, run ./benchmarks/runjudge.jl. Both scripts take a while to run and generate a report at the end.","category":"page"},{"location":"CONTRIBUTING/#Documentation","page":"Community and Contributions","title":"Documentation","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The /docs directory includes Finch documentation in /src, and a built website in /build. You can build the website with ./docs/make.jl. You can run doctests with ./docs/test.jl, and fix doctests with ./docs/fix.jl, though both are included as part of the test suite.","category":"page"},{"location":"CONTRIBUTING/#Code-Style","page":"Community and Contributions","title":"Code Style","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"We use Blue Style formatting, with a few tweaks defined in .JuliaFormatter.toml. Running the tests in overwrite mode will automatically reformat your code, but you can also add JuliaFormatter to your editor to reformat as you go.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/parallel/#Parallel-Processing-in-Finch","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"","category":"section"},{"location":"docs/internals/parallel/#Modelling-the-Architecture","page":"Parallel Processing in Finch","title":"Modelling the Architecture","text":"","category":"section"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"Finch uses a simple, hierarchical representation of devices and tasks to model different kind of parallel processing. An AbstractDevice is a physical or virtual device on which we can execute tasks, which may each be represented by an AbstractTask.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"AbstractTask\nAbstractDevice","category":"page"},{"location":"docs/internals/parallel/#Finch.AbstractTask","page":"Parallel Processing in Finch","title":"Finch.AbstractTask","text":"AbstractTask\n\nAn individual processing unit on a device, responsible for running code.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.AbstractDevice","page":"Parallel Processing in Finch","title":"Finch.AbstractDevice","text":"AbstractDevice\n\nA datatype representing a device on which tasks can be executed.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"The current task in a compilation context can be queried with get_task. Each device has a set of numbered child tasks, and each task has a parent task.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"get_num_tasks\nget_task_num\nget_device\nget_parent_task","category":"page"},{"location":"docs/internals/parallel/#Finch.get_num_tasks","page":"Parallel Processing in Finch","title":"Finch.get_num_tasks","text":"get_num_tasks(dev::AbstractDevice)\n\nReturn the number of tasks on the device dev.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.get_task_num","page":"Parallel Processing in Finch","title":"Finch.get_task_num","text":"get_task_num(task::AbstractTask)\n\nReturn the task number of task.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.get_device","page":"Parallel Processing in Finch","title":"Finch.get_device","text":"get_device(task::AbstractTask)\n\nReturn the device that task is running on.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.get_parent_task","page":"Parallel Processing in Finch","title":"Finch.get_parent_task","text":"get_parent_task(task::AbstractTask)\n\nReturn the task which spawned task.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Data-Transfer","page":"Parallel Processing in Finch","title":"Data Transfer","text":"","category":"section"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"Before entering a parallel loop, a tensor may reside on a single task, or represent a single view of data distributed across multiple tasks, or represent multiple separate tensors local to multiple tasks. A tensor's data must be resident in the current task to process operations on that tensor, such as loops over the indices, accesses to the tensor, or declare, freeze, or thaw. Upon entering a parallel loop, we must transfer the tensor to the tasks where it is needed. Upon exiting the parallel loop, we may need to combine the data from multiple tasks into a single tensor.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"All tensor and buffer transfers are accomplished with the transfer function.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"The distribute function is used by the compiler to orchestrate data distribution before and after a parallel region, with","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"different style objects signaling the type of transfer.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"Note: After distributing a tensor, we must also update any in-progress traversals over the tensor that may appear throughout the program. This is done with the redistribute function. Tensors are responsible for defining their own redistribute behavior, but it should be guaranteed that distribute(tns, diff) == redistribute(tns, diff). In general, this means that any nested structure in the tensor should be preserved through transfers. Most subtensors will store a list of property names describing how to reach the subtensor from the root tensor.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"distribute\nredistribute","category":"page"},{"location":"docs/internals/parallel/#Finch.distribute","page":"Parallel Processing in Finch","title":"Finch.distribute","text":"distribute(ctx, arr, device, diff, style)\n\nIf the virtual array is not on the given device, copy the array to that device. This function may modify underlying data arrays, but cannot change the virtual itself. This function is used to move data to the device before a kernel is launched. Since this function may modify the root node, iterators in-progress may need to be updated. We can store new root objects in the diff dictionary.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.redistribute","page":"Parallel Processing in Finch","title":"Finch.redistribute","text":"redistribute(ctx, node, diff)\n\nWhen the root node is distributed, several iterators may need to be updated.\n\nThe redistribute function traverses tns and updates it based on the updated objects in the diff dictionary.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"The distribute function is called on the Host and on the Device, and is responsible for distributing the tensor among tasks and collecting the results, if applicable.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"If the tensor is a temporary tensor declared within the parallel loop, we distribute the tensor to Local scope. If the tensor is declared outside the parallel loop and is not modified, we distribute the tensor to Global scope. If the tensor is declared outside the parallel loop and is modified, we distribute the tensor to Shared scope. Depending on the architecture, several of these operations may be no-ops.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"HostLocal\nHostGlobal\nHostShared\nDeviceLocal\nDeviceGlobal\nDeviceShared","category":"page"},{"location":"docs/internals/parallel/#Finch.HostLocal","page":"Parallel Processing in Finch","title":"Finch.HostLocal","text":"HostLocal()\n\nFrom the host, distribute the tensor to device local memory.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.HostGlobal","page":"Parallel Processing in Finch","title":"Finch.HostGlobal","text":"HostGlobal()\n\nFrom the host, distribute the tensor to device global memory.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.HostShared","page":"Parallel Processing in Finch","title":"Finch.HostShared","text":"HostShared()\n\nFrom the host, distribute the tensor to device shared memory.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.DeviceLocal","page":"Parallel Processing in Finch","title":"Finch.DeviceLocal","text":"DeviceLocal()\n\nFrom the device, load the local version of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.DeviceGlobal","page":"Parallel Processing in Finch","title":"Finch.DeviceGlobal","text":"DeviceGlobal()\n\nFrom the device, load the global view of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/#Finch.DeviceShared","page":"Parallel Processing in Finch","title":"Finch.DeviceShared","text":"DeviceShared()\n\nFrom the device, load the shared view of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"The transfer function is used to distribute tensors and their constituent buffers to different memory spaces.  We can ask for the default local, shared, or global memory spaces of an AbstractDevice with the local_memory, etc. trait functions.","category":"page"},{"location":"docs/internals/parallel/","page":"Parallel Processing in Finch","title":"Parallel Processing in Finch","text":"transfer\nlocal_memory\nshared_memory\nglobal_memory","category":"page"},{"location":"docs/internals/parallel/#Finch.transfer","page":"Parallel Processing in Finch","title":"Finch.transfer","text":"transfer(device, arr)\n\nIf the array is not on the given device, it creates a new version of this array on that device and copies the data in to it, according to the device trait. If the device is simply a data buffer, we copy the array into the buffer.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.local_memory","page":"Parallel Processing in Finch","title":"Finch.local_memory","text":"local_memory(dev::AbstractDevice)\n\nReturn the default local memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.shared_memory","page":"Parallel Processing in Finch","title":"Finch.shared_memory","text":"shared_memory(dev::AbstractDevice)\n\nReturn the default shared memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/parallel/#Finch.global_memory","page":"Parallel Processing in Finch","title":"Finch.global_memory","text":"global_memory(dev::AbstractDevice)\n\nReturn the default global memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"appendices/glossary/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"appendices/changelog/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/compiler_interface/#Compiler-Internals","page":"Compiler Interfaces","title":"Compiler Internals","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"Finch has several compiler modules with separate interfaces.","category":"page"},{"location":"docs/internals/compiler_interface/#SymbolicContexts","page":"Compiler Interfaces","title":"SymbolicContexts","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"SymbolicContexts are used to represent the symbolic information of a program. They are used to reason about the bounds of loops and the symbolic information of the program, and are defined on an algebra","category":"page"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"StaticHash\nSymbolicContext\nget_algebra\nget_static_hash\nprove\nsimplify","category":"page"},{"location":"docs/internals/compiler_interface/#Finch.StaticHash","page":"Compiler Interfaces","title":"Finch.StaticHash","text":"StaticHash\n\nA hash function which is static, i.e. the hashes are the same when objects are hashed in the same order. The hash is used to memoize the results of simplification and proof rules.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.SymbolicContext","page":"Compiler Interfaces","title":"Finch.SymbolicContext","text":"SymbolicContext\n\nA compiler context for symbolic computation, defined on an algebra.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.get_algebra","page":"Compiler Interfaces","title":"Finch.get_algebra","text":"get_algebra(ctx)\n\nget the algebra used in the current context\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.get_static_hash","page":"Compiler Interfaces","title":"Finch.get_static_hash","text":"get_static_hash(ctx)\n\nReturn an object which can be called as a hash function. The hashes are the same when objects are hashed in the same order.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.prove","page":"Compiler Interfaces","title":"Finch.prove","text":"prove(ctx, root; verbose = false)\n\nuse the rules in ctx to attempt to prove that the program root is true. Return false if the program cannot be shown to be true.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.simplify","page":"Compiler Interfaces","title":"Finch.simplify","text":"simplify(ctx, node)\n\nsimplify the program node using the rules in ctx\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#ScopeContexts","page":"Compiler Interfaces","title":"ScopeContexts","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"ScopeContexts are used to represent the scope of a program. They are used to reason about values bound to variables and also the modes of tensor variables.","category":"page"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"ScopeContext\nget_binding\nhas_binding\nset_binding!\nset_declared!\nset_frozen!\nset_thawed!\nget_tensor_mode\nopen_scope","category":"page"},{"location":"docs/internals/compiler_interface/#Finch.ScopeContext","page":"Compiler Interfaces","title":"Finch.ScopeContext","text":"ScopeContext\n\nA context for managing variable bindings and tensor modes.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.get_binding","page":"Compiler Interfaces","title":"Finch.get_binding","text":"get_binding(ctx, var)\n\nGet the binding of a variable in the context.\n\n\n\n\n\nget_binding(ctx, var, val)\n\nGet the binding of a variable in the context, or return a default value.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.has_binding","page":"Compiler Interfaces","title":"Finch.has_binding","text":"has_binding(ctx, var)\n\nCheck if a variable is bound in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.set_binding!","page":"Compiler Interfaces","title":"Finch.set_binding!","text":"set_binding!(ctx, var, val)\n\nSet the binding of a variable in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.set_declared!","page":"Compiler Interfaces","title":"Finch.set_declared!","text":"set_declared!(ctx, var, val, op)\n\nMark a tensor variable as declared in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.set_frozen!","page":"Compiler Interfaces","title":"Finch.set_frozen!","text":"set_frozen!(ctx, var, val)\n\nMark a tensor variable as frozen in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.set_thawed!","page":"Compiler Interfaces","title":"Finch.set_thawed!","text":"set_thawed!(ctx, var, val, op)\n\nMark a tensor variable as thawed in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.get_tensor_mode","page":"Compiler Interfaces","title":"Finch.get_tensor_mode","text":"get_tensor_mode(ctx, var)\n\nGet the mode of a tensor variable in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.open_scope","page":"Compiler Interfaces","title":"Finch.open_scope","text":"open_scope(f, ctx)\n\nCall the function f(ctx_2) in a new scope ctx_2.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#JuliaContexts","page":"Compiler Interfaces","title":"JuliaContexts","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"JuliaContexts are used to represent the execution environment of a program, including variables and tasks. They are used to generate code.","category":"page"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"Namespace\nJuliaContext\npush_preamble!\npush_epilogue!\nget_task\nfreshen\ncontain","category":"page"},{"location":"docs/internals/compiler_interface/#Finch.Namespace","page":"Compiler Interfaces","title":"Finch.Namespace","text":"Namespace\n\nA namespace for managing variable names and aesthetic fresh variable generation.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.JuliaContext","page":"Compiler Interfaces","title":"Finch.JuliaContext","text":"JuliaContext\n\nA context for compiling Julia code, managing side effects, parallelism, and variable names in the generated code of the executing environment.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.push_preamble!","page":"Compiler Interfaces","title":"Finch.push_preamble!","text":"push_preamble!(ctx, thunk)\n\nPush the thunk onto the preamble in the currently executing context. The preamble will be evaluated before the code returned by the given function in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.push_epilogue!","page":"Compiler Interfaces","title":"Finch.push_epilogue!","text":"push_epilogue!(ctx, thunk)\n\nPush the thunk onto the epilogue in the currently executing context. The epilogue will be evaluated after the code returned by the given function in the context.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.get_task","page":"Compiler Interfaces","title":"Finch.get_task","text":"get_task(ctx)\n\nGet the task which will execute code in this context\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.freshen","page":"Compiler Interfaces","title":"Finch.freshen","text":"freshen(ctx, tags...)\n\nReturn a fresh variable in the current context named after Symbol(tags...)\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.contain","page":"Compiler Interfaces","title":"Finch.contain","text":"contain(f, ctx)\n\nCall f on a subcontext of ctx and return the result. Variable bindings, preambles, and epilogues defined in the subcontext will not escape the call to contain.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#AbstractCompiler","page":"Compiler Interfaces","title":"AbstractCompiler","text":"","category":"section"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"The AbstractCompiler interface requires all of the functionality of the above contexts, as well as the following two methods:","category":"page"},{"location":"docs/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"FinchCompiler\nget_result\nget_mode_flag","category":"page"},{"location":"docs/internals/compiler_interface/#Finch.FinchCompiler","page":"Compiler Interfaces","title":"Finch.FinchCompiler","text":"FinchCompiler\n\nThe core compiler for Finch, lowering canonicalized Finch IR to Julia code.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/compiler_interface/#Finch.get_result","page":"Compiler Interfaces","title":"Finch.get_result","text":"get_result(ctx)\n\nReturn a variable which evaluates to the result of the program which should be returned to the user.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/compiler_interface/#Finch.get_mode_flag","page":"Compiler Interfaces","title":"Finch.get_mode_flag","text":"get_mode_flag(ctx)\n\nReturn the mode flag given in @finch mode = ?.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/tensor_interface/#Tensor-Interface","page":"Tensor Interface","title":"Tensor Interface","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The AbstractTensor interface (defined in src/abstract_tensor.jl) is the interface through which Finch understands tensors. It is a high-level interace which allows tensors to interact with the rest of the Finch system. The interface is designed to be extensible, allowing users to define their own tensor types and behaviors. For a minimal example, read the definitions in /ext/SparseArraysExt.jl and in /src/interface/abstractarray.jl. Once these methods are defined that tell Finch how to generate code for an array, the AbstractTensor interface will also use Finch to generate code for several Julia AbstractArray methods, such as getindex, setindex!, map, and reduce. An important note: getindex and setindex! are not a source of truth for Finch tensors. Search the codebase for ::AbstractTensor for a full list of methods that are implemented for AbstractTensor. Note than most AbstractTensor implement labelled_show and labelled_children methods instead of show(::IO, ::MIME\"text/plain\", t::AbstractTensor) for pretty printed display.","category":"page"},{"location":"docs/internals/tensor_interface/#Tensor-Methods","page":"Tensor Interface","title":"Tensor Methods","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"declare!\nfreeze!\nthaw!\nunfurl\ninstantiate\nvirtual_eltype\nvirtual_fill_value\nvirtual_size\nvirtual_resize!\nlabelled_show\nlabelled_children\nis_injective\nis_atomic\nis_concurrent","category":"page"},{"location":"docs/internals/tensor_interface/#Finch.declare!","page":"Tensor Interface","title":"Finch.declare!","text":"declare!(ctx, tns, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.freeze!","page":"Tensor Interface","title":"Finch.freeze!","text":"freeze!(ctx, tns)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. This may involve trimming any excess overallocated memory.  Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.thaw!","page":"Tensor Interface","title":"Finch.thaw!","text":"thaw!(ctx, tns)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.unfurl","page":"Tensor Interface","title":"Finch.unfurl","text":"unfurl(ctx, tns, ext, proto)\n\nReturn an array object (usually a looplet nest) for lowering the outermost dimension of virtual tensor tns. ext is the extent of the looplet. proto is the protocol that should be used for this index, but one doesn't need to unfurl all the indices at once.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.instantiate","page":"Tensor Interface","title":"Finch.instantiate","text":"instantiate(ctx, tns, mode)\n\nProcess the tensor tns in the context ctx, just after it has been unfurled, declared, or thawed. The earliest opportunity to process tns.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.virtual_eltype","page":"Tensor Interface","title":"Finch.virtual_eltype","text":"virtual_eltype(arr)\n\nReturn the element type of the virtual tensor arr.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.virtual_fill_value","page":"Tensor Interface","title":"Finch.virtual_fill_value","text":"virtual fill_value(arr)\n\nReturn the initializer for virtual array arr.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.virtual_size","page":"Tensor Interface","title":"Finch.virtual_size","text":"virtual_size(ctx, tns)\n\nReturn a tuple of the dimensions of tns in the context ctx. This is a function similar in spirit to Base.axes.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.virtual_resize!","page":"Tensor Interface","title":"Finch.virtual_resize!","text":"virtual_resize!(ctx, tns, dims...)\n\nResize tns in the context ctx. This is a function similar in spirit to Base.resize!.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.labelled_show","page":"Tensor Interface","title":"Finch.labelled_show","text":"labelled_show(node)\n\nShow the node in a LabelledTree.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.labelled_children","page":"Tensor Interface","title":"Finch.labelled_children","text":"labelled_children(node)\n\nReturn the children of node in a LabelledTree. You may label the children by returning a LabelledTree(key, value), which will be shown as key: value a.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.is_injective","page":"Tensor Interface","title":"Finch.is_injective","text":"is_injective(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating whether the access is injective in that dimension.  A dimension is injective if each index in that dimension maps to a different memory space in the underlying array.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.is_atomic","page":"Tensor Interface","title":"Finch.is_atomic","text":"is_atomic(ctx, tns)\n\nReturns a tuple (atomicities, overall) where atomicities is a vector, indicating which indices have an atomic that guards them,\nand overall is a boolean that indicates is the last level had an atomic guarding it.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.is_concurrent","page":"Tensor Interface","title":"Finch.is_concurrent","text":"is_concurrent(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating\nwhether the index can be written to without any execution state. So if a matrix returns [true, false],\nthen we can write to A[i, j] and A[i_2, j] without any shared execution state between the two, but\nwe can't write to A[i, j] and A[i, j_2] without carrying over execution state.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Level-Interface","page":"Tensor Interface","title":"Level Interface","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> A_fbr = Tensor(Dense(Dense(Element(0.0))), A)\n4×3 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: Dense [1:4]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: Dense [1:4]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 0.0\n   │  ├─ [3]: 0.0\n   │  └─ [4]: 0.0\n   └─ [:, 3]: Dense [1:4]\n      ├─ [1]: 4.4\n      ├─ [2]: 0.0\n      ├─ [3]: 5.5\n      └─ [4]: 0.0","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"We refer to a node in the tree as a subfiber. All of the nodes at the same level are stored in the same datastructure, and disambiguated by an integer position.  in the above example, there are three levels: the rootmost level contains only one subfiber, the root. The middle level has 3 subfibers, one for each column. The leafmost level has 12 subfibers, one for each element of the array.  For example, the first level is A_fbr.lvl, and we can represent it's third position as SubFiber(A_fbr.lvl.lvl, 3). The second level is A_fbr.lvl.lvl, and we can access it's 9th position as SubFiber(A_fbr.lvl.lvl.lvl, 9). For instructional purposes, you can use parentheses to call a subfiber on an index to select among children of a subfiber.","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> tensor_tree(Finch.SubFiber(A_fbr.lvl.lvl, 3))\nDense [1:4]\n├─ [1]: 4.4\n├─ [2]: 0.0\n├─ [3]: 5.5\n└─ [4]: 0.0\n\njulia> tensor_tree(A_fbr[:, 3])\n4-Tensor\n└─ Dense [1:4]\n   ├─ [1]: 4.4\n   ├─ [2]: 0.0\n   ├─ [3]: 5.5\n   └─ [4]: 0.0\n\njulia> tensor_tree(A_fbr(3))\nDense [1:4]\n├─ [1]: 4.4\n├─ [2]: 0.0\n├─ [3]: 5.5\n└─ [4]: 0.0\n\njulia> Finch.SubFiber(A_fbr.lvl.lvl.lvl, 9)\n Finch.SubFiber{ElementLevel{0.0, Float64, Int64, Vector{Float64}}, Int64}:\n4.4\n\njulia> A_fbr[1, 3]\n4.4\n\njulia> A_fbr(3)(1)\n4.4","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"When we print the tree in text, positions are numbered from top to bottom.","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Because our array is sparse, (mostly zero, or another fill value), it would be more efficient to store only the nonzero values. In Finch, each level is represented with a different format. A sparse level only stores non-fill values. This time, we'll use a tensor constructor with sl (for \"SparseList of nonzeros\") instead of d (for \"Dense\"):","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), A)\n4×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Our Dense(SparseList(Element(0.0))) format is also known as \"CSC\" and is equivalent to SparseMatrixCSC. The Tensor function will perform a zero-cost copy between Finch fibers and sparse matrices, when available.  CSC is an excellent general-purpose representation when we expect most of the columns to have a few nonzeros. However, when most of the columns are entirely fill (a situation known as hypersparsity), it is better to compress the root level as well:","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(SparseList(SparseList(Element(0.0))), A)\n4×3 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Here we see that the entirely zero column has also been compressed. The SparseList(SparseList(Element(0.0))) format is also known as \"DCSC\".","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The \"COO\" (or \"Coordinate\") format is often used in practice for ease of interchange between libraries. In an N-dimensional array A, COO stores N lists of indices I_1, ..., I_N where A[I_1[p], ..., I_N[p]] is the p^th stored value in column-major numbering. In Finch, COO is represented as a multi-index level, which can handle more than one index at once. We use curly brackets to declare the number of indices handled by the level:","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(SparseCOO{2}(Element(0.0)), A)\n4×3 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [2, 1]: 1.1\n   ├─ [3, 1]: 2.2\n   ├─ ⋮\n   ├─ [1, 3]: 4.4\n   └─ [3, 3]: 5.5","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The COO format is compact and straightforward, but doesn't support random access. For random access, one should use the SparseDict or SparseBytemap format. A full listing of supported formats is described after a rough description of shared common internals of level, relating to types and storage.","category":"page"},{"location":"docs/internals/tensor_interface/#Types-and-Storage-of-Level","page":"Tensor Interface","title":"Types and Storage of Level","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"All levels have a postype, typically denoted as Tp in the constructors, used for internal pointer types but accessible by the function:","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"postype","category":"page"},{"location":"docs/internals/tensor_interface/#Finch.postype","page":"Tensor Interface","title":"Finch.postype","text":"postype(lvl)\n\nReturn a position type with the same flavor as those used to store the positions of the fibers contained in lvl. The name position descends from the pos or position or pointer arrays found in many definitions of CSR or CSC. In Finch, positions should be data used to access either a subfiber or some other similar auxiliary data. Thus, we often end up iterating over positions.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Additionally, many levels have a Vp or Vi in their constructors; these stand for vector of element type Tp or Ti. More generally, levels are paramterized by the types that they use for storage. By default, all levels use Vector, but a user could could change any or all of the storage types of a tensor so that the tensor would be stored on a GPU or CPU or some combination thereof, or even just via a vector with a different allocation mechanism.  The storage type should behave like AbstractArray and needs to implement the usual abstract array functions and Base.resize!. See the tests for an example.","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"When levels are constructed in short form as in the examples above, the index, position, and storage types are inferred from the level below. All the levels at the bottom of a Tensor (Element, Pattern, Repeater) specify an index type, position type, and storage type even if they don't need them. These are used by levels that take these as parameters.","category":"page"},{"location":"docs/internals/tensor_interface/#Level-Methods","page":"Tensor Interface","title":"Level Methods","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Tensor levels are implemented using the following methods:","category":"page"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"declare_level!\nassemble_level!\nreassemble_level!\nfreeze_level!\nlevel_ndims\nlevel_size\nlevel_axes\nlevel_eltype\nlevel_fill_value","category":"page"},{"location":"docs/internals/tensor_interface/#Finch.declare_level!","page":"Tensor Interface","title":"Finch.declare_level!","text":"declare_level!(ctx, lvl, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.assemble_level!","page":"Tensor Interface","title":"Finch.assemble_level!","text":"assemble_level!(ctx, lvl, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.reassemble_level!","page":"Tensor Interface","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_fill_value(lvl).  Not avaliable on all level types as this presumes updating.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.freeze_level!","page":"Tensor Interface","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, freeze all fibers within lvl assuming that we have potentially updated 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.level_ndims","page":"Tensor Interface","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.level_size","page":"Tensor Interface","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.level_axes","page":"Tensor Interface","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.level_eltype","page":"Tensor Interface","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Finch.level_fill_value","page":"Tensor Interface","title":"Finch.level_fill_value","text":"level_fill_value(::Type{Lvl})\n\nThe result of level_fill_value(Lvl) defines fill_value for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/tensor_interface/#Combinator-Interface","page":"Tensor Interface","title":"Combinator Interface","text":"","category":"section"},{"location":"docs/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Tensor Combinators allow us to modify the behavior of tensors. The AbstractCombinator interface (defined in src/tensors/abstract_combinator.jl) is the interface through which Finch understands tensor combinators. The interface requires the combinator to overload all of the tensor methods, as well as the methods used by Looplets when lowering ranges, etc. For a minimal example, read the definitions in /src/tensors/combinators/offset.jl.","category":"page"},{"location":"docs/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"CurrentModule = Finch","category":"page"},{"location":"docs/internals/finch_notation/#Finch-Notation-Internals","page":"Finch Notation","title":"Finch Notation Internals","text":"","category":"section"},{"location":"docs/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"Finch IR is a tree structure that represents a finch program. Different types of nodes are delineated by a FinchKind enum, for type stability. There are a few useful functions to be aware of:","category":"page"},{"location":"docs/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"FinchNode\ncached\nfinch_leaf\nFinchNotation.isstateful\nisliteral\nisvalue\nisconstant\nisvirtual\nisvariable\nisindex","category":"page"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.FinchNode","page":"Finch Notation","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node, used to represent an imperative, physical Finch program.\n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.cached","page":"Finch Notation","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.finch_leaf","page":"Finch Notation","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isstateful","page":"Finch Notation","title":"Finch.FinchNotation.isstateful","text":"isstateful(node)\n\nReturns true if the node is a finch statement, and false if the node is an index expression. Typically, statements specify control flow and expressions describe values.\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isliteral","page":"Finch Notation","title":"Finch.FinchNotation.isliteral","text":"isliteral(node)\n\nReturns true if the node is a finch literal\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isvalue","page":"Finch Notation","title":"Finch.FinchNotation.isvalue","text":"isvalue(node)\n\nReturns true if the node is a finch value\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isconstant","page":"Finch Notation","title":"Finch.FinchNotation.isconstant","text":"isconstant(node)\n\nReturns true if the node can be expected to be constant within the current finch context\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isvirtual","page":"Finch Notation","title":"Finch.FinchNotation.isvirtual","text":"isvirtual(node)\n\nReturns true if the node is a finch virtual\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isvariable","page":"Finch Notation","title":"Finch.FinchNotation.isvariable","text":"isvariable(node)\n\nReturns true if the node is a finch variable\n\n\n\n\n\n","category":"function"},{"location":"docs/internals/finch_notation/#Finch.FinchNotation.isindex","page":"Finch Notation","title":"Finch.FinchNotation.isindex","text":"isindex(node)\n\nReturns true if the node is a finch index\n\n\n\n\n\n","category":"function"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"CurrentModule = Finch","category":"page"},{"location":"docs/language/calling_finch/#Finch-Language","page":"Calling Finch","title":"Finch Language","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Writing Finch language directly is a powerful way to express complex array operations. Finch supports complex optimizations under the hood, including conditionals, multiple outputs, and even user-defined types and functions.","category":"page"},{"location":"docs/language/calling_finch/#Supported-Syntax-and-Structures","page":"Calling Finch","title":"Supported Syntax and Structures","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Feature/Structure Example Usage\nMajor Sparse Formats and Structured Arrays A = Tensor(Dense(SparseList(Element(0.0)), 3, 4)\nBackground Values Other Than Zero B = Tensor(SparseList(Element(1.0)), 9)\nBroadcasts and Reductions sum(A .* B)\nUser-Defined Functions x[] <<min>>= y[i] + z[i]\nMultiple Outputs x[] <<min>>= y[i]; z[] <<max>>= y[i]\nMulticore Parallelism for i = parallel(1:100)\nConditionals if dist[] < best_dist[]\nAffine Indexing (e.g. Convolution) A[i + j]","category":"page"},{"location":"docs/language/calling_finch/#Quick-Start:-Examples","page":"Calling Finch","title":"Quick Start: Examples","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"To begin, the following program sums the rows of a sparse matrix:","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"using Finch\nA = sprand(5, 5, 0.5)\ny = zeros(5)\n@finch begin\n    y .= 0\n    for i in _, j in _\n        y[i] += A[i, j]\n    end\nend","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"The @finch macro takes a block of code, and compiles it using the sparsity attributes of the arguments. In this case, A is a sparse matrix, so the compiler generates a sparse loop nest. The compiler takes care of applying rules like x * 0 => 0 during compilation to make the code more efficient.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"You can call @finch on any loop program, but it will only generate sparse code if the arguments are sparse. For example, the following program calculates the sum of the elements of a dense matrix:","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"using Finch\nA = rand(5, 5)\ns = Scalar(0.0)\n@finch begin\n    s .= 0\n    for i in _, j in _\n        s[] += A[i, j]\n    end\nend","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"You can call @finch_code to see the generated code (since A is dense, the code is dense):","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"julia> @finch_code for i in _, j in _\n           s[] += A[i, j]\n       end\nquote\n    s_data = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s_data.val\n    A_data = (ex.bodies[1]).body.body.rhs.tns.bind\n    sugar_1 = size((ex.bodies[1]).body.body.rhs.tns.bind)\n    A_mode1_stop = sugar_1[1]\n    A_mode2_stop = sugar_1[2]\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_mode1_stop\n        for j_3 = 1:A_mode2_stop\n            val = A_data[i_3, j_3]\n            s_val = val + s_val\n        end\n    end\n    result = ()\n    s_data.val = s_val\n    result\nend","category":"page"},{"location":"docs/language/calling_finch/#Calculating-Sparse-Vector-Statistics","page":"Calling Finch","title":"Calculating Sparse Vector Statistics","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Here, we write a Julia program using Finch to compute the minimum, maximum, sum, and variance of a sparse vector. This program efficiently reads the vector once, focusing only on nonzero values.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"using Finch\n\nX = Tensor(SparseList(Element(0.0)), fsprand(10, 0.5))\nx_min = Scalar(Inf)\nx_max = Scalar(-Inf)\nx_sum = Scalar(0.0)\nx_var = Scalar(0.0)\n\n@finch begin\n    for i in _\n        let x = X[i]\n            x_min[] << min >>= x\n            x_max[] << max >>= x\n            x_sum[] += x\n            x_var[] += x * x\n        end\n    end\nend;","category":"page"},{"location":"docs/language/calling_finch/#Sparse-Matrix-Vector-Multiplication","page":"Calling Finch","title":"Sparse Matrix-Vector Multiplication","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"As a more traditional example, what follows is a sparse matrix-vector multiplication using a column-major approach.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"x = Tensor(Dense(Element(0.0)), rand(42));\nA = Tensor(Dense(SparseList(Element(0.0))), fsprand(42, 42, 0.1));\ny = Tensor(Dense(Element(0.0)));\n\n@finch begin\n    y .= 0\n    for j in _, i in _\n        y[i] += A[i, j] * x[j]\n    end\nend","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"More examples are given in the examples directory.","category":"page"},{"location":"docs/language/calling_finch/#Usage","page":"Calling Finch","title":"Usage","text":"","category":"section"},{"location":"docs/language/calling_finch/#General-Purpose-(@finch)","page":"Calling Finch","title":"General Purpose (@finch)","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Most users will want to use the @finch macro, which executes the given program immediately in the given scope. The program will be JIT-compiled on the first call to @finch with the given array argument types. If the array arguments to @finch are type stable, the program will be JIT-compiled when the surrounding function is compiled.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Very often, the best way to inspect Finch compiler behavior is through the @finch_code macro, which prints the generated code instead of executing it.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"@finch\n@finch_code","category":"page"},{"location":"docs/language/calling_finch/#Finch.@finch","page":"Calling Finch","title":"Finch.@finch","text":"@finch [options...] prgm\n\nRun a finch program prgm. The syntax for a finch program is a set of nested loops, statements, and branches over pointwise array assignments. For example, the following program computes the sum of two arrays A = B + C:\n\n@finch begin\n    A .= 0\n    for i = _\n        A[i] = B[i] + C[i]\n    end\n    return A\nend\n\nFinch programs are composed using the following syntax:\n\narr .= 0: an array declaration initializing arr to zero.\narr[inds...]: an array access, the array must be a variable and each index may be another finch expression.\nx + y, f(x, y): function calls, where x and y are finch expressions.\narr[inds...] = ex: an array assignment expression, setting arr[inds] to the value of ex.\narr[inds...] += ex: an incrementing array expression, adding ex to arr[inds]. *, &, |, are supported.\narr[inds...] <<min>>= ex: a incrementing array expression with a custom operator, e.g. <<min>> is the minimum operator.\nfor i = _ body end: a loop over the index i, where _ is computed from array access with i in body.\nif cond body end: a conditional branch that executes only iterations where cond is true.\nreturn (tnss...,): at global scope, exit the program and return the tensors tnss with their new dimensions. By default, any tensor declared in global scope is returned.\n\nSymbols are used to represent variables, and their values are taken from the environment. Loops introduce index variables into the scope of their bodies.\n\nFinch uses the types of the arrays and symbolic analysis to discover program optimizations. If B and C are sparse array types, the program will only run over the nonzeros of either.\n\nSemantically, Finch programs execute every iteration. However, Finch can use sparsity information to reliably skip iterations when possible.\n\noptions are optional keyword arguments:\n\nalgebra: the algebra to use for the program. The default is DefaultAlgebra().\nmode: the optimization mode to use for the program. Possible modes are:\n:debug: run the program in debug mode, with bounds checking and better error handling.\n:safe: run the program in safe mode, with modest checks for performance and correctness.\n:fast: run the program in fast mode, with no checks or warnings, this mode is for power users.\nThe default is :safe.\n\nSee also: @finch_code\n\n\n\n\n\n","category":"macro"},{"location":"docs/language/calling_finch/#Finch.@finch_code","page":"Calling Finch","title":"Finch.@finch_code","text":"@finch_code [options...] prgm\n\nReturn the code that would be executed in order to run a finch program prgm.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"docs/language/calling_finch/#Ahead-Of-Time-(@finch_kernel)","page":"Calling Finch","title":"Ahead Of Time (@finch_kernel)","text":"","category":"section"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"While @finch is the recommended way to use Finch, it is also possible to run finch ahead-of-time. The @finch_kernel macro generates a function definition ahead-of-time, which can be evaluated and then called later.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"There are several reasons one might want to do this:","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"If we want to make tweaks to the Finch implementation, we can directly modify the source code of the resulting function.\nWhen benchmarking Finch functions, we can easily and reliably ensure the benchmarked code is inferrable.\nIf we want to use Finch to generate code but don't want to include Finch as a dependency in our project, we can use @finch_kernel to generate the functions ahead of time and copy and paste the generated code into our project.  Consider automating this workflow to keep the kernels up to date!","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"    @finch_kernel","category":"page"},{"location":"docs/language/calling_finch/#Finch.@finch_kernel","page":"Calling Finch","title":"Finch.@finch_kernel","text":"@finch_kernel [options...] fname(args...) = prgm\n\nReturn a definition for a function named fname which executes @finch prgm on the arguments args. args should be a list of variables holding representative argument instances.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"As an example, the following code generates an spmv kernel definition, evaluates the definition, and then calls the kernel several times.","category":"page"},{"location":"docs/language/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"let\n    A = Tensor(Dense(SparseList(Element(0.0))))\n    x = Tensor(Dense(Element(0.0)))\n    y = Tensor(Dense(Element(0.0)))\n    def = @finch_kernel function spmv(y, A, x)\n        y .= 0.0\n        for j in _, i in _\n            y[i] += A[i, j] * x[j]\n        end\n        return y\n    end\n    eval(def)\nend\n\nfunction main()\n    for i in 1:10\n        A2 = Tensor(Dense(SparseList(Element(0.0))), fsprand(10, 10, 0.1))\n        x2 = Tensor(Dense(Element(0.0)), rand(10))\n        y2 = Tensor(Dense(Element(0.0)))\n        spmv(y2, A2, x2)\n    end\nend\n\nmain()","category":"page"},{"location":"docs/language/debugging_tips/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"appendices/directory_structure/#Directory-Structure","page":"Directory Structure","title":"Directory Structure","text":"","category":"section"},{"location":"appendices/directory_structure/","page":"Directory Structure","title":"Directory Structure","text":"Here's a little roadmap to the Finch codebase! Please file an issue if this is not up to date.","category":"page"},{"location":"appendices/directory_structure/","page":"Directory Structure","title":"Directory Structure","text":".\n├── benchmark                  # benchmarks for internal use\n│   ├── runbenchmarks.jl       # run benchmarks\n│   ├── runjudge.jl            # run benchmarks on current branch and compare with main\n│   └── ...\n├── docs                       # documentation\n│   ├── [build]                # rendered docs website\n│   ├── src                    # docs website source\n│   ├── fix.jl                 # fix docstrings\n│   ├── examples               # example applications implemented in Finch!\n│   │   └── ...\n│   ├── make.jl                # build documentation locally\n│   └── ...\n├── ext                        # conditionally-loaded code for interaction with other packages (e.g. SparseArrays)\n├── src                        # Source files\n│   ├── interface              # Implementations of array api functions (e.g. map, reduce, etc.)\n│   │   ├── fileio             # File IO function definitions\n│   │   └── ...\n│   ├── FinchLogic             # SubModule containing the High-Level IR\n│   │   ├── nodes.jl           # defines the High-Level IR\n│   │   └── ...\n│   ├── scheduler              # Auto-Scheduler to compile High-Level IR to Finch IR\n│   │   └── ...\n│   ├── FinchNotation          # SubModule containing the Finch IR\n│   │   ├── nodes.jl           # defines the Finch IR\n│   │   ├── syntax.jl          # defines the @finch frontend syntax\n│   │   └── ...\n│   ├── looplets               # this is where all the Looplets live\n│   ├── symbolic               # term rewriting systems for program and bounds\n│   ├── tensors                # built-in Finch tensor definitions\n│   │   ├── levels             # all of the levels\n│   │   │   └── ...\n│   │   ├── combinators        # tensor combinators which modify tensor behavior\n│   │   │   └── ...\n│   │   ├── fibers.jl          # fibers combine levels to form tensors\n│   │   ├── scalars.jl         # a nice scalar type\n│   │   └── masks.jl           # mask tensors (e.g. upper-triangular mask)\n│   ├── transformations        # global program transformations\n│   │   ├── scopes.jl          # gives unique names to indices\n│   │   ├── lifetimes.jl       # adds freeze and thaw\n│   │   ├── dimensionalize.jl  # computes extents for loops and declarations\n│   │   ├── concordize.jl      # adds loops to ensure all accesses are concordant\n│   │   └── wrapperize.jl      # converts index expressions to array wrappers\n│   ├── abstract_tensor.jl     # finch array interface functions\n│   ├── execute.jl             # global compiler calls\n│   ├── lower.jl               # inner compiler definition\n│   ├── util                   # shims and julia codegen utils (Dead code elimination, etc...)\n│   │   └── ...\n│   └── ...\n├── test                       # tests\n│   ├──  suites                # test suite files, each suite can run independently\n│   ├──  reference32           # reference output for 32-bit systems\n│   ├──  reference64           # reference output for 64-bit systems\n│   ├──  runtests.jl           # runs the test suite. (pass -h for options and more info!)\n│   └── ...\n├── Project.toml               # julia-readable listing of project dependencies\n├── [Manifest.toml]            # local listing of installed dependencies (don't commit this)\n├── LICENSE\n├── CONTRIBUTING.md\n└── README.md","category":"page"},{"location":"docs/language/iteration_protocols/#Iteration-Protocols","page":"Iteration Protocols","title":"Iteration Protocols","text":"","category":"section"},{"location":"docs/language/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"Finch is a flexible tensor compiler with many ways to iterate over the same data. For example, consider the case where we are intersecting two sparse vectors x[i] and y[i]. By default, we would iterate over all of the nonzeros of each vector. However, if we want to skip over the nonzeros in y based on the nonzeros in x, we could declare the tensor x as the leader tensor with an x[gallop(i)] protocol. When x leads the iteration, the generated code uses the nonzeros of x as an outer loop and the nonzeros of y as an inner loop. If we know that the nonzero datastructure of y supports efficient random access, we might ask to iterate over y with a y[follow(i)] protocol, where we look up each value of y[i] only when x[i] is nonzero.","category":"page"},{"location":"docs/language/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"Finch supports several iteration protocols, documented below. Note that not all formats support all protocols, consult the documentation for each format to figure out which protocols are supported.","category":"page"},{"location":"docs/language/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"follow\nwalk\ngallop\nextrude\nlaminate","category":"page"},{"location":"docs/language/iteration_protocols/#Finch.FinchNotation.follow","page":"Iteration Protocols","title":"Finch.FinchNotation.follow","text":"follow(i)\n\nThe follow protocol ignores the structure of the tensor. By itself, the follow protocol iterates over each value of the tensor in order, looking it up with random access.  The follow protocol may specialize on e.g. the zero value of the tensor, but does not specialize on the structure of the tensor. This enables efficient random access and avoids large code sizes.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/iteration_protocols/#Finch.FinchNotation.walk","page":"Iteration Protocols","title":"Finch.FinchNotation.walk","text":"walk(i)\n\nThe walk protocol usually iterates over each pattern element of a tensor in order. Note that the walk protocol \"imposes\" the structure of its argument on the kernel, so that we specialize the kernel to the structure of the tensor.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/iteration_protocols/#Finch.FinchNotation.gallop","page":"Iteration Protocols","title":"Finch.FinchNotation.gallop","text":"gallop(i)\n\nThe gallop protocol iterates over each pattern element of a tensor, leading the iteration and superceding the priority of other tensors. Mutual leading is possible, where we fast-forward to the largest step between either leader.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/iteration_protocols/#Finch.FinchNotation.extrude","page":"Iteration Protocols","title":"Finch.FinchNotation.extrude","text":"extrude(i)\n\nThe extrude protocol declares that the tensor update happens in order and only once, so that reduction loops occur below the extrude loop. It is not usually necessary to declare an extrude protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"function"},{"location":"docs/language/iteration_protocols/#Finch.FinchNotation.laminate","page":"Iteration Protocols","title":"Finch.FinchNotation.laminate","text":"laminate(i)\n\nThe laminate protocol declares that the tensor update may happen out of order and multiple times. It is not usually necessary to declare a laminate protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"CurrentModule = Finch","category":"page"},{"location":"docs/fileio/#Finch-Tensor-File-Input/Output","page":"FileIO","title":"Finch Tensor File Input/Output","text":"","category":"section"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"All of the file formats supported by Finch are listed below. Each format has a corresponding read and write function, and can be selected automatically based on the file extension with the following functions:","category":"page"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"fread\nfwrite","category":"page"},{"location":"docs/fileio/#Finch.fread","page":"FileIO","title":"Finch.fread","text":"fread(filename::AbstractString)\n\nRead the Finch tensor from a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#Finch.fwrite","page":"FileIO","title":"Finch.fwrite","text":"fwrite(filename::AbstractString, tns::Finch.Tensor)\n\nWrite the Finch tensor to a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#Binsparse-Format-(.bsp)","page":"FileIO","title":"Binsparse Format (.bsp)","text":"","category":"section"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the most recent revision of the Binsparse binary sparse tensor format, including the v2.0 tensor extension. This is a good option for those who want an efficient way to transfer sparse tensors between supporting libraries and languages. The Binsparse format represents the tensor format as a JSON string in the underlying data container, which can be either HDF5 or a combination of NPY or JSON files.  Binsparse arrays are stored 0-indexed.","category":"page"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"bspwrite\nbspread","category":"page"},{"location":"docs/fileio/#Finch.bspwrite","page":"FileIO","title":"Finch.bspwrite","text":"bspwrite(::AbstractString, tns)\nbspwrite(::HDF5.File, tns)\nbspwrite(::NPYPath, tns)\n\nWrite the Finch tensor to a file using Binsparse file format.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#Finch.bspread","page":"FileIO","title":"Finch.bspread","text":"bspread(::AbstractString) bspread(::HDF5.File) bspread(::NPYPath)\n\nRead the Binsparse file into a Finch tensor.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\n\n\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#TensorMarket-(.mtx,-.ttx)","page":"FileIO","title":"TensorMarket (.mtx, .ttx)","text":"","category":"section"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the MatrixMarket and TensorMarket formats, which prioritize readability and archiveability, storing matrices and tensors in plaintext.","category":"page"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"fttwrite\nfttread","category":"page"},{"location":"docs/fileio/#Finch.fttwrite","page":"FileIO","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch tensor to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#Finch.fttread","page":"FileIO","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch tensor. The tensor will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#FROSTT-(.tns)","page":"FileIO","title":"FROSTT (.tns)","text":"","category":"section"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the FROSTT format for legacy codes that still use it.","category":"page"},{"location":"docs/fileio/","page":"FileIO","title":"FileIO","text":"ftnswrite\nftnsread","category":"page"},{"location":"docs/fileio/#Finch.ftnswrite","page":"FileIO","title":"Finch.ftnswrite","text":"ftnswrite(filename, tns)\n\nWrite a sparse Finch tensor to a FROSTT .tns file.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnswrite\n\n\n\n\n\n","category":"function"},{"location":"docs/fileio/#Finch.ftnsread","page":"FileIO","title":"Finch.ftnsread","text":"ftnsread(filename)\n\nRead the contents of the FROSTT .tns file 'filename' into a Finch COO Tensor.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnsread\n\n\n\n\n\n","category":"function"},{"location":"docs/language/mask_sugar/#Mask-Sugar","page":"Mask Sugar","title":"Mask Sugar","text":"","category":"section"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"In Finch, expressions like i == j are treated as a sugar for mask tensors, which can be used to encode fancy iteration patterns. For example, the expression i == j is converted to a diagonal boolean mask tensor DiagMask()[i, j], which allows an expression like","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"@finch begin\n    for i in _, j in _\n        if i == j\n            s[] += A[i, j]\n        end\n    end\nend","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"to compile to something like","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"for i in 1:n\n    s[] += A[i, i]\nend","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"There are several mask tensors and syntaxes available, summarized in the following table where i, j are indices:","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"Expression Transformed Expression\ni < j UpTriMask()[i, j - 1]\ni <= j UpTriMask()[i, j]\ni > j LoTriMask()[i, j + 1]\ni >= j LoTriMask()[i, j]\ni == j DiagMask()[i, j]\ni != j !(DiagMask()[i, j])","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"Note that either i or j may be expressions, so long as the expression is constant with respect to the loop over the index.","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"The mask tensors are described below:","category":"page"},{"location":"docs/language/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"uptrimask\nlotrimask\ndiagmask\nbandmask\nsplitmask\nchunkmask","category":"page"},{"location":"docs/language/mask_sugar/#Finch.uptrimask","page":"Mask Sugar","title":"Finch.uptrimask","text":"uptrimask\n\nA mask for an upper triangular tensor, uptrimask[i, j] = i <= j. Note that this specializes each column for the cases where i <= j and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/mask_sugar/#Finch.lotrimask","page":"Mask Sugar","title":"Finch.lotrimask","text":"lotrimask\n\nA mask for an upper triangular tensor, lotrimask[i, j] = i >= j. Note that this specializes each column for the cases where i < j and i >= j.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/mask_sugar/#Finch.diagmask","page":"Mask Sugar","title":"Finch.diagmask","text":"diagmask\n\nA mask for a diagonal tensor, diagmask[i, j] = i == j. Note that this specializes each column for the cases where i < j, i == j, and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/mask_sugar/#Finch.bandmask","page":"Mask Sugar","title":"Finch.bandmask","text":"bandmask\n\nA mask for a banded tensor, bandmask[i, j, k] = j <= i <= k. Note that this specializes each column for the cases where i < j, j <= i <= k, and k < i.\n\n\n\n\n\n","category":"constant"},{"location":"docs/language/mask_sugar/#Finch.splitmask","page":"Mask Sugar","title":"Finch.splitmask","text":"splitmask(n, P)\n\nA mask to evenly divide n indices into P regions. If M = splitmask(P, n), then M[i, j] = fld(n * (j - 1), P) <= i < fld(n * j, P).\n\njulia> splitmask(10, 3)\n10×3 Finch.SplitMask{Int64}:\n 1  0  0\n 1  0  0\n 1  0  0\n 0  1  0\n 0  1  0\n 0  1  0\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n\n\n\n\n\n\n","category":"function"},{"location":"docs/language/mask_sugar/#Finch.chunkmask","page":"Mask Sugar","title":"Finch.chunkmask","text":"chunkmask(n, b)\n\nA mask to evenly divide n indices into regions of size b. If m = chunkmask(b, n), thenm[i, j] = b * (j - 1) < i <= b * j`. Note that this specializes for the cleanup case at the end of the range.\n\njulia> chunkmask(10, 3)\n10×4 Finch.ChunkMask{Int64}:\n 1  0  0  0\n 1  0  0  0\n 1  0  0  0\n 0  1  0  0\n 0  1  0  0\n 0  1  0  0\n 0  0  1  0\n 0  0  1  0\n 0  0  1  0\n 0  0  0  1\n\n\n\n\n\n\n","category":"function"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"CurrentModule = Finch","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Tensor-Formats","page":"Getting Started","title":"Tensor Formats","text":"","category":"section"},{"location":"getting_started/#Creating-Tensors","page":"Getting Started","title":"Creating Tensors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You can create Finch tensors using the Tensor constructor, which closely follows the Array constructor syntax. The first argument specifies the storage format.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = Tensor(CSCFormat(), 4, 3);\n\njulia> B = Tensor(COOFormat(2), A);\n","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Some pre-defined formats include:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Signature Description\nDenseFormat(N, z = 0.0, T = typeof(z)) A dense format with a fill value of z.\nCSFFormat(N, z = 0.0, T = typeof(z)) An N-dimensional CSC format for sparse tensors.\nCSCFormat(z = 0.0, T = typeof(z)) A 2D CSC format storing matrices as dense lists.\nDCSFFormat(N, z = 0.0, T = typeof(z)) A DCSF format storing tensors as nested lists.\nHashFormat(N, z = 0.0, T = typeof(z)) A hash-table-based format for sparse data.\nByteMapFormat(N, z = 0.0, T = typeof(z)) A byte-map-based format for compact storage.\nDCSCFormat(z = 0.0, T = typeof(z)) A 2D DCSC format storing matrices as lists.\nCOOFormat(N, T = Float64, z = zero(T)) An N-dimensional COO format for coordinate lists.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"It is also possible to build custom formats using the interface, as described in the Tensor Formats section.","category":"page"},{"location":"getting_started/#High-Level-Array-API","page":"Getting Started","title":"High-Level Array API","text":"","category":"section"},{"location":"getting_started/#Basic-Array-Operations","page":"Getting Started","title":"Basic Array Operations","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finch tensors support indexing, slicing, mapping, broadcasting, and reducing. Many functions in the Julia standard array library are supported.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = Tensor(CSCFormat(), [0 1; 2 3]);\n\njulia> B = A .+ 1\n2×2 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}}:\n 1.0  2.0\n 3.0  4.0\n\njulia> C = max.(A, B)\n2×2 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}}:\n 1.0  2.0\n 3.0  4.0\n\njulia> D = sum(C; dims=2)\n2 Tensor{DenseLevel{Int64, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 3.0\n 7.0\n\njulia> E = B[1, :]\n2 Tensor{DenseLevel{Int64, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}:\n 1.0\n 2.0","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For situations which are difficult to express in the julia standard library, Finch also supports an @einsum syntax:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> @einsum F[i, j, k] *= A[i, j] * B[j, k]\n2×2×2 Tensor{DenseLevel{Int64, DenseLevel{Int64, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}}:\n[:, :, 1] =\n 0.0  3.0\n 2.0  9.0\n\n[:, :, 2] =\n 0.0   4.0\n 4.0  12.0\n\njulia> @einsum G[j, k] << max >>= A[i, j] + B[j, k]\n2×2 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{-Inf, Float64, Int64, Vector{Float64}}}}}:\n 3.0  4.0\n 6.0  7.0","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The @einsum macro is a powerful tool for expressing complex array operations concisely.","category":"page"},{"location":"getting_started/#Array-Fusion","page":"Getting Started","title":"Array Fusion","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To get the full benefits of a sparse compiler, it is critical to fuse certain operations together. For this, Finch exposes two functions, lazy and compute. The lazy function creates a lazy tensor, which is a symbolic representation of the computation. The compute function evaluates the computation. For convenience, you may wish to use the fused function, which automatically fuses the computations it contains.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  1.0  0.0  2.0  0.0  0.0\n 0.0  0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> B = A .* 2\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  2.0  0.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  6.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> C = lazy(A)\n?×?-LazyTensor{Float64}\n\njulia> D = lazy(B)\n?×?-LazyTensor{Float64}\n\njulia> E = (C .+ D) ./ 2\n?×?-LazyTensor{Float64}\n\njulia> compute(E)\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  1.5  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  4.5  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The lazy and compute functions allow the compiler to fuse operations together, resulting in asymptotically more efficient code.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using BenchmarkTools\n\njulia> A = fsprand(1000, 1000, 100);\n       B = Tensor(rand(1000, 1000));\n       C = Tensor(rand(1000, 1000));\n\njulia> @btime A .* (B * C);\n  145.940 ms (859 allocations: 7.69 MiB)\n\njulia> @btime compute(lazy(A) .* (lazy(B) * lazy(C)));\n  694.666 μs (712 allocations: 60.86 KiB)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Different optimizers can be used with compute, such as the state-of-the-art Galley optimizer, which can adapt to the sparsity patterns of the inputs.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = fsprand(1000, 1000, 0.1);\n       B = fsprand(1000, 1000, 0.1);\n       C = fsprand(1000, 1000, 0.0001);\n\njulia> A = lazy(A);\n       B = lazy(B);\n       C = lazy(C);\n\njulia> @btime compute(sum(A * B * C));\n  282.503 ms (1018 allocations: 184.43 MiB)\n\njulia> @btime compute(sum(A * B * C), ctx=galley_scheduler());\n  152.792 μs (672 allocations: 28.81 KiB)","category":"page"},{"location":"getting_started/#Sparse-and-Structured-Utilities","page":"Getting Started","title":"Sparse and Structured Utilities","text":"","category":"section"},{"location":"getting_started/#Sparse-Constructors","page":"Getting Started","title":"Sparse Constructors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"fsparse constructs a tensor from lists of nonzero coordinates. For example,","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = fsparse([1, 2, 3], [2, 3, 4], [1.0, 2.0, 3.0])\n3×4 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  1.0  0.0  0.0\n 0.0  0.0  2.0  0.0\n 0.0  0.0  0.0  3.0","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The inverse of fsparse is ffindnz, which returns a list of nonzero coordinates in a tensor.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> ffindnz(A)\n([1, 2, 3], [2, 3, 4], [1.0, 2.0, 3.0])","category":"page"},{"location":"getting_started/#Random-Sparse-Tensors","page":"Getting Started","title":"Random Sparse Tensors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The fsprand constructs a random sparse tensor with a specified sparsity or number of nonzeros:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = fsprand(5, 5, 0.1)\n","category":"page"},{"location":"getting_started/#Fill-Values","page":"Getting Started","title":"Fill Values","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Fill values represent the background value of a sparse tensor. Usually, this value is zero, but some applications may choose to use other fill values as fits their application. Only values which are not equal to the fill value are stored","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"fill_value: Retrieve the fill value.\nset_fill_value!: Set a new fill value.\ndropfills or dropfills!: Remove elements matching the fill value.\ncountstored: Return the number of stored values in a tensor","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> t = Tensor(Dense(SparseList(Element(0.0))), 3, 3)\n3×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia> fill_value(t)\n0.0\n\njulia> set_fill_value!(t, -1.0)\n3×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{-1.0, Float64, Int64, Vector{Float64}}}}}:\n -1.0  -1.0  -1.0\n -1.0  -1.0  -1.0\n -1.0  -1.0  -1.0\n\njulia> countstored(t)\n0\n\njulia> countstored(dropfills(t))\n0","category":"page"},{"location":"getting_started/#Empty-Tensors","page":"Getting Started","title":"Empty Tensors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The Tensor constructor initializes tensors to their fill value when given a list of dimensions, but you can also use fspzeros for an empty COO Tensor, for consistency with MATLAB.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = fspzeros(3, 3)\n3×3 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia> B = Tensor(CSCFormat(1.0), 3, 3)\n3×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0","category":"page"},{"location":"getting_started/#Converting-Between-Formats","page":"Getting Started","title":"Converting Between Formats","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You can convert between tensor formats with the Tensor constructor. Simply construct a new Tensor in the desired format and","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = Tensor(CSCFormat(), [0 0 2 1; 0 0 1 0; 1 0 0 0])\n3×4 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0\n\njulia> B = Tensor(DCSCFormat(), A)\n3×4 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0","category":"page"},{"location":"getting_started/#Storage-Order","page":"Getting Started","title":"Storage Order","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"By default, tensors in Finch are column-major. However, you can use the swizzle function to transpose them lazily. To convert to a transposed format, use the dropfills! function.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> A = Tensor(CSCFormat(), [0 0 2 1; 0 0 1 0; 1 0 0 0])\n3×4 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0\n\njulia> swizzle(A, 2, 1)\n4×3 Finch.SwizzleArray{(2, 1), Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}}:\n 0.0  0.0  1.0\n 0.0  0.0  0.0\n 2.0  1.0  0.0\n 1.0  0.0  0.0\n\njulia> dropfills!(swizzle(Tensor(CSCFormat()), 2, 1), A)\n3×4 Finch.SwizzleArray{(2, 1), Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0","category":"page"},{"location":"getting_started/#File-I/O","page":"Getting Started","title":"File I/O","text":"","category":"section"},{"location":"getting_started/#Reading-and-Writing-Files","page":"Getting Started","title":"Reading and Writing Files","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finch supports multiple formats, such as .bsp, .mtx, and .tns. Use fread and fwrite to read and write tensors.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> fwrite(\"tensor.bsp\", A)\n\njulia> B = fread(\"tensor.bsp\")\n","category":"page"},{"location":"docs/sparse_utils/#Sparse-Array-Utilities","page":"Sparse and Structured Utilities","title":"Sparse Array Utilities","text":"","category":"section"},{"location":"docs/sparse_utils/#Sparse-Constructors","page":"Sparse and Structured Utilities","title":"Sparse Constructors","text":"","category":"section"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"In addition to the Tensor constructor, Finch provides a number of convenience constructors for common tensor types. For example, the spzeros and sprand functions have fspzeros and fsprand counterparts that return Finch tensors. We can also construct a sparse COO Tensor from a list of indices and values using the fsparse function.","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"fsparse\nfsparse!\nfsprand\nfspzeros\nffindnz","category":"page"},{"location":"docs/sparse_utils/#Finch.fsparse","page":"Sparse and Structured Utilities","title":"Finch.fsparse","text":"fsparse(I::Tuple, V,[ M::Tuple, combine]; fill_value=zero(eltype(V)))\n\nCreate a sparse COO tensor S such that size(S) == M and S[(i[q] for i = I)...] = V[q]. The combine function is used to combine duplicates. If M is not specified, it is set to map(maximum, I). If the combine function is not supplied, combine defaults to + unless the elements of V are Booleans in which case combine defaults to |. All elements of I must satisfy 1 <= I[n][q] <= M[n].  Numerical zeros are retained as structural nonzeros; to drop numerical zeros, use dropzeros!.\n\nSee also: sparse\n\nExamples\n\njulia> I = (     [1, 2, 3],     [1, 2, 3],     [1, 2, 3]);\n\njulia> V = [1.0; 2.0; 3.0];\n\njulia> fsparse(I, V) SparseCOO (0.0) [1:3×1:3×1:3] │ │ │ └─└─└─[1, 1, 1] [2, 2, 2] [3, 3, 3]       1.0       2.0       3.0\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.fsparse!","page":"Sparse and Structured Utilities","title":"Finch.fsparse!","text":"fsparse!(I..., V,[ M::Tuple])\n\nLike fsparse, but the coordinates must be sorted and unique, and memory is reused.\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.fsprand","page":"Sparse and Structured Utilities","title":"Finch.fsprand","text":"fsprand([rng],[type], M..., p, [rfn])\n\nCreate a random sparse tensor of size m in COO format. There are two cases:     - If p is floating point, the probability of any element being nonzero is     independently given by p (and hence the expected density of nonzeros is     also p).     - If p is an integer, exactly p nonzeros are distributed uniformly at     random throughout the tensor (and hence the density of nonzeros is exactly     p / prod(M)). Nonzero values are sampled from the distribution specified by rfn and have the type type. The uniform distribution is used in case rfn is not specified. The optional rng argument specifies a random number generator.\n\nSee also: (sprand)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.sprand)\n\nExamples\n\njulia> fsprand(Bool, 3, 3, 0.5)\nSparseCOO (false) [1:3,1:3]\n├─├─[1, 1]: true\n├─├─[3, 1]: true\n├─├─[2, 2]: true\n├─├─[3, 2]: true\n├─├─[3, 3]: true\n\njulia> fsprand(Float64, 2, 2, 2, 0.5)\nSparseCOO (0.0) [1:2,1:2,1:2]\n├─├─├─[2, 2, 1]: 0.6478553157718558\n├─├─├─[1, 1, 2]: 0.996665291437684\n├─├─├─[2, 1, 2]: 0.7491940599574348\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.fspzeros","page":"Sparse and Structured Utilities","title":"Finch.fspzeros","text":"fspzeros([type], M...)\n\nCreate a random zero tensor of size M, with elements of type type. The tensor is in COO format.\n\nSee also: (spzeros)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.spzeros)\n\nExamples\n\njulia> A = fspzeros(Bool, 3, 3)\n3×3 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{false, Bool, Int64, Vector{Bool}}}}:\n 0  0  0\n 0  0  0\n 0  0  0\n\njulia> countstored(A)\n0\n\njulia> B = fspzeros(Float64, 2, 2, 2)\n2×2×2 Tensor{SparseCOOLevel{3, Tuple{Int64, Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n[:, :, 1] =\n 0.0  0.0\n 0.0  0.0\n\n[:, :, 2] =\n 0.0  0.0\n 0.0  0.0\n\njulia> countstored(B)\n0\n\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.ffindnz","page":"Sparse and Structured Utilities","title":"Finch.ffindnz","text":"ffindnz(arr)\n\nReturn the nonzero elements of arr, as Finch understands arr. Returns (I..., V), where I are the coordinate vectors, one for each mode of arr, and V is a vector of corresponding nonzero values, which can be passed to fsparse.\n\nSee also: (findnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.findnz)\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Fill-Values","page":"Sparse and Structured Utilities","title":"Fill Values","text":"","category":"section"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"Finch tensors support an arbitrary \"background\" value for sparse arrays. While most arrays use 0 as the background value, this is not always the case. For example, a sparse array of Int might use typemin(Int) as the background value. The fill_value function returns the background value of a tensor. If you ever want to change the background value of an existing array, you can use the set_fill_value! function. The countstored function returns the number of stored elements in a tensor, and calling pattern! on a tensor returns tensor which is true whereever the original tensor stores a value. Note that countstored doesn't always return the number of non-zero elements in a tensor, as it counts the number of stored elements, and stored elements may include the background value. You can call dropfills! to remove explicitly stored background values from a tensor.","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0  1.0  0.0  2.0  0.0  0.0\n 0.0  0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0\n\njulia> min.(A, -1)\n3×6 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{-1.0, Float64, Int64, Vector{Float64}}}}}:\n -1.0  -1.0  -1.0  -1.0  -1.0  -1.0\n -1.0  -1.0  -1.0  -1.0  -1.0  -1.0\n -1.0  -1.0  -1.0  -1.0  -1.0  -1.0\n\njulia> fill_value(A)\n0.0\n\njulia> B = set_fill_value!(A, -Inf)\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{-Inf, Float64, Int64, Vector{Float64}}}}:\n -Inf    1.0  -Inf    2.0  -Inf   -Inf\n -Inf  -Inf   -Inf  -Inf     3.0  -Inf\n -Inf  -Inf   -Inf  -Inf   -Inf   -Inf\n\njulia> min.(B, -1)\n3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{-Inf, Float64, Int64, Vector{Float64}}}}}:\n -Inf   -1.0  -Inf   -1.0  -Inf   -Inf\n -Inf  -Inf   -Inf  -Inf    -1.0  -Inf\n -Inf  -Inf   -Inf  -Inf   -Inf   -Inf\n\njulia> countstored(A)\n3\n\njulia> pattern!(A)\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, PatternLevel{Int64}}}:\n 0  1  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  0","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"fill_value\nset_fill_value!\npattern!\ncountstored\ndropfills\ndropfills!","category":"page"},{"location":"docs/sparse_utils/#Finch.fill_value","page":"Sparse and Structured Utilities","title":"Finch.fill_value","text":"fill_value(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the \"fill\" value becomes the \"background\" value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.set_fill_value!","page":"Sparse and Structured Utilities","title":"Finch.set_fill_value!","text":"set_fill_value!(fbr, init)\n\nReturn a tensor which is equal to fbr, but with the fill (implicit) value set to init.  May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 2.0\n 0.0\n 3.0\n 0.0\n 4.0\n 0.0\n 5.0\n 0.0\n 6.0\n 0.0\n\njulia> set_fill_value!(A, Inf)\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{Inf, Float64, Int64, Vector{Float64}}}}:\n  2.0\n Inf\n  3.0\n Inf\n  4.0\n Inf\n  5.0\n Inf\n  6.0\n Inf\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.pattern!","page":"Sparse and Structured Utilities","title":"Finch.pattern!","text":"pattern!(fbr)\n\nReturn the pattern of fbr. That is, return a tensor which is true wherever fbr is structurally unequal to its fill_value. May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 2.0\n 0.0\n 3.0\n 0.0\n 4.0\n 0.0\n 5.0\n 0.0\n 6.0\n 0.0\n\njulia> pattern!(A)\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, PatternLevel{Int64}}}:\n 1\n 0\n 1\n 0\n 1\n 0\n 1\n 0\n 1\n 0\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.countstored","page":"Sparse and Structured Utilities","title":"Finch.countstored","text":"countstored(arr)\n\nReturn the number of stored elements in arr. If there are explicitly stored fill elements, they are counted too.\n\nSee also: (SparseArrays.nnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.nnz) and (Base.summarysize)(https://docs.julialang.org/en/v1/base/base/#Base.summarysize)\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.dropfills","page":"Sparse and Structured Utilities","title":"Finch.dropfills","text":"dropfills(src)\n\nDrop the fill values from src and return a new tensor with the same shape and format.\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#Finch.dropfills!","page":"Sparse and Structured Utilities","title":"Finch.dropfills!","text":"dropfills!(dst, src)\n\nCopy only the non-fill values from src into dst.\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/#How-to-tell-whether-an-entry-is-\"fill\"","page":"Sparse and Structured Utilities","title":"How to tell whether an entry is \"fill\"","text":"","category":"section"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"In the sparse world, a semantic distinction is sometimes made between \"explicitly stored\" values and \"implicit\" or \"fill\" values (usually zero). However, the formats in the Finch compiler represent a diverse set of structures beyond sparsity, and it is often unclear whether any of the values in the tensor are \"explicit\" (consider a mask matrix, which can be represented with a constant number of bits). Thus, Finch makes no semantic distinction between values which are stored explicitly or not. If users wish to make this distinction, they should instead store a tensor of tuples of the form (value, is_fill). For example,","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = fsparse(\n           [1, 1, 2, 3],\n           [2, 4, 5, 6],\n           [(1.0, false), (0.0, true), (3.0, false)];\n           fill_value=(0.0, true),\n       )\n3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{(0.0, true), Tuple{Float64, Bool}, Int64, Vector{Tuple{Float64, Bool}}}}}:\n (0.0, 1)  (1.0, 0)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)\n (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (3.0, 0)  (0.0, 1)\n (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)\n\njulia> B = Tensor(Dense(SparseList(Element((0.0, true)))), A)\n3×6 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{(0.0, true), Tuple{Float64, Bool}, Int64, Vector{Tuple{Float64, Bool}}}}}}:\n (0.0, 1)  (1.0, 0)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)\n (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (3.0, 0)  (0.0, 1)\n (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)  (0.0, 1)\n\njulia> sum(map(last, B))\n16\n\njulia> sum(map(first, B))\n4.0","category":"page"},{"location":"docs/sparse_utils/#Format-Conversion-and-Storage-Order","page":"Sparse and Structured Utilities","title":"Format Conversion and Storage Order","text":"","category":"section"},{"location":"docs/sparse_utils/#Converting-Between-Formats","page":"Sparse and Structured Utilities","title":"Converting Between Formats","text":"","category":"section"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"You can convert between tensor formats with the Tensor constructor. Simply construct a new Tensor in the desired format and","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = Tensor(CSCFormat(), [0 0 2 1; 0 0 1 0; 1 0 0 0])\n3×4 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0\n\njulia> B = Tensor(DCSCFormat(), A)\n3×4 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0","category":"page"},{"location":"docs/sparse_utils/#Storage-Order","page":"Sparse and Structured Utilities","title":"Storage Order","text":"","category":"section"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"By default, tensors in Finch are column-major. However, you can use the swizzle function to transpose them lazily. To convert to a transposed format, use the dropfills! function. Note that the permutedims function transposes eagerly.","category":"page"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"swizzle","category":"page"},{"location":"docs/sparse_utils/#Finch.swizzle","page":"Sparse and Structured Utilities","title":"Finch.swizzle","text":"swizzle(tns, dims)\n\nCreate a SwizzleArray to transpose any tensor tns such that\n\n    swizzle(tns, dims)[i...] == tns[i[dims]]\n\n\n\n\n\n","category":"function"},{"location":"docs/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = Tensor(CSCFormat(), [0 0 2 1; 0 0 1 0; 1 0 0 0])\n3×4 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0\n\njulia> tensor_tree(swizzle(A, 2, 1))\nSwizzleArray (2, 1)\n└─ 3×4-Tensor\n   └─ Dense [:,1:4]\n      ├─ [:, 1]: SparseList (0.0) [1:3]\n      │  └─ [3]: 1.0\n      ├─ [:, 2]: SparseList (0.0) [1:3]\n      ├─ [:, 3]: SparseList (0.0) [1:3]\n      │  ├─ [1]: 2.0\n      │  └─ [2]: 1.0\n      └─ [:, 4]: SparseList (0.0) [1:3]\n         └─ [1]: 1.0\n\njulia> tensor_tree(permutedims(A, (2, 1)))\n4×3-Tensor\n└─ SparseDict (0.0) [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:4]\n   │  ├─ [3]: 2.0\n   │  └─ [4]: 1.0\n   ├─ [:, 2]: SparseDict (0.0) [1:4]\n   │  └─ [3]: 1.0\n   └─ [:, 3]: SparseDict (0.0) [1:4]\n      └─ [1]: 1.0\n\njulia> dropfills!(swizzle(Tensor(CSCFormat()), 2, 1), A)\n3×4 Finch.SwizzleArray{(2, 1), Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}}:\n 0.0  0.0  2.0  1.0\n 0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  0.0","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"CurrentModule = Finch","category":"page"},{"location":"docs/tensor_formats/#Constructing-Tensors","page":"Tensor Formats","title":"Constructing Tensors","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"You can build a finch tensor with the Tensor constructor. In general, the Tensor constructor mirrors Julia's Array constructor, but with an additional prefixed argument which specifies the formatted storage for the tensor.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Tensor\nTensor(lvl::AbstractLevel)\nTensor(lvl::AbstractLevel, dims::Number...)\nTensor(lvl::AbstractLevel, init::UndefInitializer)\nTensor(lvl::AbstractLevel, arr)\nTensor(arr)","category":"page"},{"location":"docs/tensor_formats/#Finch.Tensor","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor{Lvl} <: AbstractFiber{Lvl}\n\nThe multidimensional array type used by Finch. Tensor is a thin wrapper around the hierarchical level storage of type Lvl.\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl)\n\nConstruct a Tensor using the tensor level storage lvl. No initialization of storage is performed, it is assumed that position 1 of lvl corresponds to a valid tensor, and lvl will be wrapped as-is. Call a different constructor to initialize the storage.\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, [undef], dims...)\n\nConstruct a Tensor of size dims, and initialize to undef, potentially allocating memory.  Here undef is the UndefInitializer singleton type. dims... may be a variable number of dimensions or a tuple of dimensions, but it must correspond to the number of dimensions in lvl.\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, UndefInitializer}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, Any}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/#Finch.Tensor-Tuple{Any}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(arr, [init = zero(eltype(arr))])\n\nCopy an array-like object arr into a corresponding, similar Tensor datastructure. Uses init as an initial value. May reuse memory when possible. To explicitly copy into a tensor, use @ref[copyto!].\n\nExamples\n\njulia> println(summary(Tensor(sparse([1 0; 0 1]))))\n2×2 Tensor(Dense(SparseList(Element(0))))\n\njulia> println(summary(Tensor(ones(3, 2, 4))))\n3×2×4 Tensor(Dense(Dense(Dense(Element(0.0)))))\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"A few predefined formats are available for use in the first argument to the Tensor constructor:","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"DenseFormat\nCSCFormat\nCSFFormat\nDCSCFormat\nDCSFFormat\nCOOFormat\nHashFormat\nByteMapFormat","category":"page"},{"location":"docs/tensor_formats/#Finch.DenseFormat","page":"Tensor Formats","title":"Finch.DenseFormat","text":"DenseFormat(N, z = 0.0, T = typeof(z))\n\nA dense format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.CSCFormat","page":"Tensor Formats","title":"Finch.CSCFormat","text":"CSCFormat(z = 0.0, T = typeof(z))\n\nA CSC format with a fill value of z. CSC stores a sparse matrix as a dense array of lists.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.CSFFormat","page":"Tensor Formats","title":"Finch.CSFFormat","text":"CSFFormat(N, z = 0.0, T = typeof(z))\n\nAn N-dimensional CSC format with a fill value of z. CSF supports random access in the rightmost index, and uses a tree structure to store the rest of the data.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.DCSCFormat","page":"Tensor Formats","title":"Finch.DCSCFormat","text":"DCSCFormat(z = 0.0, T = typeof(z))\n\nA DCSC format with a fill value of z. DCSC stores a sparse matrix as a list of lists.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.DCSFFormat","page":"Tensor Formats","title":"Finch.DCSFFormat","text":"DCSFFormat(z = 0.0, T = typeof(z))\n\nA DCSF format with a fill value of z. DCSF stores a sparse tensor as a list of lists of lists.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.COOFormat","page":"Tensor Formats","title":"Finch.COOFormat","text":"COOFormat(N, z = 0.0, T = typeof(z))\n\nAn N-dimensional COO format with a fill value of z. COO stores a sparse tensor as a list of coordinates.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.HashFormat","page":"Tensor Formats","title":"Finch.HashFormat","text":"HashFormat(N, z = 0.0, T = typeof(z))\n\nA hash-table based format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/#Finch.ByteMapFormat","page":"Tensor Formats","title":"Finch.ByteMapFormat","text":"ByteMapFormat(N, z = 0.0, T = typeof(z))\n\nA byte-map based format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"For example, to construct an empty sparse matrix:","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"julia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), 4, 3)\n4×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"To initialize a sparse matrix with some values:","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), A)\n4×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> tensor_tree(A_fbr)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"docs/tensor_formats/#Custom-Storage-Tree-Level-Formats","page":"Tensor Formats","title":"Custom Storage Tree Level Formats","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"This section describes the formatted storage for Finch tensors, the first argument to the Tensor constructor. Level storage types holds all of the tensor data, and can be nested hierarchichally.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch represents tensors hierarchically in a tree, where each node in the tree is a vector of subtensors and the leaves are the elements.  Thus, a matrix is analogous to a vector of vectors, and a 3-tensor is analogous to a vector of vectors of vectors.  The vectors at each level of the tensor all have the same structure, which can be selected by the user. You can visualize the tree using the tensor_tree function.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"tensor_tree(::AbstractTensor)\ntensor_tree(::IO, ::AbstractTensor)","category":"page"},{"location":"docs/tensor_formats/#Finch.tensor_tree-Tuple{Finch.AbstractTensor}","page":"Tensor Formats","title":"Finch.tensor_tree","text":"tensor_tree(tns; nmax = 2)\n\nPrint a tree representation of the tensor tns to the standard output. nmax is half the maximum number of children to show before truncating.\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/#Finch.tensor_tree-Tuple{IO, Finch.AbstractTensor}","page":"Tensor Formats","title":"Finch.tensor_tree","text":"tensor_tree(io::IO, tns; nmax = 2)\n\nPrint a tree representation of the tensor tns to io. nmax is half the maximum number of children to show before truncating.\n\n\n\n\n\n","category":"method"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"In a Finch tensor tree, the child of each node is selected by an array index. All of the children at the same level will use the same format and share the same storage. Finch is column major, so in an expression A[i_1, ..., i_N], the rightmost dimension i_N corresponds to the root level of the tree, and the leftmost dimension i_1 corresponds to the leaf level.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Our example could be visualized as follows:","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"(Image: CSC Format Index Tree)","category":"page"},{"location":"docs/tensor_formats/#Types-of-Level-Storage","page":"Tensor Formats","title":"Types of Level Storage","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch supports a variety of storage formats for each level of the tensor tree, each with advantages and disadvantages. Some storage formats support in-order access, while others support random access. Some storage formats must be written to in column-major order, while others support out-of-order writes. The capabilities of each level are summarized in the following tables along with some general descriptions.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Level Format Name Group Data Characteristic Column-Major Reads Random Reads Column-Major Bulk Update Random Bulk Update Random Updates Status\nDense Core Dense ✅ ✅ ✅ ✅ ✅ ✅\nSparseTree Core Sparse ✅ ✅ ✅ ✅ ✅ ⚙️\nSparseRunListTree Core Sparse Runs ✅ ✅ ✅ ✅ ✅ ⚙️\nElement Core Leaf ✅ ✅ ✅ ✅ ✅ ✅\nPattern Core Leaf ✅ ✅ ✅ ✅ ✅ ✅\nSparseList Advanced Sparse ✅ ❌ ✅ ❌ ❌ ✅\nSparseRunList Advanced Sparse Runs ✅ ❌ ✅ ❌ ❌ ✅\nSparseBlockList Advanced Sparse Blocks ✅ ❌ ✅ ❌ ❌ ✅\nSparsePoint Advanced Single Sparse ✅ ✅ ✅ ❌ ❌ ✅\nSparseInterval Advanced Single Sparse Run ✅ ✅ ✅ ❌ ❌ ✅\nSparseBand Advanced Single Sparse Block ✅ ✅ ✅ ❌ ❌ ⚙️\nRunList Advanced Dense Runs ✅ ❌ ✅ ❌ ❌ ⚙️\nSparseBytemap Advanced Sparse ✅ ✅ ✅ ✅ ❌ ✅\nSparseDict Advanced Sparse ✅ ✅ ✅ ✅ ❌ ✅️\nMutexLevel Modifier No Data ✅ ✅ ✅ ✅ ✅ ⚙️\nSeperationLevel Modifier No Data ✅ ✅ ✅ ✅ ✅ ⚙️\nSparseCOO Legacy Sparse ✅ ✅ ✅ ❌ ✅ ✅️","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"The \"Level Format Name\" is the name of the level datatype. Other columns have descriptions below.","category":"page"},{"location":"docs/tensor_formats/#Status","page":"Tensor Formats","title":"Status","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Symbol Meaning\n✅ Indicates the level is ready for serious use.\n⚙️ Indicates the level is experimental and under development.\n🕸️ Indicates the level is deprecated, and may be removed in a future release.","category":"page"},{"location":"docs/tensor_formats/#Groups","page":"Tensor Formats","title":"Groups","text":"","category":"section"},{"location":"docs/tensor_formats/#Core-Group","page":"Tensor Formats","title":"Core Group","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains the basic, minimal set of levels one should use to build and manipulate tensors.  These levels can be efficiently read and written to in any order.","category":"page"},{"location":"docs/tensor_formats/#Advanced-Group","page":"Tensor Formats","title":"Advanced Group","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are more specialized, and geared towards bulk updates. These levels may be more efficient in certain cases, but are also more restrictive about access orders and intended for more advanced usage.","category":"page"},{"location":"docs/tensor_formats/#Modifier-Group","page":"Tensor Formats","title":"Modifier Group","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are also more specialized, but not towards a sparsity pattern. These levels modify other levels in a variety of ways, but don't store novel sparsity patterns. Typically, they modify how levels are stored or attach data to levels to support the utilization of various hardware features.","category":"page"},{"location":"docs/tensor_formats/#Legacy-Group","page":"Tensor Formats","title":"Legacy Group","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are not recommended for new code, but are included for compatibility with older code.","category":"page"},{"location":"docs/tensor_formats/#Data-Characteristics","page":"Tensor Formats","title":"Data Characteristics","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Level Type Description\nDense Levels which store every subtensor.\nLeaf Levels which store only scalars, used for the leaf level of the tree.\nSparse Levels which store only non-fill values, used for levels with few nonzeros.\nSparse Runs Levels which store runs of repeated non-fill values.\nSparse Blocks Levels which store Blocks of repeated non-fill values.\nDense Runs Levels which store runs of repeated values, and no compile-time zero annihilation.\nNo Data Levels which don't store data but which alter the storage pattern or attach additional meta-data.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Note that the Single sparse levels store a single instance of each nonzero, run, or block. These are useful with a parent level to represent IDs.","category":"page"},{"location":"docs/tensor_formats/#Access-Characteristics","page":"Tensor Formats","title":"Access Characteristics","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Operation Type Description\nColumn-Major Reads Indicates efficient reading of data in column-major order.\nRandom Reads Indicates efficient reading of data in random-access order.\nColumn-Major Bulk Update Indicates efficient writing of data in column-major order, the total time roughly linear to the size of the tensor.\nColumn-Major Random Update Indicates efficient writing of data in random-access order, the total time roughly linear to the size of the tensor.\nRandom Update Indicates efficient writing of data in random-access order, the total time roughly linear to the number of updates.","category":"page"},{"location":"docs/tensor_formats/#Diagrams","page":"Tensor Formats","title":"Diagrams","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"The following diagrams illustrate the structure of the levels individually.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"(Image: Diagram of Core Level Structures)","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"The following diagrams illustrate the way that levels can be combined to form a tensor tree.","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"(Image: Diagram of Core Level Structures)","category":"page"},{"location":"docs/tensor_formats/#Examples-of-Popular-Formats-in-Finch","page":"Tensor Formats","title":"Examples of Popular Formats in Finch","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch levels can be used to construct a variety of popular sparse formats. A few examples follow:","category":"page"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Format Type Syntax\nSparse Vector Tensor(SparseList(Element(0.0)), args...)\nCSC Matrix Tensor(Dense(SparseList(Element(0.0))), args...)\nCSF 3-Tensor Tensor(Dense(SparseList(SparseList(Element(0.0)))), args...)\nDCSC (Hypersparse) Matrix Tensor(SparseList(SparseList(Element(0.0))), args...)\nCOO Matrix Tensor(SparseCOO{2}(Element(0.0)), args...)\nCOO 3-Tensor Tensor(SparseCOO{3}(Element(0.0)), args...)\nRun-Length-Encoded Image Tensor(Dense(RunList(Element(0.0))), args...)","category":"page"},{"location":"docs/tensor_formats/#Level-Constructors","page":"Tensor Formats","title":"Level Constructors","text":"","category":"section"},{"location":"docs/tensor_formats/#Core-Levels","page":"Tensor Formats","title":"Core Levels","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"DenseLevel\nElementLevel\nPatternLevel","category":"page"},{"location":"docs/tensor_formats/#Finch.DenseLevel","page":"Tensor Formats","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\njulia> ndims(Tensor(Dense(Element(0.0))))\n1\n\njulia> ndims(Tensor(Dense(Dense(Element(0.0)))))\n2\n\njulia> tensor_tree(Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4]))\n2×2-Tensor\n└─ Dense [:,1:2]\n   ├─ [:, 1]: Dense [1:2]\n   │  ├─ [1]: 1.0\n   │  └─ [2]: 3.0\n   └─ [:, 2]: Dense [1:2]\n      ├─ [1]: 2.0\n      └─ [2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.ElementLevel","page":"Tensor Formats","title":"Finch.ElementLevel","text":"ElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to Vf. Vf may optionally be given as the first argument.\n\nThe data is stored in a vector of type Val with eltype(Val) = Tv. The type Tp is the index type used to access Val.\n\njulia> tensor_tree(Tensor(Dense(Element(0.0)), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.PatternLevel","page":"Tensor Formats","title":"Finch.PatternLevel","text":"PatternLevel{[Tp=Int]}()\n\nA subfiber of a pattern level is the Boolean value true, but it's fill_value is false. PatternLevels are used to create tensors that represent which values are stored by other fibers. See pattern! for usage examples.\n\njulia> tensor_tree(Tensor(Dense(Pattern()), 3))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: true\n   ├─ [2]: true\n   └─ [3]: true\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Advanced-Levels","page":"Tensor Formats","title":"Advanced Levels","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"SparseListLevel\nRunListLevel\nSparseRunListLevel\nSparseBlockListLevel\nSparseBandLevel\nSparsePointLevel\nSparseIntervalLevel\nSparseByteMapLevel\nSparseDictLevel","category":"page"},{"location":"docs/tensor_formats/#Finch.SparseListLevel","page":"Tensor Formats","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseList (0.0) [1:3]\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.RunListLevel","page":"Tensor Formats","title":"Finch.RunListLevel","text":"RunListLevel{[Ti=Int], [Ptr, Right]}(lvl, [dim], [merge = true])\n\nThe RunListLevel represent runs of equivalent slices A[:, ..., :, i]. A sorted list is used to record the right endpoint of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> tensor_tree(Tensor(Dense(RunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: RunList (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  ├─ [2:2]: 30.0\n   │  └─ [3:3]: 0.0\n   ├─ [:, 2]: RunList (0.0) [1:3]\n   │  └─ [1:3]: 0.0\n   └─ [:, 3]: RunList (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      ├─ [2:2]: 0.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseRunListLevel","page":"Tensor Formats","title":"Finch.SparseRunListLevel","text":"SparseRunListLevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim]; [merge = true])\n\nThe SparseRunListLevel represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A sorted list is used to record the left and right endpoints of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr, Left, and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> tensor_tree(Tensor(Dense(SparseRunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseRunList (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  └─ [2:2]: 30.0\n   ├─ [:, 2]: SparseRunList (0.0) [1:3]\n   └─ [:, 3]: SparseRunList (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseBlockListLevel","page":"Tensor Formats","title":"Finch.SparseBlockListLevel","text":"SparseBlockListLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseListLevel, but contiguous subfibers are stored together in blocks.\n\n```jldoctest julia> Tensor(Dense(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\njulia> Tensor(SparseBlockList(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) SparseList (0.0) [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseBandLevel","page":"Tensor Formats","title":"Finch.SparseBandLevel","text":"SparseBandLevel{[Ti=Int], [Idx, Ofs]}(lvl, [dim])\n\nLike the SparseBlockListLevel, but stores only a single block, and fills in zeros.\n\n```jldoctest julia> Tensor(Dense(SparseBand(Element(0.0))), [10 0 20; 30 40 0; 0 0 50]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparsePointLevel","page":"Tensor Formats","title":"Finch.SparsePointLevel","text":"SparsePointLevel{[Ti=Int], [Idx]}(lvl, [dim])\n\nA subfiber of a SparsePoint level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. A main difference compared to SparseList level is that SparsePoint level only stores a 'single' non-fill slice. It emits an error if the program tries to write multiple (>=2) coordinates into SparsePoint.\n\nTi is the type of the last tensor index. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparsePoint(Element(0.0))), [10 0 0; 0 20 0; 0 0 30]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparsePoint (0.0) [1:3]\n   │  └─ [1]: 10.0\n   ├─ [:, 2]: SparsePoint (0.0) [1:3]\n   │  └─ [2]: 20.0\n   └─ [:, 3]: SparsePoint (0.0) [1:3]\n      └─ [3]: 30.0\n\njulia> tensor_tree(Tensor(SparsePoint(Dense(Element(0.0))), [0 0 0; 0 0 30; 0 0 30]))\n3×3-Tensor\n└─ SparsePoint (0.0) [:,1:3]\n   └─ [:, 3]: Dense [1:3]\n      ├─ [1]: 0.0\n      ├─ [2]: 30.0\n      └─ [3]: 30.0\n\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseIntervalLevel","page":"Tensor Formats","title":"Finch.SparseIntervalLevel","text":"SparseIntervalLevel{[Ti=Int], [Left, Right]}(lvl, [dim])\n\nThe SparseIntervalLevel represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A main difference compared to SparseRunList level is that SparseInterval level only stores a 'single' non-fill run. It emits an error if the program tries to write multiple (>=2) runs into SparseInterval.\n\nTi is the type of the last tensor index. The types Left, and 'Right' are the types of the arrays used to store positions and endpoints.\n\njulia> tensor_tree(Tensor(SparseInterval(Element(0)), [0, 10, 0]))\n3-Tensor\n└─ SparseInterval (0) [1:3]\n   └─ [2:2]: 10\n\njulia> x = Tensor(SparseInterval(Element(0)), 10);\n\njulia> @finch begin for i = extent(3,6); x[~i] = 1 end end;\n\njulia> tensor_tree(x)\n10-Tensor\n└─ SparseInterval (0) [1:10]\n   └─ [3:6]: 1\n\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseByteMapLevel","page":"Tensor Formats","title":"Finch.SparseByteMapLevel","text":"SparseByteMapLevel{[Ti=Int], [Ptr, Tbl]}(lvl, [dims])\n\nLike the SparseListLevel, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level.\n\njulia> tensor_tree(Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseByteMap (0.0) [1:3]\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n      ├─ [1]: 0.0\n      └─ [3]: 0.0\n\njulia> tensor_tree(Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseByteMap (0.0) [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Finch.SparseDictLevel","page":"Tensor Formats","title":"Finch.SparseDictLevel","text":"SparseDictLevel{[Ti=Int], [Tp=Int], [Ptr, Idx, Val, Tbl, Pool=Dict]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A datastructure specified by Tbl is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseDict(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseDict (0.0) [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"docs/tensor_formats/#Legacy-Levels","page":"Tensor Formats","title":"Legacy Levels","text":"","category":"section"},{"location":"docs/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"SparseCOOLevel","category":"page"},{"location":"docs/tensor_formats/#Finch.SparseCOOLevel","page":"Tensor Formats","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr, Tbl]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTI is the type of the last N tensor indices, and Tp is the type used for positions in the level.\n\nThe type Tbl is an NTuple type where each entry k is a subtype AbstractVector{TI[k]}.\n\nThe type Ptr is the type for the pointer array.\n\njulia> tensor_tree(Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseCOO{1} (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseCOO{1} (0.0) [1:3]\n   └─ [:, 3]: SparseCOO{1} (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [1, 1]: 10.0\n   ├─ [2, 1]: 30.0\n   ├─ [1, 3]: 20.0\n   └─ [3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"docs/internals/looplets_coiteration/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"CurrentModule = Finch","category":"page"},{"location":"docs/user-defined_functions/#User-Defined-Functions","page":"User-Defined Functions","title":"User-Defined Functions","text":"","category":"section"},{"location":"docs/user-defined_functions/#User-Functions","page":"User-Defined Functions","title":"User Functions","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch supports arbitrary Julia Base functions over isbits types.  You can also use your own functions and use them in Finch! Just remember to define any special algebraic properties of your functions so that Finch can optimize them better. You must declare the properties of your functions before you call any Finch functions on them.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch only supports incrementing assignments to arrays such as += or *=. If you would like to increment A[i...] by the value of ex with a custom reduction operator op, you may use the following syntax: A[i...] <<op>>= ex.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Consider the greatest common divisor function gcd. This function is associative and commutative, and the greatest common divisor of 1 and anything else is 1, so 1 is an annihilator.  We declare these properties by overloading trait functions on Finch's default algebra as follows:","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch.isassociative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::Finch.DefaultAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Then, the following code will only call gcd when neither u[i] nor v[i] are 1 (just once!).","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"u = Tensor(SparseList(Element(1)), [3, 1, 6, 1, 9, 1, 4, 1, 8, 1])\nv = Tensor(SparseList(Element(1)), [1, 2, 3, 1, 1, 1, 1, 4, 1, 1])\nw = Tensor(SparseList(Element(1)))\n\n@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end)","category":"page"},{"location":"docs/user-defined_functions/#A-Few-Convenient-Functions","page":"User-Defined Functions","title":"A Few Convenient Functions","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"For your convenience, Finch defines a few useful functions that help express common array operations inside Finch:","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"choose\nminby\nmaxby","category":"page"},{"location":"docs/user-defined_functions/#Finch.choose","page":"User-Defined Functions","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0\n 1.1\n 0.0\n 4.4\n 0.0\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<choose(0.0)>>= a[i] end;\n\njulia> x[]\n1.1\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.minby","page":"User-Defined Functions","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch for i=_; x[] <<minby>>= a[i] => i end;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.maxby","page":"User-Defined Functions","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch for i=_; x[] <<maxby>>= a[i] => i end;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Properties","page":"User-Defined Functions","title":"Properties","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"The full list of properties recognized by Finch is as follows (use these to declare the properties of your own functions):","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"isassociative\niscommutative\nisdistributive\nisidempotent\nisidentity\nisannihilator\nisinverse\nisinvolution\nFinch.return_type","category":"page"},{"location":"docs/user-defined_functions/#Finch.isassociative","page":"User-Defined Functions","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.iscommutative","page":"User-Defined Functions","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isdistributive","page":"User-Defined Functions","title":"Finch.isdistributive","text":"isdistributive(algebra, f, g)\n\nReturn true when f(a, g(b, c)) = g(f(a, b), f(a, c)) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isidempotent","page":"User-Defined Functions","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isidentity","page":"User-Defined Functions","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isannihilator","page":"User-Defined Functions","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isinverse","page":"User-Defined Functions","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.isinvolution","page":"User-Defined Functions","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.return_type","page":"User-Defined Functions","title":"Finch.return_type","text":"return_type(algebra, f, arg_types...)\n\nGive the return type of f when applied to arguments of types arg_types... in algebra. Used to determine output types of functions in the high-level interface. This function falls back to Base.promote_op.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch-Kernel-Caching","page":"User-Defined Functions","title":"Finch Kernel Caching","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch code is cached when you first run it. Thus, if you run a Finch function once, then make changes to the Finch compiler (like defining new properties), the cached code will be used and the changes will not be reflected.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"It's best to design your code so that modifications to the Finch compiler occur before any Finch functions are called. However, if you really need to modify a precompiled Finch kernel, you can call Finch.refresh() to invalidate the code cache.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"refresh","category":"page"},{"location":"docs/user-defined_functions/#Finch.refresh","page":"User-Defined Functions","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#(Advanced)-On-World-Age-and-Generated-Functions","page":"User-Defined Functions","title":"(Advanced) On World-Age and Generated Functions","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Julia uses a \"world age\" to describe the set of defined functions at a point in time. Generated functions run in the same world age in which they were defined, so they can't call functions defined after the generated function. This means that if Finch used normal generated functions, users can't define their own functions without first redefining all of Finch's generated functions.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch uses special generators that run in the current world age, but do not update with subsequent compiler function invalidations. If two packages modify the behavior of Finch in different ways, and call those Finch functions during precompilation, the resulting behavior is undefined.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"There are several packages that take similar, but different, approaches to allow user participation in staged Julia programming (not to mention Base eval or @generated): StagedFunctions.jl, GeneralizedGenerated.jl, RuntimeGeneratedFunctions.jl, or Zygote.jl.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Our approach is most similar to that of StagedFunctions.jl or Zygote.jl. We chose our approach to be the simple and flexible while keeping the kernel call overhead low.","category":"page"},{"location":"docs/user-defined_functions/#(Advanced)-Separate-Algebras","page":"User-Defined Functions","title":"(Advanced) Separate Algebras","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"If you want to define non-standard properties or custom rewrite rules for some functions in a separate context, you can represent these changes with your own algebra type.  We express this by subtyping AbstractAlgebra and defining properties as follows:","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"struct MyAlgebra <: AbstractAlgebra end\n\nFinch.isassociative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::MyAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"We pass the algebra to Finch as an optional first argument:","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end; return w)","category":"page"},{"location":"docs/user-defined_functions/#Rewriting","page":"User-Defined Functions","title":"Rewriting","text":"","category":"section"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Define custom rewrite rules by overloading the get_simplify_rules function on your algebra.  Unless you want to write the full rule set from scratch, be sure to append your new rules to the old rules, which can be obtained by calling get_simplify_rules with another algebra. Rules can be specified directly on Finch IR using RewriteTools.jl.","category":"page"},{"location":"docs/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"get_simplify_rules\nget_prove_rules","category":"page"},{"location":"docs/user-defined_functions/#Finch.get_simplify_rules","page":"User-Defined Functions","title":"Finch.get_simplify_rules","text":"get_simplify_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"function"},{"location":"docs/user-defined_functions/#Finch.get_prove_rules","page":"User-Defined Functions","title":"Finch.get_prove_rules","text":"get_prove_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"function"},{"location":"#Finch.jl","page":"Home","title":"Finch.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch is a Julia-to-Julia compiler for sparse or structured multidimensional arrays. Finch empowers users to write high-level array programs which are transformed behind-the-scenes into fast sparse code.","category":"page"},{"location":"#Why-Finch.jl?","page":"Home","title":"Why Finch.jl?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch was built to make sparse and structured array programming easier and more efficient.  Finch.jl leverages compiler technology to automatically generate customized, fused sparse kernels for each specific use case. This allows users to write readable, high-level sparse array programs without worrying about the performance of the generated code. Finch can automatically generate efficient implementations even for unique problems that lack existing library solutions.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"At the Julia REPL, install the latest stable version by running:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg;\n       Pkg.add(\"Finch\");\n","category":"page"},{"location":"#Quickstart","page":"Home","title":"Quickstart","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using Finch\n\n# Create a sparse tensor\n\njulia> A = Tensor(CSCFormat(), [1 0 0; 0 2 0; 0 0 3])\n3×3 Tensor{DenseLevel{Int64, SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 1.0  0.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  3.0\n\n# Perform a simple operation\n\njulia> B = A + A\n3×3 Tensor{DenseLevel{Int64, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:\n 2.0  0.0  0.0\n 0.0  4.0  0.0\n 0.0  0.0  6.0","category":"page"},{"location":"#Sparse-and-Structured-Tensors","page":"Home","title":"Sparse and Structured Tensors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch supports most major sparse formats (CSR, CSC, DCSR, DCSC, CSF, COO, Hash, Bytemap). Finch also allows users to define their own sparse formats with a parameterized format language.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CSC_matrix = Tensor(CSCFormat())\nCSR_matrix = swizzle(Tensor(CSCFormat()), 2, 1)\nCSF_tensor = Tensor(CSFFormat(3))","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finch also supports a wide variety of array structure beyond sparsity. Whether you're dealing with custom background (zero) values, run-length encoding, or matrices with special structures like banded or triangular matrices, Finch’s compiler can understand and optimize various data patterns and computational rules to adapt to the structure of data.","category":"page"},{"location":"#Examples:","page":"Home","title":"Examples:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch supports many high-level array operations out of the box, such as +, *, maximum, sum, map, broadcast, and reduce.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Finch\n\n# Define sparse tensor A\n\njulia> A = Tensor(Dense(SparseList(Element(0.0))), [0 1.1 0; 2.2 0 3.3; 4.4 0 0; 0 0 5.5])\n\n# Define sparse tensor B\n\njulia> B = Tensor(Dense(SparseList(Element(0.0))), [0 1 1; 1 0 0; 0 0 1; 0 0 1])\n\n# Element-wise multiplication\n\njulia> C = A .* B\n\n# Element-wise max\n\njulia> C = max.(A, B)\n\n# Sum over rows\n\njulia> D = sum(C; dims=2)\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"For situations where more complex operations are needed, Finch supports an @einsum syntax on sparse and structured tensors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> @einsum E[i] += A[i, j] * B[i, j]\n\njulia> @einsum F[i, k] << max >>= A[i, j] + B[j, k]\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finch even allows users to fuse multiple operations into a single kernel with lazy and compute.  The lazy function creates a lazy tensor, which is a symbolic representation of the computation. The compute function evaluates the computation. Different optimizers can be used with compute, such as the state-of-the-art Galley optimizer, which can adapt to the sparsity patterns of the inputs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Finch, BenchmarkTools\n\njulia> A = fsprand(1000, 1000, 0.1);\n       B = fsprand(1000, 1000, 0.1);\n       C = fsprand(1000, 1000, 0.0001);\n\njulia> A = lazy(A);\n       B = lazy(B);\n       C = lazy(C);\n\njulia> sum(A * B * C)\n\njulia> @btime compute(sum(A * B * C));\n  263.612 ms (1012 allocations: 185.08 MiB)\n\njulia> @btime compute(sum(A * B * C), ctx=galley_scheduler());\n  153.708 μs (667 allocations: 29.02 KiB)","category":"page"},{"location":"#How-it-Works","page":"Home","title":"How it Works","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch first translates high-level array code into FinchLogic, a custom intermediate representation that captures operator fusion and enables loop ordering optimizations. Using advanced schedulers, Finch optimizes FinchLogic and lowers it to FinchNotation, a more refined representation that precisely defines control flow. This optimized FinchNotation is then compiled into highly efficient, sparsity-aware code. Finch can specialize to each combination of sparse formats and algebraic properties, such as x * 0 => 0, eliminating unnecessary computations in sparse code automatically.","category":"page"},{"location":"#Learn-More","page":"Home","title":"Learn More","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following manuscripts provide a good description of the research behind Finch:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finch: Sparse and Structured Array Programming with Control Flow. Willow Ahrens, Teodoro Fields Collin, Radha Patel, Kyle Deeds, Changwan Hong, Saman Amarasinghe.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Looplets: A Language for Structured Coiteration. CGO 2023. Willow Ahrens, Daniel Donenfeld, Fredrik Kjolstad, Saman Amarasinghe.","category":"page"},{"location":"#Beyond-Finch","page":"Home","title":"Beyond Finch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following research efforts use Finch:","category":"page"},{"location":"","page":"Home","title":"Home","text":"SySTeC: A Symmetric Sparse Tensor Compiler. Radha Patel, Willow Ahrens, Saman Amarasinghe.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Continuous Tensor Abstraction: Where Indices are Real. Jaeyeon Won, Willow Ahrens, Joel S. Emer, Saman Amarasinghe.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Galley: Modern Query Optimization for Sparse Tensor Programs. Kyle Deeds, Willow Ahrens, Magda Balazinska, Dan Suciu.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are welcome! Please see our contribution guidelines for more information.","category":"page"},{"location":"appendices/listing/#Documentation-Listing","page":"Documentation Listing","title":"Documentation Listing","text":"","category":"section"},{"location":"appendices/listing/","page":"Documentation Listing","title":"Documentation Listing","text":"Modules = [Finch, Finch.FinchNotation]","category":"page"},{"location":"appendices/listing/#Finch.bandmask-appendices-listing","page":"Documentation Listing","title":"Finch.bandmask","text":"bandmask\n\nA mask for a banded tensor, bandmask[i, j, k] = j <= i <= k. Note that this specializes each column for the cases where i < j, j <= i <= k, and k < i.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.diagmask-appendices-listing","page":"Documentation Listing","title":"Finch.diagmask","text":"diagmask\n\nA mask for a diagonal tensor, diagmask[i, j] = i == j. Note that this specializes each column for the cases where i < j, i == j, and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.lotrimask-appendices-listing","page":"Documentation Listing","title":"Finch.lotrimask","text":"lotrimask\n\nA mask for an upper triangular tensor, lotrimask[i, j] = i >= j. Note that this specializes each column for the cases where i < j and i >= j.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.uptrimask-appendices-listing","page":"Documentation Listing","title":"Finch.uptrimask","text":"uptrimask\n\nA mask for an upper triangular tensor, uptrimask[i, j] = i <= j. Note that this specializes each column for the cases where i <= j and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Core.Array-Tuple{Union{Finch.SwizzleArray, Tensor}}-appendices-listing","page":"Documentation Listing","title":"Core.Array","text":"Array(arr::Union{Tensor, SwizzleArray})\n\nConstruct an array from a tensor or swizzle. May reuse memory, will usually densify the tensor.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.AbstractDevice-appendices-listing","page":"Documentation Listing","title":"Finch.AbstractDevice","text":"AbstractDevice\n\nA datatype representing a device on which tasks can be executed.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.AbstractTask-appendices-listing","page":"Documentation Listing","title":"Finch.AbstractTask","text":"AbstractTask\n\nAn individual processing unit on a device, responsible for running code.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.AtomicElementLevel-appendices-listing","page":"Documentation Listing","title":"Finch.AtomicElementLevel","text":"AtomicElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nLike an ElementLevel, but updates to the level are performed atomically.\n\njulia> tensor_tree(Tensor(Dense(AtomicElement(0.0)), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.CPU-appendices-listing","page":"Documentation Listing","title":"Finch.CPU","text":"CPU(n)\n\nA device that represents a CPU with n threads.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DefaultLogicOptimizer-appendices-listing","page":"Documentation Listing","title":"Finch.DefaultLogicOptimizer","text":"DefaultLogicOptimizer(ctx)\n\nThe default optimizer for finch logic programs. Optimizes to a structure suitable for the LogicCompiler or LogicInterpreter, then calls ctx on the resulting program.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DenseData-appendices-listing","page":"Documentation Listing","title":"Finch.DenseData","text":"DenseData(lvl)\n\nRepresents a tensor A where each A[:, ..., :, i] is represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DenseLevel-appendices-listing","page":"Documentation Listing","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\njulia> ndims(Tensor(Dense(Element(0.0))))\n1\n\njulia> ndims(Tensor(Dense(Dense(Element(0.0)))))\n2\n\njulia> tensor_tree(Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4]))\n2×2-Tensor\n└─ Dense [:,1:2]\n   ├─ [:, 1]: Dense [1:2]\n   │  ├─ [1]: 1.0\n   │  └─ [2]: 3.0\n   └─ [:, 2]: Dense [1:2]\n      ├─ [1]: 2.0\n      └─ [2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DeviceGlobal-appendices-listing","page":"Documentation Listing","title":"Finch.DeviceGlobal","text":"DeviceGlobal()\n\nFrom the device, load the global view of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DeviceLocal-appendices-listing","page":"Documentation Listing","title":"Finch.DeviceLocal","text":"DeviceLocal()\n\nFrom the device, load the local version of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.DeviceShared-appendices-listing","page":"Documentation Listing","title":"Finch.DeviceShared","text":"DeviceShared()\n\nFrom the device, load the shared view of the tensor.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.ElementData-appendices-listing","page":"Documentation Listing","title":"Finch.ElementData","text":"ElementData(fill_value, eltype)\n\nRepresents a scalar element of type eltype and fillvalue `fillvalue`.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.ElementLevel-appendices-listing","page":"Documentation Listing","title":"Finch.ElementLevel","text":"ElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to Vf. Vf may optionally be given as the first argument.\n\nThe data is stored in a vector of type Val with eltype(Val) = Tv. The type Tp is the index type used to access Val.\n\njulia> tensor_tree(Tensor(Dense(Element(0.0)), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.ExtrudeData-appendices-listing","page":"Documentation Listing","title":"Finch.ExtrudeData","text":"ExtrudeData(lvl)\n\nRepresents a tensor A where A[:, ..., :, 1] is the only slice, and is represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.FinchCompiler-appendices-listing","page":"Documentation Listing","title":"Finch.FinchCompiler","text":"FinchCompiler\n\nThe core compiler for Finch, lowering canonicalized Finch IR to Julia code.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.HollowData-appendices-listing","page":"Documentation Listing","title":"Finch.HollowData","text":"HollowData(lvl)\n\nRepresents a tensor which is represented by lvl but is sometimes entirely fill_value(lvl).\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.HostGlobal-appendices-listing","page":"Documentation Listing","title":"Finch.HostGlobal","text":"HostGlobal()\n\nFrom the host, distribute the tensor to device global memory.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.HostLocal-appendices-listing","page":"Documentation Listing","title":"Finch.HostLocal","text":"HostLocal()\n\nFrom the host, distribute the tensor to device local memory.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.HostShared-appendices-listing","page":"Documentation Listing","title":"Finch.HostShared","text":"HostShared()\n\nFrom the host, distribute the tensor to device shared memory.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.Infinitesimal-appendices-listing","page":"Documentation Listing","title":"Finch.Infinitesimal","text":"Infintesimal(s)\n\nThe Infintesimal type represents an infinitestimal number.  The sign field is used to represent positive, negative, or zero in this number system.\n\n```jl-doctest julia> tiny() +0\n\njulia> positive_tiny() +ϵ\n\njulia> negative_tiny() -ϵ\n\njulia> positivetiny() + negativetiny() +0\n\njulia> positive_tiny() * 2 +ϵ\n\njulia> positivetiny() * negativetiny() -ϵ\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.InitMax-Union{Tuple{D}, Tuple{Any, Any}} where D-appendices-listing","page":"Documentation Listing","title":"Finch.InitMax","text":"InitMax{D}(x, y)\n\nReturns max(x, y, D). Used to add an init parameter to a maximum operation.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.InitMin-Union{Tuple{D}, Tuple{Any, Any}} where D-appendices-listing","page":"Documentation Listing","title":"Finch.InitMin","text":"InitMin{D}(x, y)\n\nReturns min(x, y, D). Used to add an init parameter to a minimum operation.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.JuliaContext-appendices-listing","page":"Documentation Listing","title":"Finch.JuliaContext","text":"JuliaContext\n\nA context for compiling Julia code, managing side effects, parallelism, and variable names in the generated code of the executing environment.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.Limit-appendices-listing","page":"Documentation Listing","title":"Finch.Limit","text":"Limit{T}(x, s)\n\nThe Limit type represents endpoints of closed and open intervals.  The val field is the value of the endpoint.  The sign field is used to represent the openness/closedness of the interval endpoint, using an Infinitesmal.\n\n```jl-doctest julia> limit(1.0) 1.0+0\n\njulia> plus_eps(1.0) 1.0+ϵ\n\njulia> minus_eps(1.0) 1.0-ϵ\n\njulia> pluseps(1.0) + minuseps(1.0) 2.0+0.0\n\njulia> plus_eps(1.0) * 2 2.0+2.0ϵ\n\njulia> pluseps(1.0) * minuseps(1.0) 1.0-1.0ϵ\n\njulia> pluseps(-1.0) * minuseps(1.0) -1.0+2.0ϵ\n\njulia> 1.0 < plus_eps(1.0) true\n\njulia> 1.0 < minus_eps(1.0) false\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.LogicCompiler-appendices-listing","page":"Documentation Listing","title":"Finch.LogicCompiler","text":"LogicCompiler\n\nThe LogicCompiler is a simple compiler for finch logic programs. The interpreter is only capable of executing programs of the form:       REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)        ACCESS := reorder(relabel(ALIAS, idxs1::FIELD...), idxs2::FIELD...) where issubsequence(idxs1, idxs2)     POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE     MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)        TABLE  := table(IMMEDIATE | DEFERRED, FIELD...) COMPUTEQUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))   INPUTQUERY := query(ALIAS, TABLE)          STEP := COMPUTEQUERY | INPUTQUERY | produces(ALIAS...)          ROOT := PLAN(STEP...)\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.LogicExecutor-appendices-listing","page":"Documentation Listing","title":"Finch.LogicExecutor","text":"LogicExecutor(ctx, tag=:global, verbose=false)\n\nExecutes a logic program by compiling it with the given compiler ctx. Compiled codes are cached, and are only compiled once for each program with the same structure. The tag argument is used to distinguish between different use cases for the same program structure.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.LogicExecutorCode-appendices-listing","page":"Documentation Listing","title":"Finch.LogicExecutorCode","text":"LogicExecutorCode(ctx)\n\nReturn the code that would normally be used by the LogicExecutor to run a program.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.LogicInterpreter-appendices-listing","page":"Documentation Listing","title":"Finch.LogicInterpreter","text":"LogicInterpreter(scope = Dict(), verbose = false, mode = :fast)\n\nThe LogicInterpreter is a simple interpreter for finch logic programs. The interpreter is only capable of executing programs of the form:       REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)        ACCESS := reorder(relabel(ALIAS, idxs1::FIELD...), idxs2::FIELD...) where issubsequence(idxs1, idxs2)     POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE     MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)        TABLE  := table(IMMEDIATE, FIELD...) COMPUTEQUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))   INPUTQUERY := query(ALIAS, TABLE)          STEP := COMPUTEQUERY | INPUTQUERY | produces(ALIAS...)          ROOT := PLAN(STEP...)\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.MutexLevel-appendices-listing","page":"Documentation Listing","title":"Finch.MutexLevel","text":"MutexLevel{Val, Lvl}()\n\nMutex Level Protects the level directly below it with atomics\n\nEach position in the level below the Mutex level is protected by a lock.\n\njulia> tensor_tree(Tensor(Dense(Mutex(Element(0.0))), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Mutex ->\n   │  └─ 1.0\n   ├─ [2]: Mutex ->\n   │  └─ 2.0\n   └─ [3]: Mutex ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.Namespace-appendices-listing","page":"Documentation Listing","title":"Finch.Namespace","text":"Namespace\n\nA namespace for managing variable names and aesthetic fresh variable generation.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.PatternLevel-appendices-listing","page":"Documentation Listing","title":"Finch.PatternLevel","text":"PatternLevel{[Tp=Int]}()\n\nA subfiber of a pattern level is the Boolean value true, but it's fill_value is false. PatternLevels are used to create tensors that represent which values are stored by other fibers. See pattern! for usage examples.\n\njulia> tensor_tree(Tensor(Dense(Pattern()), 3))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: true\n   ├─ [2]: true\n   └─ [3]: true\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.RepeatData-appendices-listing","page":"Documentation Listing","title":"Finch.RepeatData","text":"RepeatData(lvl)\n\nRepresents a tensor A where A[:, ..., :, i] is sometimes entirely fill_value(lvl) and is sometimes represented by repeated runs of lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.RunListLevel-appendices-listing","page":"Documentation Listing","title":"Finch.RunListLevel","text":"RunListLevel{[Ti=Int], [Ptr, Right]}(lvl, [dim], [merge = true])\n\nThe RunListLevel represent runs of equivalent slices A[:, ..., :, i]. A sorted list is used to record the right endpoint of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> tensor_tree(Tensor(Dense(RunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: RunList (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  ├─ [2:2]: 30.0\n   │  └─ [3:3]: 0.0\n   ├─ [:, 2]: RunList (0.0) [1:3]\n   │  └─ [1:3]: 0.0\n   └─ [:, 3]: RunList (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      ├─ [2:2]: 0.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.ScopeContext-appendices-listing","page":"Documentation Listing","title":"Finch.ScopeContext","text":"ScopeContext\n\nA context for managing variable bindings and tensor modes.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SeparateLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SeparateLevel","text":"SeparateLevel{Lvl, [Val]}()\n\nA subfiber of a Separate level is a separate tensor of type Lvl, in it's own memory space.\n\nEach sublevel is stored in a vector of type Val with eltype(Val) = Lvl.\n\njulia> tensor_tree(Tensor(Dense(Separate(Element(0.0))), [1, 2, 3]))\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Pointer ->\n   │  └─ 1.0\n   ├─ [2]: Pointer ->\n   │  └─ 2.0\n   └─ [3]: Pointer ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.Serial-appendices-listing","page":"Documentation Listing","title":"Finch.Serial","text":"Serial()\n\nA device that represents a serial CPU execution.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseBandLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseBandLevel","text":"SparseBandLevel{[Ti=Int], [Idx, Ofs]}(lvl, [dim])\n\nLike the SparseBlockListLevel, but stores only a single block, and fills in zeros.\n\n```jldoctest julia> Tensor(Dense(SparseBand(Element(0.0))), [10 0 20; 30 40 0; 0 0 50]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseBlockListLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseBlockListLevel","text":"SparseBlockListLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseListLevel, but contiguous subfibers are stored together in blocks.\n\n```jldoctest julia> Tensor(Dense(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\njulia> Tensor(SparseBlockList(SparseBlockList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) SparseList (0.0) [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseByteMapLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseByteMapLevel","text":"SparseByteMapLevel{[Ti=Int], [Ptr, Tbl]}(lvl, [dims])\n\nLike the SparseListLevel, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level.\n\njulia> tensor_tree(Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseByteMap (0.0) [1:3]\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n      ├─ [1]: 0.0\n      └─ [3]: 0.0\n\njulia> tensor_tree(Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseByteMap (0.0) [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseCOOLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr, Tbl]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTI is the type of the last N tensor indices, and Tp is the type used for positions in the level.\n\nThe type Tbl is an NTuple type where each entry k is a subtype AbstractVector{TI[k]}.\n\nThe type Ptr is the type for the pointer array.\n\njulia> tensor_tree(Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseCOO{1} (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseCOO{1} (0.0) [1:3]\n   └─ [:, 3]: SparseCOO{1} (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [1, 1]: 10.0\n   ├─ [2, 1]: 30.0\n   ├─ [1, 3]: 20.0\n   └─ [3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseData-appendices-listing","page":"Documentation Listing","title":"Finch.SparseData","text":"SparseData(lvl)\n\nRepresents a tensor A where A[:, ..., :, i] is sometimes entirely fill_value(lvl) and is sometimes represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseDictLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseDictLevel","text":"SparseDictLevel{[Ti=Int], [Tp=Int], [Ptr, Idx, Val, Tbl, Pool=Dict]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A datastructure specified by Tbl is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseDict(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseDict (0.0) [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseIntervalLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseIntervalLevel","text":"SparseIntervalLevel{[Ti=Int], [Left, Right]}(lvl, [dim])\n\nThe SparseIntervalLevel represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A main difference compared to SparseRunList level is that SparseInterval level only stores a 'single' non-fill run. It emits an error if the program tries to write multiple (>=2) runs into SparseInterval.\n\nTi is the type of the last tensor index. The types Left, and 'Right' are the types of the arrays used to store positions and endpoints.\n\njulia> tensor_tree(Tensor(SparseInterval(Element(0)), [0, 10, 0]))\n3-Tensor\n└─ SparseInterval (0) [1:3]\n   └─ [2:2]: 10\n\njulia> x = Tensor(SparseInterval(Element(0)), 10);\n\njulia> @finch begin for i = extent(3,6); x[~i] = 1 end end;\n\njulia> tensor_tree(x)\n10-Tensor\n└─ SparseInterval (0) [1:10]\n   └─ [3:6]: 1\n\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseListLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseList (0.0) [1:3]\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> tensor_tree(Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparsePointLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparsePointLevel","text":"SparsePointLevel{[Ti=Int], [Idx]}(lvl, [dim])\n\nA subfiber of a SparsePoint level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. A main difference compared to SparseList level is that SparsePoint level only stores a 'single' non-fill slice. It emits an error if the program tries to write multiple (>=2) coordinates into SparsePoint.\n\nTi is the type of the last tensor index. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> tensor_tree(Tensor(Dense(SparsePoint(Element(0.0))), [10 0 0; 0 20 0; 0 0 30]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparsePoint (0.0) [1:3]\n   │  └─ [1]: 10.0\n   ├─ [:, 2]: SparsePoint (0.0) [1:3]\n   │  └─ [2]: 20.0\n   └─ [:, 3]: SparsePoint (0.0) [1:3]\n      └─ [3]: 30.0\n\njulia> tensor_tree(Tensor(SparsePoint(Dense(Element(0.0))), [0 0 0; 0 0 30; 0 0 30]))\n3×3-Tensor\n└─ SparsePoint (0.0) [:,1:3]\n   └─ [:, 3]: Dense [1:3]\n      ├─ [1]: 0.0\n      ├─ [2]: 30.0\n      └─ [3]: 30.0\n\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SparseRunListLevel-appendices-listing","page":"Documentation Listing","title":"Finch.SparseRunListLevel","text":"SparseRunListLevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim]; [merge = true])\n\nThe SparseRunListLevel represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A sorted list is used to record the left and right endpoints of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr, Left, and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> tensor_tree(Tensor(Dense(SparseRunListLevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]))\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseRunList (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  └─ [2:2]: 30.0\n   ├─ [:, 2]: SparseRunList (0.0) [1:3]\n   └─ [:, 3]: SparseRunList (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.StaticHash-appendices-listing","page":"Documentation Listing","title":"Finch.StaticHash","text":"StaticHash\n\nA hash function which is static, i.e. the hashes are the same when objects are hashed in the same order. The hash is used to memoize the results of simplification and proof rules.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SubFiber-appendices-listing","page":"Documentation Listing","title":"Finch.SubFiber","text":"SubFiber(lvl, pos)\n\nSubFiber represents a tensor at position pos within lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.SymbolicContext-appendices-listing","page":"Documentation Listing","title":"Finch.SymbolicContext","text":"SymbolicContext\n\nA compiler context for symbolic computation, defined on an algebra.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.Tensor-Tuple{Finch.AbstractLevel, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}-appendices-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl, [undef], dims...)\n\nConstruct a Tensor of size dims, and initialize to undef, potentially allocating memory.  Here undef is the UndefInitializer singleton type. dims... may be a variable number of dimensions or a tuple of dimensions, but it must correspond to the number of dimensions in lvl.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.Tensor-Tuple{Lvl} where Lvl<:Finch.AbstractLevel-appendices-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl)\n\nConstruct a Tensor using the tensor level storage lvl. No initialization of storage is performed, it is assumed that position 1 of lvl corresponds to a valid tensor, and lvl will be wrapped as-is. Call a different constructor to initialize the storage.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.Tensor-Union{Tuple{AbstractArray{Tv, N}}, Tuple{N}, Tuple{Tv}, Tuple{AbstractArray{Tv, N}, Tv}} where {Tv, N}-appendices-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(arr, [init = zero(eltype(arr))])\n\nCopy an array-like object arr into a corresponding, similar Tensor datastructure. Uses init as an initial value. May reuse memory when possible. To explicitly copy into a tensor, use @ref[copyto!].\n\nExamples\n\njulia> println(summary(Tensor(sparse([1 0; 0 1]))))\n2×2 Tensor(Dense(SparseList(Element(0))))\n\njulia> println(summary(Tensor(ones(3, 2, 4))))\n3×2×4 Tensor(Dense(Dense(Dense(Element(0.0)))))\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.Tensor-appendices-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor{Lvl} <: AbstractFiber{Lvl}\n\nThe multidimensional array type used by Finch. Tensor is a thin wrapper around the hierarchical level storage of type Lvl.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Base.resize!-Tuple{Tensor, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Base.resize!","text":"resize!(fbr, dims...)\n\nSet the shape of fbr equal to dims. May reuse memory and render the original tensor unusable when modified.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.ByteMapFormat-appendices-listing","page":"Documentation Listing","title":"Finch.ByteMapFormat","text":"ByteMapFormat(N, z = 0.0, T = typeof(z))\n\nA byte-map based format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.COOFormat-appendices-listing","page":"Documentation Listing","title":"Finch.COOFormat","text":"COOFormat(N, z = 0.0, T = typeof(z))\n\nAn N-dimensional COO format with a fill value of z. COO stores a sparse tensor as a list of coordinates.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.CSCFormat-appendices-listing","page":"Documentation Listing","title":"Finch.CSCFormat","text":"CSCFormat(z = 0.0, T = typeof(z))\n\nA CSC format with a fill value of z. CSC stores a sparse matrix as a dense array of lists.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.CSFFormat-appendices-listing","page":"Documentation Listing","title":"Finch.CSFFormat","text":"CSFFormat(N, z = 0.0, T = typeof(z))\n\nAn N-dimensional CSC format with a fill value of z. CSF supports random access in the rightmost index, and uses a tree structure to store the rest of the data.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.DCSCFormat-appendices-listing","page":"Documentation Listing","title":"Finch.DCSCFormat","text":"DCSCFormat(z = 0.0, T = typeof(z))\n\nA DCSC format with a fill value of z. DCSC stores a sparse matrix as a list of lists.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.DCSFFormat-appendices-listing","page":"Documentation Listing","title":"Finch.DCSFFormat","text":"DCSFFormat(z = 0.0, T = typeof(z))\n\nA DCSF format with a fill value of z. DCSF stores a sparse tensor as a list of lists of lists.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.DenseFormat-appendices-listing","page":"Documentation Listing","title":"Finch.DenseFormat","text":"DenseFormat(N, z = 0.0, T = typeof(z))\n\nA dense format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.HashFormat-appendices-listing","page":"Documentation Listing","title":"Finch.HashFormat","text":"HashFormat(N, z = 0.0, T = typeof(z))\n\nA hash-table based format with a fill value of z.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.aggregate_rep-NTuple{4, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.aggregate_rep","text":"aggregate_rep(op, init, tns, dims)\n\nReturn a trait object representing the result of reducing a tensor represented by tns on dims by op starting at init.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.aquire_lock!-Tuple{Finch.AbstractDevice, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.aquire_lock!","text":"aquire_lock!(dev::AbstractDevice, val)\n\nLock the lock, val, on the device dev, waiting until it can acquire lock.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.assemble_level!-appendices-listing","page":"Documentation Listing","title":"Finch.assemble_level!","text":"assemble_level!(ctx, lvl, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.bspread-appendices-listing","page":"Documentation Listing","title":"Finch.bspread","text":"bspread(::AbstractString) bspread(::HDF5.File) bspread(::NPYPath)\n\nRead the Binsparse file into a Finch tensor.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\n\n\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.bspwrite-appendices-listing","page":"Documentation Listing","title":"Finch.bspwrite","text":"bspwrite(::AbstractString, tns)\nbspwrite(::HDF5.File, tns)\nbspwrite(::NPYPath, tns)\n\nWrite the Finch tensor to a file using Binsparse file format.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.cache_deferred!-Tuple{Any, Finch.FinchLogic.LogicNode}-appendices-listing","page":"Documentation Listing","title":"Finch.cache_deferred!","text":"cache_deferred(ctx, root::LogicNode, seen)\n\nReplace deferred expressions with simpler expressions, and cache their evaluation in the preamble.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.choose-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0\n 1.1\n 0.0\n 4.4\n 0.0\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<choose(0.0)>>= a[i] end;\n\njulia> x[]\n1.1\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.chunkmask-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.chunkmask","text":"chunkmask(n, b)\n\nA mask to evenly divide n indices into regions of size b. If m = chunkmask(b, n), thenm[i, j] = b * (j - 1) < i <= b * j`. Note that this specializes for the cleanup case at the end of the range.\n\njulia> chunkmask(10, 3)\n10×4 Finch.ChunkMask{Int64}:\n 1  0  0  0\n 1  0  0  0\n 1  0  0  0\n 0  1  0  0\n 0  1  0  0\n 0  1  0  0\n 0  0  1  0\n 0  0  1  0\n 0  0  1  0\n 0  0  0  1\n\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.cld_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.cld_nothrow","text":"cld_nothrow(x, y)\n\nReturns cld(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.collapse_rep-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.collapse_rep","text":"collapse_rep(tns)\n\nNormalize a trait object to collapse subfiber information into the parent tensor.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.collapsed-NTuple{6, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.collapsed","text":"collapsed(algebra, f, idx, ext, node)\n\nReturn collapsed expression with respect to f.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.combinedim-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.combinedim","text":"combinedim(ctx, a, b)\n\nCombine the two dimensions a and b.  To avoid ambiguity, only define one of\n\ncombinedim(ctx, ::A, ::B)\ncombinedim(ctx, ::B, ::A)\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.compute-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.compute","text":"compute(args...; ctx=default_scheduler(), kwargs...) -> Any\n\nCompute the value of a lazy tensor. The result is the argument itself, or a tuple of arguments if multiple arguments are passed. Some keyword arguments can be passed to control the execution of the program:     - verbose=false: Print the generated code before execution     - tag=:global: A tag to distinguish between different classes of inputs for the same program.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.concordize-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.concordize","text":"concordize(root)\n\nAccepts a program of the following form:\n\n        TABLE := table(IMMEDIATE, FIELD...)\n       ACCESS := reorder(relabel(ALIAS, FIELD...), FIELD...)\n      COMPUTE := ACCESS |\n                 mapjoin(IMMEDIATE, COMPUTE...) |\n                 aggregate(IMMEDIATE, IMMEDIATE, COMPUTE, FIELD...) |\n                 reformat(IMMEDIATE, COMPUTE) |\n                 IMMEDIATE\nCOMPUTE_QUERY := query(ALIAS, COMPUTE)\n  INPUT_QUERY := query(ALIAS, TABLE)\n         STEP := COMPUTE_QUERY | INPUT_QUERY | produces((ALIAS | ACCESS)...)\n         ROOT := PLAN(STEP...)\n\nInserts permutation statements of the form query(ALIAS, reorder(ALIAS, FIELD...)) and updates relabels so that they match their containing reorders. Modified ACCESS statements match the form:\n\nACCESS := reorder(relabel(ALIAS, idxs_1::FIELD...), idxs_2::FIELD...) where issubsequence(idxs_1, idxs_2)\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.concordize-Tuple{Finch.AbstractCompiler, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.concordize","text":"concordize(ctx, root)\n\nA raw index is an index expression consisting of a single index node (i.e. A[i] as opposed to A[i + 1]). A Finch program is concordant when all indices are raw and column major with respect to the program loop ordering.  The concordize transformation ensures that tensor indices are concordant by inserting loops and lifting index expressions or transposed indices into the loop bounds.\n\nFor example,\n\n@finch for i = :\n    b[] += A[f(i)]\nend\n\nbecomes\n\n@finch for i = :\n    t = f(i)\n    for s = t:t\n        b[] += A[s]\n    end\nend\n\nand\n\n@finch for i = :, j = :\n    b[] += A[i, j]\nend\n\nbecomes\n\n@finch for i = :, j = :, s = i:i\n    b[] += A[s, j]\nend\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.contain-Tuple{Any, Finch.JuliaContext}-appendices-listing","page":"Documentation Listing","title":"Finch.contain","text":"contain(f, ctx)\n\nCall f on a subcontext of ctx and return the result. Variable bindings, preambles, and epilogues defined in the subcontext will not escape the call to contain.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.countstored-Tuple{Tensor}-appendices-listing","page":"Documentation Listing","title":"Finch.countstored","text":"countstored(arr)\n\nReturn the number of stored elements in arr. If there are explicitly stored fill elements, they are counted too.\n\nSee also: (SparseArrays.nnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.nnz) and (Base.summarysize)(https://docs.julialang.org/en/v1/base/base/#Base.summarysize)\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.data_rep-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.data_rep","text":"data_rep(tns)\n\nReturn a trait object representing everything that can be learned about the data based on the storage format (type) of the tensor\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.dataflow-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.dataflow","text":"dataflow(ex)\n\nRun dead code elimination and constant propagation. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.declare!-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.declare!","text":"declare!(ctx, tns, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.declare_level!-appendices-listing","page":"Documentation Listing","title":"Finch.declare_level!","text":"declare_level!(ctx, lvl, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.default_scheduler-Tuple{}-appendices-listing","page":"Documentation Listing","title":"Finch.default_scheduler","text":"default_scheduler(;verbose=false)\n\nThe default scheduler used by compute to execute lazy tensor programs. Fuses all pointwise expresions into reductions. Only fuses reductions into pointwise expressions when they are the only usage of the reduction.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.defer_tables-Tuple{Any, Finch.FinchLogic.LogicNode}-appendices-listing","page":"Documentation Listing","title":"Finch.defer_tables","text":"defer_tables(root::LogicNode)\n\nReplace immediate tensors with deferred expressions assuming the original program structure is given as input to the program.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.dimensionalize!-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.dimensionalize!","text":"dimensionalize!(prgm, ctx)\n\nA program traversal which coordinates dimensions based on shared indices. In particular, loops and declaration statements have dimensions. Accessing a tensor with a raw index hints that the loop should have a dimension corresponding to the tensor axis. Accessing a tensor on the left hand side with a raw index also hints that the tensor declaration should have a dimension corresponding to the loop axis.  All hints inside a loop body are used to evaluate loop dimensions, and all hints after a declaration until the first freeze are used to evaluate declaration dimensions. One may refer to the automatically determined dimension using a variable named _ or :. Index sharing is transitive, so A[i] = B[i] and B[j] = C[j] will induce a gathering of the dimensions of A, B, and C into one.\n\nThe dimensions are semantically evaluated just before the corresponding loop or declaration statement.  The program is assumed to be scoped, so that all loops have unique index names.\n\nSee also: virtual_size, virtual_resize!, combinedim\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.distribute-NTuple{5, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.distribute","text":"distribute(ctx, arr, device, diff, style)\n\nIf the virtual array is not on the given device, copy the array to that device. This function may modify underlying data arrays, but cannot change the virtual itself. This function is used to move data to the device before a kernel is launched. Since this function may modify the root node, iterators in-progress may need to be updated. We can store new root objects in the diff dictionary.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.dropfills!-Tuple{Finch.AbstractTensor, Finch.AbstractTensor}-appendices-listing","page":"Documentation Listing","title":"Finch.dropfills!","text":"dropfills!(dst, src)\n\nCopy only the non-fill values from src into dst.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.dropfills-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.dropfills","text":"dropfills(src)\n\nDrop the fill values from src and return a new tensor with the same shape and format.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.enforce_lifecycles-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.enforce_lifecycles","text":"enforce_lifecycles(prgm)\n\nA transformation which adds freeze and thaw statements automatically to tensor roots, depending on whether they appear on the left or right hand side.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.enforce_scopes-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.enforce_scopes","text":"enforce_scopes(prgm)\n\nA transformation which gives all loops unique index names and enforces that tensor roots are declared in a containing scope and enforces that variables are declared once within their scope. Note that loop and sieve both introduce new scopes.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.ensure_concurrent-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.ensure_concurrent","text":"ensure_concurrent(root, ctx)\n\nEnsures that all nonlocal assignments to the tensor root are consistently accessed with the same indices and associative operator.  Also ensures that the tensor is either atomic, or accessed by i and concurrent and injective on i.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.evaluate_partial-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.evaluate_partial","text":"evaluate_partial(ctx, root)\n\nThis pass evaluates tags, global variable definitions, and foldable functions into the context bindings.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.exit_on_yieldbind-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.exit_on_yieldbind","text":"exit_on_yieldbind(prgm)\n\nThis pass rewrites the program so that yieldbind expressions are only present at the end of a block. It also adds a yieldbind if not present already.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.expanddims-Tuple{Finch.AbstractTensor, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.expanddims","text":"expanddims(arr::AbstractTensor, dims)\n\nExpand the dimensions of an array by inserting a new singleton axis or axes that will appear at the dims position in the expanded array shape.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.expanddims_rep-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.expanddims_rep","text":"expanddims_rep(tns, dims)\n\nExpand the representation of tns by inserting singleton dimensions dims.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.ffindnz-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.ffindnz","text":"ffindnz(arr)\n\nReturn the nonzero elements of arr, as Finch understands arr. Returns (I..., V), where I are the coordinate vectors, one for each mode of arr, and V is a vector of corresponding nonzero values, which can be passed to fsparse.\n\nSee also: (findnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.findnz)\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fiber_ctr-appendices-listing","page":"Documentation Listing","title":"Finch.fiber_ctr","text":"fiber_ctr(tns, protos...)\n\nReturn an expression that would construct a tensor suitable to hold data with a representation described by tns. Assumes representation is collapsed.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.fill_value-appendices-listing","page":"Documentation Listing","title":"Finch.fill_value","text":"fill_value(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the \"fill\" value becomes the \"background\" value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.filterop-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.filterop","text":"filterop(z)(cond, arg)\n\nfilterop(z) is a function which returns ifelse(cond, arg, z). This operation is handy for filtering out values based on a mask or a predicate. map(filterop(0), cond, arg) is analogous to filter(x -> cond ? x: z, arg).\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0\n 1.1\n 0.0\n 4.4\n 0.0\n\njulia> x = Tensor(SparseList(Element(0.0)));\n\njulia> c = Tensor(SparseList(Element(false)), [false, false, false, true, false]);\n\njulia> @finch (x .= 0; for i=_; x[i] = filterop(0)(c[i], a[i]) end)\n(x = Tensor(SparseList{Int64}(Element{0.0, Float64, Int64}([4.4]), 5, [1, 2], [4])),)\n\njulia> x\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0\n 0.0\n 0.0\n 4.4\n 0.0\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.finch_kernel-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.finch_kernel","text":"finch_kernel(fname, args, prgm; options...)\n\nReturn a function definition for which can execute a Finch program of type prgm. Here, fname is the name of the function and args is a iterable of argument name => type pairs.\n\nSee also: @finch\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fld1_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.fld1_nothrow","text":"fld1_nothrow(x, y)\n\nReturns fld1(x, y) normally, returns one and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fld_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.fld_nothrow","text":"fld_nothrow(x, y)\n\nReturns fld(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fread-Tuple{AbstractString}-appendices-listing","page":"Documentation Listing","title":"Finch.fread","text":"fread(filename::AbstractString)\n\nRead the Finch tensor from a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.freeze!-appendices-listing","page":"Documentation Listing","title":"Finch.freeze!","text":"freeze!(ctx, tns)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. This may involve trimming any excess overallocated memory.  Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.freeze_level!-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos)\n\nFreeze all fibers in lvl. Positions 1:pos need freezing.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.freeze_level!-appendices-listing","page":"Documentation Listing","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, freeze all fibers within lvl assuming that we have potentially updated 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.freshen-Tuple{Finch.Namespace, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.freshen","text":"freshen(ctx, tags...)\n\nReturn a fresh variable in the current context named after Symbol(tags...)\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fsparse!-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.fsparse!","text":"fsparse!(I..., V,[ M::Tuple])\n\nLike fsparse, but the coordinates must be sorted and unique, and memory is reused.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fsparse-Tuple{AbstractVector, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.fsparse","text":"fsparse(I::Tuple, V,[ M::Tuple, combine]; fill_value=zero(eltype(V)))\n\nCreate a sparse COO tensor S such that size(S) == M and S[(i[q] for i = I)...] = V[q]. The combine function is used to combine duplicates. If M is not specified, it is set to map(maximum, I). If the combine function is not supplied, combine defaults to + unless the elements of V are Booleans in which case combine defaults to |. All elements of I must satisfy 1 <= I[n][q] <= M[n].  Numerical zeros are retained as structural nonzeros; to drop numerical zeros, use dropzeros!.\n\nSee also: sparse\n\nExamples\n\njulia> I = (     [1, 2, 3],     [1, 2, 3],     [1, 2, 3]);\n\njulia> V = [1.0; 2.0; 3.0];\n\njulia> fsparse(I, V) SparseCOO (0.0) [1:3×1:3×1:3] │ │ │ └─└─└─[1, 1, 1] [2, 2, 2] [3, 3, 3]       1.0       2.0       3.0\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fsprand-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.fsprand","text":"fsprand([rng],[type], M..., p, [rfn])\n\nCreate a random sparse tensor of size m in COO format. There are two cases:     - If p is floating point, the probability of any element being nonzero is     independently given by p (and hence the expected density of nonzeros is     also p).     - If p is an integer, exactly p nonzeros are distributed uniformly at     random throughout the tensor (and hence the density of nonzeros is exactly     p / prod(M)). Nonzero values are sampled from the distribution specified by rfn and have the type type. The uniform distribution is used in case rfn is not specified. The optional rng argument specifies a random number generator.\n\nSee also: (sprand)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.sprand)\n\nExamples\n\njulia> fsprand(Bool, 3, 3, 0.5)\nSparseCOO (false) [1:3,1:3]\n├─├─[1, 1]: true\n├─├─[3, 1]: true\n├─├─[2, 2]: true\n├─├─[3, 2]: true\n├─├─[3, 3]: true\n\njulia> fsprand(Float64, 2, 2, 2, 0.5)\nSparseCOO (0.0) [1:2,1:2,1:2]\n├─├─├─[2, 2, 1]: 0.6478553157718558\n├─├─├─[1, 1, 2]: 0.996665291437684\n├─├─├─[2, 1, 2]: 0.7491940599574348\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fspzeros-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.fspzeros","text":"fspzeros([type], M...)\n\nCreate a random zero tensor of size M, with elements of type type. The tensor is in COO format.\n\nSee also: (spzeros)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.spzeros)\n\nExamples\n\njulia> A = fspzeros(Bool, 3, 3)\n3×3 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{false, Bool, Int64, Vector{Bool}}}}:\n 0  0  0\n 0  0  0\n 0  0  0\n\njulia> countstored(A)\n0\n\njulia> B = fspzeros(Float64, 2, 2, 2)\n2×2×2 Tensor{SparseCOOLevel{3, Tuple{Int64, Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n[:, :, 1] =\n 0.0  0.0\n 0.0  0.0\n\n[:, :, 2] =\n 0.0  0.0\n 0.0  0.0\n\njulia> countstored(B)\n0\n\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.ftnsread-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.ftnsread","text":"ftnsread(filename)\n\nRead the contents of the FROSTT .tns file 'filename' into a Finch COO Tensor.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnsread\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.ftnswrite-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.ftnswrite","text":"ftnswrite(filename, tns)\n\nWrite a sparse Finch tensor to a FROSTT .tns file.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnswrite\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fttread-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch tensor. The tensor will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fttwrite-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch tensor to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fused-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.fused","text":"fused(f, args...; kwargs...)\n\nThis function decorator modifies f to fuse the contained array operations and optimize the resulting program. The function must return a single array or tuple of arrays.  Some keyword arguments can be passed to control the execution of the program:     - verbose=false: Print the generated code before execution     - tag=:global: A tag to distinguish between different classes of inputs for the same program.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.fwrite-Tuple{AbstractString, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.fwrite","text":"fwrite(filename::AbstractString, tns::Finch.Tensor)\n\nWrite the Finch tensor to a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_algebra-Tuple{Finch.SymbolicContext}-appendices-listing","page":"Documentation Listing","title":"Finch.get_algebra","text":"get_algebra(ctx)\n\nget the algebra used in the current context\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_binding!-Tuple{Finch.AbstractCompiler, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_binding!","text":"get_binding!(ctx, var, val)\n\nGet the binding of a variable in the context, or set it to a default value.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_binding-Tuple{Finch.AbstractCompiler, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_binding","text":"get_binding(ctx, var, val)\n\nGet the binding of a variable in the context, or return a default value.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_binding-Tuple{Finch.ScopeContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_binding","text":"get_binding(ctx, var)\n\nGet the binding of a variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_device-appendices-listing","page":"Documentation Listing","title":"Finch.get_device","text":"get_device(task::AbstractTask)\n\nReturn the device that task is running on.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.get_lock-Tuple{Finch.AbstractDevice, Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_lock","text":"get_lock(dev::AbstractDevice, arr, idx, ty)\n\nGiven a device, an array of elements of type ty, and an index to the array, idx, gets a lock of type ty associated to arr[idx] on dev.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_mode_flag-Tuple{Finch.FinchCompiler}-appendices-listing","page":"Documentation Listing","title":"Finch.get_mode_flag","text":"get_mode_flag(ctx)\n\nReturn the mode flag given in @finch mode = ?.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_num_tasks-appendices-listing","page":"Documentation Listing","title":"Finch.get_num_tasks","text":"get_num_tasks(dev::AbstractDevice)\n\nReturn the number of tasks on the device dev.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.get_parent_task-appendices-listing","page":"Documentation Listing","title":"Finch.get_parent_task","text":"get_parent_task(task::AbstractTask)\n\nReturn the task which spawned task.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.get_prove_rules-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_prove_rules","text":"get_prove_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_result-Tuple{Finch.FinchCompiler}-appendices-listing","page":"Documentation Listing","title":"Finch.get_result","text":"get_result(ctx)\n\nReturn a variable which evaluates to the result of the program which should be returned to the user.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_scheduler-Tuple{}-appendices-listing","page":"Documentation Listing","title":"Finch.get_scheduler","text":"get_scheduler()\n\nGet the current Finch scheduler used by compute to execute lazy tensor programs.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_simplify_rules-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_simplify_rules","text":"get_simplify_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_static_hash-Tuple{Finch.SymbolicContext}-appendices-listing","page":"Documentation Listing","title":"Finch.get_static_hash","text":"get_static_hash(ctx)\n\nReturn an object which can be called as a hash function. The hashes are the same when objects are hashed in the same order.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_structure-appendices-listing","page":"Documentation Listing","title":"Finch.get_structure","text":"get_structure(root::LogicNode)\n\nQuickly produce a normalized structure for a logic program. Note: the result will not be a runnable logic program, but can be hashed and compared for equality. Two programs will have equal structure if their tensors have the same type and their program structure is equivalent up to renaming.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.get_style-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_style","text":"get_style(ctx, root)\n\nreturn the style to use for lowering root in ctx. This method is used to determine which pass should be used to lower a given node. The default implementation returns DefaultStyle(). Overload the three argument form of this method, get_style(ctx, node, root) and specialize on node.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_task-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_task","text":"get_task(ctx)\n\nGet the task which will execute code in this context\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_task_num-appendices-listing","page":"Documentation Listing","title":"Finch.get_task_num","text":"get_task_num(task::AbstractTask)\n\nReturn the task number of task.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.get_tensor_mode-Tuple{Finch.ScopeContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_tensor_mode","text":"get_tensor_mode(ctx, var)\n\nGet the mode of a tensor variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.get_wrapper_rules-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.get_wrapper_rules","text":"get_wrapper_rules(ctx, depth, alg)\n\nReturn the wrapperizing rule set for Finch, which converts expressions like `A[i\n\n1]to array combinator expressions likeOffsetArray(A, (1,))`. The rules have\n\naccess to the algebra alg and the depth lookup depthOne can dispatch on thealg` trait to specialize the rule set for different algebras. These rules run after simplification so one can expect constants to be folded.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.getindex_rep-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.getindex_rep","text":"getindex_rep(tns, idxs...)\n\nReturn a trait object representing the result of calling getindex(tns, idxs...) on the tensor represented by tns. Assumes traits are in collapsed form.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.getunbound-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.getunbound","text":"getunbound(stmt)\n\nReturn an iterator over the indices in a Finch program that have yet to be bound.\n\njulia> getunbound(@finch_program for i=_; :a[i, j] += 2 end)\n[j]\njulia> getunbound(@finch_program i + j * 2 * i)\n[i, j]\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.global_memory-appendices-listing","page":"Documentation Listing","title":"Finch.global_memory","text":"global_memory(dev::AbstractDevice)\n\nReturn the default global memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.has_binding-Tuple{Finch.ScopeContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.has_binding","text":"has_binding(ctx, var)\n\nCheck if a variable is bound in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.instantiate!-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.instantiate!","text":"instantiate!(ctx, prgm)\n\nA transformation to call instantiate on tensors before executing an expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.instantiate-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.instantiate","text":"instantiate(ctx, tns, mode)\n\nProcess the tensor tns in the context ctx, just after it has been unfurled, declared, or thawed. The earliest opportunity to process tns.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.is_atomic-appendices-listing","page":"Documentation Listing","title":"Finch.is_atomic","text":"is_atomic(ctx, tns)\n\nReturns a tuple (atomicities, overall) where atomicities is a vector, indicating which indices have an atomic that guards them,\nand overall is a boolean that indicates is the last level had an atomic guarding it.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.is_concurrent-appendices-listing","page":"Documentation Listing","title":"Finch.is_concurrent","text":"is_concurrent(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating\nwhether the index can be written to without any execution state. So if a matrix returns [true, false],\nthen we can write to A[i, j] and A[i_2, j] without any shared execution state between the two, but\nwe can't write to A[i, j] and A[i, j_2] without carrying over execution state.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.is_injective-appendices-listing","page":"Documentation Listing","title":"Finch.is_injective","text":"is_injective(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating whether the access is injective in that dimension.  A dimension is injective if each index in that dimension maps to a different memory space in the underlying array.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.isannihilator-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isassociative-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.iscommutative-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isdistributive-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isdistributive","text":"isdistributive(algebra, f, g)\n\nReturn true when f(a, g(b, c)) = g(f(a, b), f(a, c)) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isidempotent-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isidentity-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isinverse-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.isinvolution-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.labelled_children-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.labelled_children","text":"labelled_children(node)\n\nReturn the children of node in a LabelledTree. You may label the children by returning a LabelledTree(key, value), which will be shown as key: value a.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.labelled_show-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.labelled_show","text":"labelled_show(node)\n\nShow the node in a LabelledTree.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.lazy-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.lazy","text":"lazy(arg)\n\nCreate a lazy tensor from an argument. All operations on lazy tensors are lazy, and will not be executed until compute is called on their result.\n\nfor example,\n\nx = lazy(rand(10))\ny = lazy(rand(10))\nz = x + y\nz = z + 1\nz = compute(z)\n\nwill not actually compute z until compute(z) is called, so the execution of x + y is fused with the execution of z + 1.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.level_axes-appendices-listing","page":"Documentation Listing","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.level_eltype-appendices-listing","page":"Documentation Listing","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.level_fill_value-appendices-listing","page":"Documentation Listing","title":"Finch.level_fill_value","text":"level_fill_value(::Type{Lvl})\n\nThe result of level_fill_value(Lvl) defines fill_value for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.level_ndims-appendices-listing","page":"Documentation Listing","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.level_size-appendices-listing","page":"Documentation Listing","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.lift_fields-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.lift_fields","text":"This one is a placeholder that places reorder statements inside aggregate and mapjoin query nodes. only works on the output of propagatefields(pushfields(prgm))\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.lift_subqueries-Tuple{Finch.FinchLogic.LogicNode}-appendices-listing","page":"Documentation Listing","title":"Finch.lift_subqueries","text":"lift_subqueries\n\nCreates a plan that lifts all subqueries to the top level of the program, with unique queries for each distinct subquery alias. This function processes the rhs of each subquery once, to carefully extract SSA form from any nested pointer structure. After calling lift_subqueries, it is safe to map over the program (recursive pointers to subquery structures will not incur exponential overhead).\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.local_memory-appendices-listing","page":"Documentation Listing","title":"Finch.local_memory","text":"local_memory(dev::AbstractDevice)\n\nReturn the default local memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.lower_global-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.lower_global","text":"lower_global(ctx, prgm)\n\nlower the program prgm at global scope in the context ctx.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.make_lock-appendices-listing","page":"Documentation Listing","title":"Finch.make_lock","text":"make_lock(ty)\n\nMakes a lock of type ty.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.map_rep-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.map_rep","text":"map_rep(f, args...)\n\nReturn a storage trait object representing the result of mapping f over storage traits args. Assumes representation is collapsed.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.materialize_squeeze_expand_productions-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.materialize_squeeze_expand_productions","text":"materializesqueezeexpand_productions(root)\n\nMakes separate kernels for squeeze and expand operations in produces statements, since swizzle does not support this.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.maxby-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch for i=_; x[] <<maxby>>= a[i] => i end;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.minby-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch for i=_; x[] <<minby>>= a[i] => i end;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.mod1_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.mod1_nothrow","text":"mod1_nothrow(x, y)\n\nReturns mod1(x, y) normally, returns one and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.mod_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.mod_nothrow","text":"mod_nothrow(x, y)\n\nReturns mod(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.offset-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.offset","text":"offset(tns, delta...)\n\nCreate an OffsetArray such that offset(tns, delta...)[i...] == tns[i .+ delta...]. The dimensions declared by an OffsetArray are shifted, so that size(offset(tns, delta...)) == size(tns) .+ delta.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.open_scope-Union{Tuple{F}, Tuple{F, Finch.ScopeContext}} where F-appendices-listing","page":"Documentation Listing","title":"Finch.open_scope","text":"open_scope(f, ctx)\n\nCall the function f(ctx_2) in a new scope ctx_2.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.parallel-appendices-listing","page":"Documentation Listing","title":"Finch.parallel","text":"parallel(ext, device=CPU(nthreads()))\n\nA dimension ext that is parallelized over device. The ext field is usually _, or dimensionless, but can be any standard dimension argument.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.pattern!-Tuple{Tensor}-appendices-listing","page":"Documentation Listing","title":"Finch.pattern!","text":"pattern!(fbr)\n\nReturn the pattern of fbr. That is, return a tensor which is true wherever fbr is structurally unequal to its fill_value. May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 2.0\n 0.0\n 3.0\n 0.0\n 4.0\n 0.0\n 5.0\n 0.0\n 6.0\n 0.0\n\njulia> pattern!(A)\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, PatternLevel{Int64}}}:\n 1\n 0\n 1\n 0\n 1\n 0\n 1\n 0\n 1\n 0\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.permissive-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.permissive","text":"permissive(tns, dims...)\n\nCreate an PermissiveArray where permissive(tns, dims...)[i...] is missing if i[n] is not in the bounds of tns when dims[n] is true.  This wrapper allows all permissive dimensions to be exempt from dimension checks, and is useful when we need to access an array out of bounds, or for padding. More formally,\n\n    permissive(tns, dims...)[i...] =\n        if any(n -> dims[n] && !(i[n] in axes(tns)[n]))\n            missing\n        else\n            tns[i...]\n        end\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.permutedims_rep-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.permutedims_rep","text":"permutedims_rep(tns, perm)\n\nReturn a trait object representing the result of permuting a tensor represented by tns to the permutation perm.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.postype-appendices-listing","page":"Documentation Listing","title":"Finch.postype","text":"postype(lvl)\n\nReturn a position type with the same flavor as those used to store the positions of the fibers contained in lvl. The name position descends from the pos or position or pointer arrays found in many definitions of CSR or CSC. In Finch, positions should be data used to access either a subfiber or some other similar auxiliary data. Thus, we often end up iterating over positions.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.pretty-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.pretty","text":"pretty(ex)\n\nMake ex prettier. Shorthand for ex |> unblock |> striplines |> regensym.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.products-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.products","text":"products(tns, dim)\n\nCreate a ProductArray such that\n\n    products(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] * i[dim + 1], i[dim + 2:end]...]\n\nThis is like toeplitz but with times instead of plus.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.propagate_transpose_queries-appendices-listing","page":"Documentation Listing","title":"Finch.propagate_transpose_queries","text":"propagatetransposequeries(root)\n\nRemoves non-materializing permutation queries by propagating them to the expressions they contain. Pushes fields and also removes copies. Removes queries of the form:\n\n    query(ALIAS, reorder(relabel(ALIAS, FIELD...), FIELD...))\n\nDoes not remove queries which define production aliases.\n\nAccepts programs of the form:\n\n       TABLE  := table(IMMEDIATE, FIELD...)\n       ACCESS := reorder(relabel(ALIAS, FIELD...), FIELD...)\n    POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE\n    MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)\n  INPUT_QUERY := query(ALIAS, TABLE)\nCOMPUTE_QUERY := query(ALIAS, reformat(IMMEDIATE, MAPREDUCE)) | query(ALIAS, MAPREDUCE))\n         PLAN := plan(STEP...)\n         STEP := COMPUTE_QUERY | INPUT_QUERY | PLAN | produces((ALIAS | ACCESS)...)\n         ROOT := STEP\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.protocolize-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.protocolize","text":"protocolize(tns, protos...)\n\nCreate a ProtocolizedArray that accesses dimension n with protocol protos[n], if protos[n] is not nothing. See the documention for Iteration Protocols for more information. For example, to gallop along the inner dimension of a matrix A, we write A[gallop(i), j], which becomes protocolize(A, gallop, nothing)[i, j].\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.prove-Tuple{Finch.SymbolicContext, Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.prove","text":"prove(ctx, root; verbose = false)\n\nuse the rules in ctx to attempt to prove that the program root is true. Return false if the program cannot be shown to be true.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.push_epilogue!-Tuple{Finch.JuliaContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.push_epilogue!","text":"push_epilogue!(ctx, thunk)\n\nPush the thunk onto the epilogue in the currently executing context. The epilogue will be evaluated after the code returned by the given function in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.push_fields-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.push_fields","text":"push_fields(node)\n\nThis program modifies all EXPR statements in the program, as defined in the following grammar:\n\n    LEAF := relabel(ALIAS, FIELD...) |\n            table(IMMEDIATE, FIELD...) |\n            IMMEDIATE\n    EXPR := LEAF |\n            reorder(EXPR, FIELD...) |\n            relabel(EXPR, FIELD...) |\n            mapjoin(IMMEDIATE, EXPR...) |\n            aggregate(IMMEDIATE, IMMEDIATE, EXPR, FIELD...)\n\nPushes all reorder and relabel statements down to LEAF nodes of each EXPR. Output LEAF nodes will match the form reorder(relabel(LEAF, FIELD...), FIELD...), omitting reorder or relabel if not present as an ancestor of the LEAF in the original EXPR. Tables and immediates will absorb relabels.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.push_preamble!-Tuple{Finch.JuliaContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.push_preamble!","text":"push_preamble!(ctx, thunk)\n\nPush the thunk onto the preamble in the currently executing context. The preamble will be evaluated before the code returned by the given function in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.reassemble_level!-appendices-listing","page":"Documentation Listing","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_fill_value(lvl).  Not avaliable on all level types as this presumes updating.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.redistribute-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.redistribute","text":"redistribute(ctx, node, diff)\n\nWhen the root node is distributed, several iterators may need to be updated.\n\nThe redistribute function traverses tns and updates it based on the updated objects in the diff dictionary.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.refresh-Tuple{}-appendices-listing","page":"Documentation Listing","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.regensym-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.regensym","text":"regensym(ex)\n\nGive gensyms prettier names by renumbering them. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.release_lock!-Tuple{Finch.AbstractDevice, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.release_lock!","text":"release_lock!(dev::AbstractDevice, val)\n\nRelease the lock, val, on the device dev.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.rem_nothrow-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.rem_nothrow","text":"rem_nothrow(x, y)\n\nReturns rem(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.rep_construct-appendices-listing","page":"Documentation Listing","title":"Finch.rep_construct","text":"rep_construct(tns, protos...)\n\nConstruct a tensor suitable to hold data with a representation described by tns. Assumes representation is collapsed.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.return_type-Tuple{Any, Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.return_type","text":"return_type(algebra, f, arg_types...)\n\nGive the return type of f when applied to arguments of types arg_types... in algebra. Used to determine output types of functions in the high-level interface. This function falls back to Base.promote_op.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.scale-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.scale","text":"scale(tns, delta...)\n\nCreate a ScaleArray such that scale(tns, delta...)[i...] == tns[i .* delta...].  The dimensions declared by an OffsetArray are shifted, so that size(scale(tns, delta...)) == size(tns) .* delta.  This is only supported on tensors with real-valued dimensions.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.scansearch-Union{Tuple{T2}, Tuple{T1}, Tuple{Any, Any, T1, T2}} where {T1<:Integer, T2<:Integer}-appendices-listing","page":"Documentation Listing","title":"Finch.scansearch","text":"scansearch(v, x, lo, hi)\n\nreturn the first value of v greater than or equal to x, within the range lo:hi. Return hi+1 if all values are less than x. This implemantation uses an exponential search strategy which involves two steps: 1) searching for binary search bounds via exponential steps rightward 2) binary searching within those bounds.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_binding!-Tuple{Finch.ScopeContext, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_binding!","text":"set_binding!(ctx, var, val)\n\nSet the binding of a variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_declared!-Tuple{Finch.ScopeContext, Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_declared!","text":"set_declared!(ctx, var, val, op)\n\nMark a tensor variable as declared in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_fill_value!-Tuple{Tensor, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_fill_value!","text":"set_fill_value!(fbr, init)\n\nReturn a tensor which is equal to fbr, but with the fill (implicit) value set to init.  May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 2.0\n 0.0\n 3.0\n 0.0\n 4.0\n 0.0\n 5.0\n 0.0\n 6.0\n 0.0\n\njulia> set_fill_value!(A, Inf)\n10 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{Inf, Float64, Int64, Vector{Float64}}}}:\n  2.0\n Inf\n  3.0\n Inf\n  4.0\n Inf\n  5.0\n Inf\n  6.0\n Inf\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_frozen!-Tuple{Finch.ScopeContext, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_frozen!","text":"set_frozen!(ctx, var, val)\n\nMark a tensor variable as frozen in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_loop_order-appendices-listing","page":"Documentation Listing","title":"Finch.set_loop_order","text":"setlooporder(root)\n\nHeuristically chooses a total order for all loops in the program by inserting reorder statments inside reformat, query, and aggregate nodes.\n\nAccepts programs of the form:\n\n      REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)\n       ACCESS := reorder(relabel(ALIAS, idxs_1::FIELD...), idxs_2::FIELD...) where issubsequence(idxs_1, idxs_2)\n    POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE\n    MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)\n       TABLE  := table(IMMEDIATE, FIELD...)\nCOMPUTE_QUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))\n  INPUT_QUERY := query(ALIAS, TABLE)\n         STEP := COMPUTE_QUERY | INPUT_QUERY\n         ROOT := PLAN(STEP..., produces(ALIAS...))\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.set_scheduler!-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_scheduler!","text":"set_scheduler!(scheduler)\n\nSet the current scheduler to scheduler. The scheduler is used by compute to execute lazy tensor programs.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.set_thawed!-Tuple{Finch.ScopeContext, Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.set_thawed!","text":"set_thawed!(ctx, var, val, op)\n\nMark a tensor variable as thawed in the context.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.shared_memory-appendices-listing","page":"Documentation Listing","title":"Finch.shared_memory","text":"shared_memory(dev::AbstractDevice)\n\nReturn the default shared memory space of dev.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.simplify-Tuple{Finch.SymbolicContext, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.simplify","text":"simplify(ctx, node)\n\nsimplify the program node using the rules in ctx\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.splitmask-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.splitmask","text":"splitmask(n, P)\n\nA mask to evenly divide n indices into P regions. If M = splitmask(P, n), then M[i, j] = fld(n * (j - 1), P) <= i < fld(n * j, P).\n\njulia> splitmask(10, 3)\n10×3 Finch.SplitMask{Int64}:\n 1  0  0\n 1  0  0\n 1  0  0\n 0  1  0\n 0  1  0\n 0  1  0\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.striplines-Tuple{Expr}-appendices-listing","page":"Documentation Listing","title":"Finch.striplines","text":"striplines(ex)\n\nRemove line numbers. ex is the target Julia expression\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.swizzle-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.swizzle","text":"swizzle(tns, dims)\n\nCreate a SwizzleArray to transpose any tensor tns such that\n\n    swizzle(tns, dims)[i...] == tns[i[dims]]\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.tensor_tree-Tuple{Finch.AbstractTensor}-appendices-listing","page":"Documentation Listing","title":"Finch.tensor_tree","text":"tensor_tree(tns; nmax = 2)\n\nPrint a tree representation of the tensor tns to the standard output. nmax is half the maximum number of children to show before truncating.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.tensor_tree-Tuple{IO, Finch.AbstractTensor}-appendices-listing","page":"Documentation Listing","title":"Finch.tensor_tree","text":"tensor_tree(io::IO, tns; nmax = 2)\n\nPrint a tree representation of the tensor tns to io. nmax is half the maximum number of children to show before truncating.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.thaw!-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.thaw!","text":"thaw!(ctx, tns)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.thaw_level!-appendices-listing","page":"Documentation Listing","title":"Finch.thaw_level!","text":"thaw_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, thaw all fibers within lvl assuming that we have previously assembled and frozen 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.toeplitz-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.toeplitz","text":"toeplitz(tns, dim)\n\nCreate a ToeplitzArray such that\n\n    Toeplitz(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] + i[dim + 1], i[dim + 2:end]...]\n\nThe ToplitzArray can be thought of as adding a dimension that shifts another dimension of the original tensor.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.transfer-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.transfer","text":"transfer(device, arr)\n\nIf the array is not on the given device, it creates a new version of this array on that device and copies the data in to it, according to the device trait. If the device is simply a data buffer, we copy the array into the buffer.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.unblock-Tuple{Expr}-appendices-listing","page":"Documentation Listing","title":"Finch.unblock","text":"unblock(ex)\n\nFlatten any redundant blocks into a single block, over the whole expression. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.unfurl-NTuple{5, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.unfurl","text":"unfurl(ctx, tns, ext, proto)\n\nReturn an array object (usually a looplet nest) for lowering the outermost dimension of virtual tensor tns. ext is the extent of the looplet. proto is the protocol that should be used for this index, but one doesn't need to unfurl all the indices at once.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.unquote_literals-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.unquote_literals","text":"unquote_literals(ex)\n\nunquote QuoteNodes when this doesn't change the semantic meaning. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.unresolve-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.unresolve","text":"unresolve(ex)\n\nUnresolve function literals into function symbols. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.virtual_call-Tuple{Any, Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_call","text":"virtual_call(ctx, f, args...)\n\nGiven the virtual arguments args..., and a literal function f, return a virtual object representing the result of the function call. If the function is not foldable, return nothing. This function is used so that we can call e.g. tensor wrapper constructors and dimension constructors in finch code. Implementations should overload virtual_call_def to provide the actual implementation.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.virtual_eltype-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_eltype","text":"virtual_eltype(arr)\n\nReturn the element type of the virtual tensor arr.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.virtual_fill_value-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_fill_value","text":"virtual fill_value(arr)\n\nReturn the initializer for virtual array arr.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.virtual_resize!-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_resize!","text":"virtual_resize!(ctx, tns, dims...)\n\nResize tns in the context ctx. This is a function similar in spirit to Base.resize!.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.virtual_size-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_size","text":"virtual_size(ctx, tns)\n\nReturn a tuple of the dimensions of tns in the context ctx. This is a function similar in spirit to Base.axes.\n\n\n\n\n\n","category":"function"},{"location":"appendices/listing/#Finch.virtual_type-Tuple{Any, Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.virtual_type","text":"virtual_type(ctx, algebra, arg)\n\nReturn the narrowest type constraint on the argument arg that is compatible with the algebra.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.virtualize-NTuple{4, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.virtualize","text":"virtualize(ctx, ex, T, [tag])\n\nReturn the virtual program corresponding to the Julia expression ex of type T in the JuliaContext ctx. Implementaters may support the optional tag argument is used to name the resulting virtual variable.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.window-Tuple{Any, Vararg{Any}}-appendices-listing","page":"Documentation Listing","title":"Finch.window","text":"window(tns, dims)\n\nCreate a WindowedArray which represents a view into another tensor\n\n    window(tns, dims)[i...] == tns[dim[1][i], dim[2][i], ...]\n\nThe windowed array restricts the new dimension to the dimension of valid indices of each dim. The dims may also be nothing to represent a full view of the underlying dimension.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.with_scheduler-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.with_scheduler","text":"with_scheduler(f, scheduler)\n\nExecute f with the current scheduler set to scheduler.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.wrapperize-Tuple{Finch.AbstractCompiler, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.wrapperize","text":"wrapperize(ctx, root)\n\nConvert index expressions in the program root to wrapper arrays, according to the rules in get_wrapper_rules. By default, the following transformations are performed:\n\nA[i - j] => A[i + (-j)]\nA[3 * i] => ScaleArray(A, (3,))[i]\nA[i * j] => ProductArray(A, 1)[i, j]\nA[i + 1] => OffsetArray(A, (1,))[i]\nA[i + j] => ToeplitzArray(A, 1)[i, j]\nA[~i] => PermissiveArray(A, 1)[i]\n\nThe loop binding order may be used to determine which index comes first in an expression like A[i + j]. Thus, for i=:,j=:; ... A[i + j] will result in ToeplitzArray(A, 1)[j, i], but for j=:,i=:; ... A[i + j] results in ToeplitzArray(A, 1)[i, j]. wrapperize runs before dimensionalization, so resulting raw indices may participate in dimensionalization according to the semantics of the wrapper.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.@barrier-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.@barrier","text":"@barrier args... ex\n\nWrap ex in a let block that captures all free variables in ex that are bound in the arguments. This is useful for ensuring that the variables in ex are not mutated by the arguments.\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@closure-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.@closure","text":"@closure closure_expression\n\nWrap the closure definition closure_expression in a let block to encourage the julia compiler to generate improved type information.  For example:\n\ncallfunc(f) = f()\n\nfunction foo(n)\n   for i=1:n\n       if i >= n\n           # Unlikely event - should be fast.  However, capture of `i` inside\n           # the closure confuses the julia-0.6 compiler and causes it to box\n           # the variable `i`, leading to a 100x performance hit if you remove\n           # the `@closure`.\n           callfunc(@closure ()->println(\"Hello $i\"))\n       end\n   end\nend\n\nThere's nothing nice about this - it's a heuristic workaround for some inefficiencies in the type information inferred by the julia 0.6 compiler. However, it can result in large speedups in many cases, without the need to restructure the code to avoid the closure.\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@einsum-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.@einsum","text":"@einsum tns[idxs...] <<op>>= ex...\n\nConstruct an einsum expression that computes the result of applying op to the tensor tns with the indices idxs and the tensors in the expression ex. The result is stored in the variable tns.\n\nex may be any pointwise expression consisting of function calls and tensor references of the form tns[idxs...], where tns and idxs are symbols.\n\nThe <<op>> operator can be any binary operator that is defined on the element type of the expression ex.\n\nThe einsum will evaluate the pointwise expression tns[idxs...] <<op>>= ex... over all combinations of index values in tns and the tensors in ex.\n\nHere are a few examples:\n\n@einsum C[i, j] += A[i, k] * B[k, j]\n@einsum C[i, j, k] += A[i, j] * B[j, k]\n@einsum D[i, k] += X[i, j] * Y[j, k]\n@einsum J[i, j] = H[i, j] * I[i, j]\n@einsum N[i, j] = K[i, k] * L[k, j] - M[i, j]\n@einsum R[i, j] <<max>>= P[i, k] + Q[k, j]\n@einsum x[i] = A[i, j] * x[j]\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@finch-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.@finch","text":"@finch [options...] prgm\n\nRun a finch program prgm. The syntax for a finch program is a set of nested loops, statements, and branches over pointwise array assignments. For example, the following program computes the sum of two arrays A = B + C:\n\n@finch begin\n    A .= 0\n    for i = _\n        A[i] = B[i] + C[i]\n    end\n    return A\nend\n\nFinch programs are composed using the following syntax:\n\narr .= 0: an array declaration initializing arr to zero.\narr[inds...]: an array access, the array must be a variable and each index may be another finch expression.\nx + y, f(x, y): function calls, where x and y are finch expressions.\narr[inds...] = ex: an array assignment expression, setting arr[inds] to the value of ex.\narr[inds...] += ex: an incrementing array expression, adding ex to arr[inds]. *, &, |, are supported.\narr[inds...] <<min>>= ex: a incrementing array expression with a custom operator, e.g. <<min>> is the minimum operator.\nfor i = _ body end: a loop over the index i, where _ is computed from array access with i in body.\nif cond body end: a conditional branch that executes only iterations where cond is true.\nreturn (tnss...,): at global scope, exit the program and return the tensors tnss with their new dimensions. By default, any tensor declared in global scope is returned.\n\nSymbols are used to represent variables, and their values are taken from the environment. Loops introduce index variables into the scope of their bodies.\n\nFinch uses the types of the arrays and symbolic analysis to discover program optimizations. If B and C are sparse array types, the program will only run over the nonzeros of either.\n\nSemantically, Finch programs execute every iteration. However, Finch can use sparsity information to reliably skip iterations when possible.\n\noptions are optional keyword arguments:\n\nalgebra: the algebra to use for the program. The default is DefaultAlgebra().\nmode: the optimization mode to use for the program. Possible modes are:\n:debug: run the program in debug mode, with bounds checking and better error handling.\n:safe: run the program in safe mode, with modest checks for performance and correctness.\n:fast: run the program in fast mode, with no checks or warnings, this mode is for power users.\nThe default is :safe.\n\nSee also: @finch_code\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@finch_code-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.@finch_code","text":"@finch_code [options...] prgm\n\nReturn the code that would be executed in order to run a finch program prgm.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@finch_kernel-Tuple-appendices-listing","page":"Documentation Listing","title":"Finch.@finch_kernel","text":"@finch_kernel [options...] fname(args...) = prgm\n\nReturn a definition for a function named fname which executes @finch prgm on the arguments args. args should be a list of variables holding representative argument instances.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.@staged-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.@staged","text":"Finch.@staged\n\nThis function is used internally in Finch in lieu of @generated functions. It ensures the first Finch invocation runs in the latest world, and leaves hooks so that subsequent calls to Finch.refresh can update the world and invalidate old versions. If the body contains closures, this macro uses an eval and invokelatest strategy. Otherwise, it uses a generated function. This macro does not support type parameters, varargs, or keyword arguments.\n\n\n\n\n\n","category":"macro"},{"location":"appendices/listing/#Finch.FinchNotation.access-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.assign-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). Overwriting is accomplished with the function overwrite(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.block-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.block","text":"block(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.cached-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.call-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.declare-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.declare","text":"declare(tns, init, op)\n\nFinch AST statement that declares tns with an initial value init reduced with op in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.define-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.define","text":"define(lhs, rhs, body)\n\nFinch AST statement that defines lhs as having the value rhs in body. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.freeze-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.freeze","text":"freeze(tns, op)\n\nFinch AST statement that freezes tns in the current scope after modifications with op, moving the tensor from update-only mode to read-only mode.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.index-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.literal-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.loop-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.reader-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.reader","text":"reader()\n\nFinch AST expression representing a read-only mode for a tensor access. Declare, freeze, and thaw statements can change the mode of a tensor.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.sieve-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.tag-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.tag","text":"tag(var, bind)\n\nFinch AST expression for a global variable var with the value bind. Because the finch compiler cannot pass variable state from the program domain to the type domain directly, the tag type represents a value bind referred to by a variable named bind. All tag in the same program must agree on the value of variables, and only one value will be virtualized.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.thaw-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.thaw","text":"thaw(tns, op)\n\nFinch AST statement that thaws tns in the current scope, moving the tensor from read-only mode to update-only mode with a reduction operator op.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.updater-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.updater","text":"updater(op)\n\nFinch AST expression representing an update-only mode for a tensor access, using the reduction operator op.  Declare, freeze, and thaw statements can change the mode of a tensor.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.value-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.variable-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.virtual-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.virtual","text":"virtual(val)\n\nFinch AST expression for an object val which has special meaning to the compiler. This type is typically used for tensors, as it allows users to specify the tensor's shape and data type.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.yieldbind-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.yieldbind","text":"yieldbind(args...)\n\nFinch AST statement that sets the result of the program to the values of variables args.... Subsequent statements will not affect the result of the program.\n\n\n\n\n\n","category":"constant"},{"location":"appendices/listing/#Finch.FinchNotation.Auto-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.Auto","text":"Auto()\n\nA singleton type representing the lack of a dimension.  This is used in place of a dimension when we want to avoid dimensionality checks. In the @finch macro, you can write Auto() with an underscore as for i = _, allowing finch to pick up the loop bounds from the tensors automatically.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.FinchNotation.FinchNode-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node, used to represent an imperative, physical Finch program.\n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"appendices/listing/#Finch.FinchNotation.extrude-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.extrude","text":"extrude(i)\n\nThe extrude protocol declares that the tensor update happens in order and only once, so that reduction loops occur below the extrude loop. It is not usually necessary to declare an extrude protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.finch_leaf-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.follow-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.follow","text":"follow(i)\n\nThe follow protocol ignores the structure of the tensor. By itself, the follow protocol iterates over each value of the tensor in order, looking it up with random access.  The follow protocol may specialize on e.g. the zero value of the tensor, but does not specialize on the structure of the tensor. This enables efficient random access and avoids large code sizes.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.gallop-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.gallop","text":"gallop(i)\n\nThe gallop protocol iterates over each pattern element of a tensor, leading the iteration and superceding the priority of other tensors. Mutual leading is possible, where we fast-forward to the largest step between either leader.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.initwrite-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.initwrite","text":"initwrite(z)(a, b)\n\ninitwrite(z) is a function which may assert that a isequal to z, and returnsb.  By default,lhs[] = rhsis equivalent tolhs[] <<initwrite(fill_value(lhs))>>= rhs`.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isconstant-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isconstant","text":"isconstant(node)\n\nReturns true if the node can be expected to be constant within the current finch context\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isindex-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isindex","text":"isindex(node)\n\nReturns true if the node is a finch index\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isliteral-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isliteral","text":"isliteral(node)\n\nReturns true if the node is a finch literal\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isstateful-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isstateful","text":"isstateful(node)\n\nReturns true if the node is a finch statement, and false if the node is an index expression. Typically, statements specify control flow and expressions describe values.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isvalue-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvalue","text":"isvalue(node)\n\nReturns true if the node is a finch value\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isvariable-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvariable","text":"isvariable(node)\n\nReturns true if the node is a finch variable\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.isvirtual-Tuple{Finch.FinchNotation.FinchNode}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvirtual","text":"isvirtual(node)\n\nReturns true if the node is a finch virtual\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.laminate-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.laminate","text":"laminate(i)\n\nThe laminate protocol declares that the tensor update may happen out of order and multiple times. It is not usually necessary to declare a laminate protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.overwrite-Tuple{Any, Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.overwrite","text":"overwrite(z)(a, b)\n\noverwrite(z) is a function which returns b always. lhs[] := rhs is equivalent to lhs[] <<overwrite>>= rhs.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5 Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:\n 0.0\n 1.1\n 0.0\n 4.4\n 0.0\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<overwrite>>= a[i] end;\n\njulia> x[]\n0.0\n\n\n\n\n\n","category":"method"},{"location":"appendices/listing/#Finch.FinchNotation.walk-Tuple{Any}-appendices-listing","page":"Documentation Listing","title":"Finch.FinchNotation.walk","text":"walk(i)\n\nThe walk protocol usually iterates over each pattern element of a tensor in order. Note that the walk protocol \"imposes\" the structure of its argument on the kernel, so that we specialize the kernel to the structure of the tensor.\n\n\n\n\n\n","category":"method"}]
}
