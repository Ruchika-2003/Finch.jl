var documenterSearchIndex = {"docs":
[{"location":"guides/interoperability/#Using-Finch-with-Other-Languages","page":"Interoperability","title":"Using Finch with Other Languages","text":"","category":"section"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"You can use Finch in other languages through interfaces like julia.h or PyJulia, but sparse arrays require special considerations for converting between 0-indexed and 1-indexed arrays.","category":"page"},{"location":"guides/interoperability/#0-Index-Compatibility","page":"Interoperability","title":"0-Index Compatibility","text":"","category":"section"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"Julia, Matlab, etc. index arrays starting at 1. C, python, etc. index starting at 0. In a dense array, we can simply subtract one from the index, and in fact, this is what Julia will does under the hood when you pass a vector between C to Julia.","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"However, for sparse array formats, it's not just a matter of subtracting one from the index, as the internal lists of indices, positions, etc all start from zero as well. To remedy the situation, Finch leverages PlusOneVector and CIndex.","category":"page"},{"location":"guides/interoperability/#PlusOneVector","page":"Interoperability","title":"PlusOneVector","text":"","category":"section"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"PlusOneVector is a view that adds 1 on access to an underlying 0-Index vector. This allows to use Python/NumPy vector, without copying, as a mutable index array.","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"julia> v = Vector([1, 0, 2, 3])\n4-element Vector{Int64}:\n 1\n 0\n 2\n 3\n\njulia> obov = PlusOneVector(v)\n4-element PlusOneVector{Int64, Vector{Int64}}:\n 2\n 1\n 3\n 4\n\njulia> obov[1] += 8\n10\n\njulia> obov\n4-element PlusOneVector{Int64, Vector{Int64}}:\n 10\n  1\n  3\n  4\n\njulia> obov.data\n4-element Vector{Int64}:\n 9\n 0\n 2\n 3\n","category":"page"},{"location":"guides/interoperability/#CIndex","page":"Interoperability","title":"CIndex","text":"","category":"section"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"warning: Warning\nCIndex is no longer recommended - use PlusOneVector instead.","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"Finch also interoperates with the CIndices package, which exports a type called CIndex. The internal representation of CIndex is one less than the value it represents, and we can use CIndex as the index or position type of a Finch array to represent arrays in other languages.","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"For example, if idx_c, ptr_c, and val_c are the internal arrays of a CSC matrix in a zero-indexed language, we can represent that matrix as a one-indexed Finch array without copying by calling","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"DocTestSetup = quote\n    using Finch\n    using CIndices\nend","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"julia> m = 4; n = 3; ptr_c = [0, 3, 3, 5]; idx_c = [1, 2, 3, 0, 2]; val_c = [1.1, 2.2, 3.3, 4.4, 5.5];\n\njulia> ptr_jl = reinterpret(CIndex{Int}, ptr_c)\n4-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 1\n 4\n 4\n 6\n\njulia> idx_jl = reinterpret(CIndex{Int}, idx_c)\n5-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 2\n 3\n 4\n 1\n 3\n\njulia> A = Tensor(Dense(SparseList{CIndex{Int}}(Element{0.0, Float64, CIndex{Int}}(val_c), m, ptr_jl, idx_jl), n))\nCIndex{Int64}(4)×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:CIndex{Int64}(4)]\n   │  ├─ [CIndex{Int64}(2)]: 1.1\n   │  ├─ [CIndex{Int64}(3)]: 2.2\n   │  └─ [CIndex{Int64}(4)]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:CIndex{Int64}(4)]\n   └─ [:, 3]: SparseList (0.0) [1:CIndex{Int64}(4)]\n      ├─ [CIndex{Int64}(1)]: 4.4\n      └─ [CIndex{Int64}(3)]: 5.5","category":"page"},{"location":"guides/interoperability/","page":"Interoperability","title":"Interoperability","text":"We can also convert between representations by copying to or from CIndex fibers.","category":"page"},{"location":"guides/debugging_tips/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/parallelization/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/index_sugar/#Index-Sugar-and-Tensor-Modifiers","page":"Index Sugar","title":"Index Sugar and Tensor Modifiers","text":"","category":"section"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"In Finch, expressions like x[i + 1] are compiled using tensor modifiers, like offset(x, 1)[i]. The user can construct tensor modifiers directly, e.g. offset(x, 1), or implicitly using the syntax x[i + 1]. Recognizable index expressions are converted to tensor modifiers before dimensionalization, so that the modified tensor will participate in dimensionalization.","category":"page"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"While tensor modifiers may change the behavior of a tensor, they reference their parent tensor as the root tensor. Modified tensors are not understoond as distinct from their roots. For example, all accesses to the root tensor must obey lifecycle and dimensionalization rules. Additionally, root tensors which are themselves modifiers are unwrapped at the beginning of the program, so that modifiers are not obscured and the new root tensor is not a modifier.","category":"page"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"The following table lists the recognized index expressions and their equivalent tensor expressions, where i is an index, a, b are constants, p is an iteration protocol, and x is an expression:","category":"page"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"Original Expression Transformed Expression\nA[i + a] offset(A, 1)[i]\nA[i + x] toeplitz(A, 1)[i, x]\nA[(a:b)(i)] window(A, a:b)[i]\nA[a * i] scale(A, (3,))[i]\nA[i * x] products(A, 1)[i, j]\nA[~i] permissive(A)[i]\nA[p(i)] protocolize(A, p)[i]","category":"page"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"Each of these tensor modifiers is described below:","category":"page"},{"location":"guides/index_sugar/","page":"Index Sugar","title":"Index Sugar","text":"offset\ntoeplitz\nwindow\nscale\nproducts\npermissive\nprotocolize","category":"page"},{"location":"guides/index_sugar/#Finch.offset","page":"Index Sugar","title":"Finch.offset","text":"offset(tns, delta...)\n\nCreate an OffsetArray such that offset(tns, delta...)[i...] == tns[i .+ delta...]. The dimensions declared by an OffsetArray are shifted, so that size(offset(tns, delta...)) == size(tns) .+ delta.\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.toeplitz","page":"Index Sugar","title":"Finch.toeplitz","text":"toeplitz(tns, dim)\n\nCreate a ToeplitzArray such that\n\n    Toeplitz(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] + i[dim + 1], i[dim + 2:end]...]\n\nThe ToplitzArray can be thought of as adding a dimension that shifts another dimension of the original tensor.\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.window","page":"Index Sugar","title":"Finch.window","text":"window(tns, dims)\n\nCreate a WindowedArray which represents a view into another tensor\n\n    window(tns, dims)[i...] == tns[dim[1][i], dim[2][i], ...]\n\nThe windowed array restricts the new dimension to the dimension of valid indices of each dim. The dims may also be nothing to represent a full view of the underlying dimension.\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.scale","page":"Index Sugar","title":"Finch.scale","text":"scale(tns, delta...)\n\nCreate a ScaleArray such that scale(tns, delta...)[i...] == tns[i .* delta...].  The dimensions declared by an OffsetArray are shifted, so that size(scale(tns, delta...)) == size(tns) .* delta.  This is only supported on tensors with real-valued dimensions.\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.products","page":"Index Sugar","title":"Finch.products","text":"products(tns, dim)\n\nCreate a ProductArray such that\n\n    products(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] * i[dim + 1], i[dim + 2:end]...]\n\nThis is like toeplitz but with times instead of plus.\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.permissive","page":"Index Sugar","title":"Finch.permissive","text":"permissive(tns, dims...)\n\nCreate an PermissiveArray where permissive(tns, dims...)[i...] is missing if i[n] is not in the bounds of tns when dims[n] is true.  This wrapper allows all permissive dimensions to be exempt from dimension checks, and is useful when we need to access an array out of bounds, or for padding. More formally,\n\n    permissive(tns, dims...)[i...] =\n        if any(n -> dims[n] && !(i[n] in axes(tns)[n]))\n            missing\n        else\n            tns[i...]\n        end\n\n\n\n\n\n","category":"function"},{"location":"guides/index_sugar/#Finch.protocolize","page":"Index Sugar","title":"Finch.protocolize","text":"protocolize(tns, protos...)\n\nCreate a ProtocolizedArray that accesses dimension n with protocol protos[n], if protos[n] is not nothing. See the documention for Iteration Protocols for more information. For example, to gallop along the inner dimension of a matrix A, we write A[gallop(i), j], which becomes protocolize(A, gallop, nothing)[i, j].\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"CurrentModule = Finch","category":"page"},{"location":"reference/internals/virtualization/#Program-Instances","page":"Virtualization","title":"Program Instances","text":"","category":"section"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Finch relies heavily on Julia's metaprogramming capabilities ( macros and generated functions in particular) to produce code. To review briefly, a macro allows us to inspect the syntax of it's arguments and generate replacement syntax. A generated function allows us to inspect the type of the function arguments and produce code for a function body.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In normal Finch usage, we might call Finch as follows:","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> C = Tensor(SparseList(Element(0)));\n\njulia> A = Tensor(SparseList(Element(0)), [0, 2, 0, 0, 3]);\n\njulia> B = Tensor(Dense(Element(0)), [11, 12, 13, 14, 15]);\n\njulia> @finch (C .= 0; for i=_; C[i] = A[i] * B[i] end);\n\njulia> C\n5-Tensor\n└─ SparseList (0) [1:5]\n   ├─ [2]: 24\n   └─ [5]: 45\n","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"The @macroexpand macro allows us to see the result of applying a macro. Let's examine what happens when we use the @finch macro (we've stripped line numbers from the result to clean it up):","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> (@macroexpand @finch (C .= 0; for i=_; C[i] = A[i] * B[i] end)) |> Finch.striplines |> Finch.regensym\nquote\n    _res_1 = (Finch.execute)((Finch.FinchNotation.block_instance)((Finch.FinchNotation.block_instance)((Finch.FinchNotation.declare_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), literal_instance(0)), begin\n                        let i = index_instance(i)\n                            (Finch.FinchNotation.loop_instance)(i, Finch.FinchNotation.Dimensionless(), (Finch.FinchNotation.assign_instance)((Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), literal_instance(Finch.FinchNotation.Updater()), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.literal_instance)(Finch.FinchNotation.initwrite), (Finch.FinchNotation.call_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:*), (Finch.FinchNotation.finch_leaf_instance)(*)), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:A), (Finch.FinchNotation.finch_leaf_instance)(A)), literal_instance(Finch.FinchNotation.Reader()), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:B), (Finch.FinchNotation.finch_leaf_instance)(B)), literal_instance(Finch.FinchNotation.Reader()), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))))))\n                        end\n                    end), (Finch.FinchNotation.yieldbind_instance)(variable_instance(:C))); )\n    begin\n        C = _res_1[:C]\n    end\n    begin\n        _res_1\n    end\nend\n","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In the above output, @finch creates an AST of program instances, then calls Finch.execute on it. A program instance is a struct that contains the program to be executed along with its arguments. Although we can use the above constructors (e.g. loop_instance) to make our own program instance, it is most convenient to use the unexported macro Finch.finch_program_instance:","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> using Finch: @finch_program_instance\n\njulia> prgm = Finch.@finch_program_instance (C .= 0; for i=_; C[i] = A[i] * B[i] end; return C)\nFinch program instance: begin\n  tag(C, Tensor(SparseList(Element(0)))) .= 0\n  for i = Dimensionless()\n    tag(C, Tensor(SparseList(Element(0))))[tag(i, i)] <<initwrite>>= tag(*, *)(tag(A, Tensor(SparseList(Element(0))))[tag(i, i)], tag(B, Tensor(Dense(Element(0))))[tag(i, i)])\n  end\n  return (tag(C, Tensor(SparseList(Element(0)))))\nend","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"As we can see, our program instance contains not only the AST to be executed, but also the data to execute the program with. The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. We can run our program using Finch.execute, which returns a NamedTuple of outputs.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> typeof(prgm)\nFinch.FinchNotation.BlockInstance{Tuple{Finch.FinchNotation.DeclareInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{0}}, Finch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.Dimensionless, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Updater()}, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.initwrite}, Finch.FinchNotation.CallInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:*}, Finch.FinchNotation.LiteralInstance{*}}, Tuple{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:A}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Reader()}, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:B}, Tensor{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Reader()}, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}}}}}, Finch.FinchNotation.YieldBindInstance{Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}}}}}\n\njulia> C = Finch.execute(prgm).C\n5-Tensor\n└─ SparseList (0) [1:5]\n   ├─ [2]: 24\n   └─ [5]: 45","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"This functionality is sufficient for building finch kernels programatically. For example, if we wish to define a function pointwise_sum() that takes the pointwise sum of a variable number of vector inputs, we might implement it as follows:","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> function pointwise_sum(As...)\n           B = Tensor(Dense(Element(0)))\n           isempty(As) && return B\n           i = Finch.FinchNotation.index_instance(:i)\n           A_vars = [Finch.FinchNotation.tag_instance(Finch.FinchNotation.variable_instance(Symbol(:A, n)), As[n]) for n in 1:length(As)]\n           #create a list of variable instances with different names to hold the input tensors\n           ex = @finch_program_instance 0\n           for A_var in A_vars\n               ex = @finch_program_instance $A_var[i] + $ex\n           end\n           prgm = @finch_program_instance (B .= 0; for i=_; B[i] = $ex end; return B)\n           return Finch.execute(prgm).B\n       end\npointwise_sum (generic function with 1 method)\n\njulia> pointwise_sum([1, 2], [3, 4])\n2-Tensor\n└─ Dense [1:2]\n   ├─ [1]: 4\n   └─ [2]: 6\n","category":"page"},{"location":"reference/internals/virtualization/#Virtualization","page":"Virtualization","title":"Virtualization","text":"","category":"section"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Finch generates different code depending on the types of the arguments to the program. For example, in the following program, Finch generates different code depending on the types of A and B. In order to execute a program, Finch builds a typed AST (Abstract Syntax Tree), then calls Finch.execute on it. The AST object is just an instance of a program to execute, and contains the program to execute along with the data to execute it.  The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. During compilation, Finch uses the type of the program to construct a more ergonomic representation, which is then used to generate code. This process is called \"virtualization\".  All of the Finch AST nodes have both instance and virtual representations. For example, the literal 42 is represented as Finch.FinchNotation.LiteralInstance(42) and then virtualized to literal(42).  The virtualization process is implemented by the virtualize function.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> A = Tensor(SparseList(Element(0)), [0, 2, 0, 0, 3]);\n\njulia> B = Tensor(Dense(Element(0)), [11, 12, 13, 14, 15]);\n\njulia> s = Scalar(0);\n\njulia> typeof(A)\nTensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}\n\njulia> typeof(B)\nTensor{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}\n\njulia> inst = Finch.@finch_program_instance begin\n           for i = _\n               s[] += A[i]\n           end\n       end\nFinch program instance: for i = Dimensionless()\n  tag(s, Scalar{0, Int64})[] <<tag(+, +)>>= tag(A, Tensor(SparseList(Element(0))))[tag(i, i)]\nend\n\njulia> typeof(inst)\nFinch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.Dimensionless, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:s}, Scalar{0, Int64}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Updater()}, Tuple{}}, Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:+}, Finch.FinchNotation.LiteralInstance{+}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:A}, Tensor{SparseListLevel{Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.Reader()}, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}}}\n\njulia> Finch.virtualize(Finch.JuliaContext(), :inst, typeof(inst))\nFinch program: for i = virtual(Finch.FinchNotation.Dimensionless)\n  tag(s, virtual(Finch.VirtualScalar))[] <<tag(+, +)>>= tag(A, virtual(Finch.VirtualFiber{Finch.VirtualSparseListLevel}))[tag(i, i)]\nend\n\njulia> @finch_code begin\n           for i = _\n               s[] += A[i]\n           end\n       end\nquote\n    s = (ex.bodies[1]).body.lhs.tns.bind\n    s_val = s.val\n    A_lvl = (ex.bodies[1]).body.rhs.tns.bind.lvl\n    A_lvl_ptr = A_lvl.ptr\n    A_lvl_idx = A_lvl.idx\n    A_lvl_val = A_lvl.lvl.val\n    A_lvl_q = A_lvl_ptr[1]\n    A_lvl_q_stop = A_lvl_ptr[1 + 1]\n    if A_lvl_q < A_lvl_q_stop\n        A_lvl_i1 = A_lvl_idx[A_lvl_q_stop - 1]\n    else\n        A_lvl_i1 = 0\n    end\n    phase_stop = min(A_lvl_i1, A_lvl.shape)\n    if phase_stop >= 1\n        if A_lvl_idx[A_lvl_q] < 1\n            A_lvl_q = Finch.scansearch(A_lvl_idx, 1, A_lvl_q, A_lvl_q_stop - 1)\n        end\n        while true\n            A_lvl_i = A_lvl_idx[A_lvl_q]\n            if A_lvl_i < phase_stop\n                A_lvl_2_val = A_lvl_val[A_lvl_q]\n                s_val = A_lvl_2_val + s_val\n                A_lvl_q += 1\n            else\n                phase_stop_3 = min(phase_stop, A_lvl_i)\n                if A_lvl_i == phase_stop_3\n                    A_lvl_2_val = A_lvl_val[A_lvl_q]\n                    s_val += A_lvl_2_val\n                    A_lvl_q += 1\n                end\n                break\n            end\n        end\n    end\n    result = ()\n    s.val = s_val\n    result\nend\n\njulia> @finch_code begin\n           for i = _\n               s[] += B[i]\n           end\n       end\nquote\n    s = (ex.bodies[1]).body.lhs.tns.bind\n    s_val = s.val\n    B_lvl = (ex.bodies[1]).body.rhs.tns.bind.lvl\n    B_lvl_val = B_lvl.lvl.val\n    for i_3 = 1:B_lvl.shape\n        B_lvl_q = (1 - 1) * B_lvl.shape + i_3\n        B_lvl_2_val = B_lvl_val[B_lvl_q]\n        s_val = B_lvl_2_val + s_val\n    end\n    result = ()\n    s.val = s_val\n    result\nend\n","category":"page"},{"location":"reference/internals/virtualization/#The-\"virtual\"-IR-Node","page":"Virtualization","title":"The \"virtual\" IR Node","text":"","category":"section"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Users can also create their own virtual nodes to represent their custom types. While most calls to virtualize result in a Finch IR Node, some objects, such as tensors and dimensions, are virtualized to a virtual object, which holds the custom virtual type.  These types may contain constants and other virtuals, as well as reference variables in the scope of the executing context. Any aspect of virtuals visible to Finch should be considered immutable, but virtuals may reference mutable variables in the scope of the executing context.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"virtualize\nFinchNotation.virtual","category":"page"},{"location":"reference/internals/virtualization/#Finch.virtualize","page":"Virtualization","title":"Finch.virtualize","text":"virtualize(ctx, ex, T, [tag])\n\nReturn the virtual program corresponding to the Julia expression ex of type T in the JuliaContext ctx. Implementaters may support the optional tag argument is used to name the resulting virtual variable.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/virtualization/#Finch.FinchNotation.virtual","page":"Virtualization","title":"Finch.FinchNotation.virtual","text":"virtual(val)\n\nFinch AST expression for an object val which has special meaning to the compiler. This type is typically used for tensors, as it allows users to specify the tensor's shape and data type.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/virtualization/#Virtual-Methods","page":"Virtualization","title":"Virtual Methods","text":"","category":"section"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Many methods have analogues we can call on the virtual version of the object. For example, we can call size an an array, and virtual_size on a virtual array. The virtual methods are used to generate code, so if they are pure they may return an expression which computes the results, and if they have side effects they may accept a context argument into which they can emit their side-effecting code.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"In addition to the special compiler methods which are prefixed virtual_, there is also a function virtual_call, which is used to evaluate function calls on Finch IR when it would result in a virtual object. The behavior should mirror the concrete behavior of the corresponding function.","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"virtual_call","category":"page"},{"location":"reference/internals/virtualization/#Finch.virtual_call","page":"Virtualization","title":"Finch.virtual_call","text":"virtual_call(ctx, f, a...)\n\nGiven the virtual arguments a..., and a literal function f, return a virtual object representing the result of the function call. If the function is not foldable, return nothing. This function is used so that we can call e.g. tensor constructors in finch code.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/virtualization/#Working-with-Finch-IR","page":"Virtualization","title":"Working with Finch IR","text":"","category":"section"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Calling print on a finch program or program instance will print the structure of the program as one would call constructors to build it. For example,","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> prgm_inst = Finch.@finch_program_instance for i = _\n            s[] += A[i]\n        end;\n\njulia> println(prgm_inst)\nloop_instance(index_instance(i), Finch.FinchNotation.Dimensionless(), assign_instance(access_instance(tag_instance(variable_instance(:s), Scalar{0, Int64}(0)), literal_instance(Finch.FinchNotation.Updater())), tag_instance(variable_instance(:+), literal_instance(+)), access_instance(tag_instance(variable_instance(:A), Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), literal_instance(Finch.FinchNotation.Reader()), tag_instance(variable_instance(:i), index_instance(i)))))\n\njulia> prgm_inst\nFinch program instance: for i = Dimensionless()\n  tag(s, Scalar{0, Int64})[] <<tag(+, +)>>= tag(A, Tensor(SparseList(Element(0))))[tag(i, i)]\nend\n\njulia> prgm = Finch.@finch_program for i = _\n               s[] += A[i]\n           end;\n\n\njulia> println(prgm)\nloop(index(i), virtual(Finch.FinchNotation.Dimensionless()), assign(access(literal(Scalar{0, Int64}(0)), literal(Finch.FinchNotation.Updater())), literal(+), access(literal(Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), literal(Finch.FinchNotation.Reader()), index(i))))\n\njulia> prgm\nFinch program: for i = virtual(Finch.FinchNotation.Dimensionless)\n  Scalar{0, Int64}(0)[] <<+>>= Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))[i]\nend\n","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"Both the virtual and instance representations of Finch IR define SyntaxInterface.jl and AbstractTrees.jl representations, so you can use the standard operation, arguments, istree, and children functions to inspect the structure of the program, as well as the rewriters defined by RewriteTools.jl","category":"page"},{"location":"reference/internals/virtualization/","page":"Virtualization","title":"Virtualization","text":"julia> using Finch.FinchNotation;\n\n\njulia> PostOrderDFS(prgm)\nPostOrderDFS{FinchNode}(loop(index(i), virtual(Dimensionless()), assign(access(literal(Scalar{0, Int64}(0)), literal(Updater())), literal(+), access(literal(Tensor(SparseList{Int64}(Element{0, Int64, Int64}([2, 3]), 5, [1, 3], [2, 5]))), literal(Reader()), index(i)))))\n\njulia> (@capture prgm loop(~idx, ~ext, ~val))\ntrue\n\njulia> idx\nFinch program: i\n","category":"page"},{"location":"tutorials_use_cases/tutorials_use_cases/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"CurrentModule = Finch","category":"page"},{"location":"guides/optimization_tips/#Optimization-Tips-for-Finch","page":"Optimization Tips","title":"Optimization Tips for Finch","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"It's easy to ask Finch to run the same operation in different ways. However, different approaches have different performance. The right approach really depends on your particular situation. Here's a collection of general approaches that help Finch generate faster code in most cases.","category":"page"},{"location":"guides/optimization_tips/#Concordant-Iteration","page":"Optimization Tips","title":"Concordant Iteration","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"By default, Finch stores arrays in column major order (first index fast). When the storage order of an array in a Finch expression corresponds to the loop order, we call this concordant iteration. For example, the following expression represents a concordant traversal of a sparse matrix, as the outer loops access the higher levels of the tensor tree:","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\ns = Scalar(0.0)\n@finch for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\nNamedTuple()","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"We can investigate the generated code with @finch_code.  This code iterates over only the nonzeros in order. If our matrix is m × n with nnz nonzeros, this takes O(n + nnz) time.","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\nquote\n    s = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s.val\n    A_lvl = (ex.bodies[1]).body.body.rhs.tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_ptr = A_lvl_2.ptr\n    A_lvl_idx = A_lvl_2.idx\n    A_lvl_2_val = A_lvl_2.lvl.val\n    for j_3 = 1:A_lvl.shape\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n        A_lvl_2_q = A_lvl_ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(A_lvl_2_i1, A_lvl_2.shape)\n        if phase_stop >= 1\n            if A_lvl_idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while true\n                A_lvl_2_i = A_lvl_idx[A_lvl_2_q]\n                if A_lvl_2_i < phase_stop\n                    A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                    s_val = A_lvl_3_val + s_val\n                    A_lvl_2_q += 1\n                else\n                    phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_3\n                        A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                        s_val += A_lvl_3_val\n                        A_lvl_2_q += 1\n                    end\n                    break\n                end\n            end\n        end\n    end\n    result = ()\n    s.val = s_val\n    result\nend","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"When the loop order does not correspond to storage order, we call this discordant iteration. For example, if we swap the loop order in the above example, then Finch needs to randomly access each sparse column for each row i. We end up needing to find each (i, j) pair because we don't know whether it will be zero until we search for it. In all, this takes time O(n * m * log(nnz)), much less efficient! We shouldn't randomly access sparse arrays unless we really need to and they support it efficiently!","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Note the double for loop in the following code","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code for i=_, j=_ ; s[] += A[i, j] end # DISCORDANT, DO NOT DO THIS\n\n# output\n\nquote\n    s = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s.val\n    A_lvl = (ex.bodies[1]).body.body.rhs.tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_ptr = A_lvl_2.ptr\n    A_lvl_idx = A_lvl_2.idx\n    A_lvl_2_val = A_lvl_2.lvl.val\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_lvl_2.shape\n        for j_3 = 1:A_lvl.shape\n            A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n            A_lvl_2_q = A_lvl_ptr[A_lvl_q]\n            A_lvl_2_q_stop = A_lvl_ptr[A_lvl_q + 1]\n            if A_lvl_2_q < A_lvl_2_q_stop\n                A_lvl_2_i1 = A_lvl_idx[A_lvl_2_q_stop - 1]\n            else\n                A_lvl_2_i1 = 0\n            end\n            phase_stop = min(i_3, A_lvl_2_i1)\n            if phase_stop >= i_3\n                if A_lvl_idx[A_lvl_2_q] < i_3\n                    A_lvl_2_q = Finch.scansearch(A_lvl_idx, i_3, A_lvl_2_q, A_lvl_2_q_stop - 1)\n                end\n                while true\n                    A_lvl_2_i = A_lvl_idx[A_lvl_2_q]\n                    if A_lvl_2_i < phase_stop\n                        A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                        s_val = A_lvl_3_val + s_val\n                        A_lvl_2_q += 1\n                    else\n                        phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                        if A_lvl_2_i == phase_stop_3\n                            A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                            s_val += A_lvl_3_val\n                            A_lvl_2_q += 1\n                        end\n                        break\n                    end\n                end\n            end\n        end\n    end\n    result = ()\n    s.val = s_val\n    result\nend","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"TL;DR: As a quick heuristic, if your array indices are all in alphabetical order, then the loop indices should be reverse alphabetical.","category":"page"},{"location":"guides/optimization_tips/#Appropriate-Fill-Values","page":"Optimization Tips","title":"Appropriate Fill Values","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"The @finch macro requires the user to specify an output format. This is the most flexibile approach, but can sometimes lead to densification unless the output fill value is appropriate for the computation.","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"For example, if A is m × n with nnz nonzeros, the following Finch kernel will densify B, filling it with m * n stored values:","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Tensor(Dense(SparseList(Element(0.0)))) #DO NOT DO THIS, B has the wrong fill value\n@finch (B .= 0; for j=_, i=_; B[i, j] = A[i, j] + 1 end; return B)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Since A is filled with 0.0, adding 1 to the fill value produces 1.0. However, B can only represent a fill value of 0.0. Instead, we should specify 1.0 for the fill.","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Tensor(Dense(SparseList(Element(1.0))))\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + 1 end; return B)\ncountstored(B)\n\n# output\n\n5","category":"page"},{"location":"guides/optimization_tips/#Static-Versus-Dynamic-Values","page":"Optimization Tips","title":"Static Versus Dynamic Values","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"In order to skip some computations, Finch must be able to determine the value of program variables. Continuing our above example, if we obscure the value of 1 behind a variable x, Finch can only determine that x has type Int, not that it is 1.","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Tensor(Dense(SparseList(Element(1.0))))\nx = 1 #DO NOT DO THIS, Finch cannot see the value of x anymore\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + x end; return B)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"However, there are some situations where you may want a value to be dynamic. For example, consider the function saxpy(x, a, y) = x .* a .+ y. Because we do not know the value of a until we run the function, we should treat it as dynamic, and the following implementation is reasonable:","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"function saxpy(x, a, y)\n    z = Tensor(SparseList(Element(0.0)))\n    @finch (z .= 0; for i=_; z[i] = a * x[i] + y[i] end; return z)\nend","category":"page"},{"location":"guides/optimization_tips/#Use-Known-Functions","page":"Optimization Tips","title":"Use Known Functions","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Unless you declare the properties of your functions using Finch's User-Defined Functions interface, Finch doesn't know how they work. For example, using a lambda obscures the meaning of *.","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), fsparse([2, 3, 4, 1, 3], [1, 1, 1, 3, 3], [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = ones(4, 3)\nC = Scalar(0.0)\nf(x, y) = x * y # DO NOT DO THIS, Obscures *\n@finch (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end; return C)\n\n# output\n\n(C = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Checking the generated code, we see that this code is indeed densifying (notice the for-loop which repeatedly evaluates f(B[i, j], 0.0)).","category":"page"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"@finch_code (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end; return C)\n\n# output\n\nquote\n    C = ((ex.bodies[1]).bodies[1]).tns.bind\n    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.rhs.args[1]).tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_ptr = A_lvl_2.ptr\n    A_lvl_idx = A_lvl_2.idx\n    A_lvl_2_val = A_lvl_2.lvl.val\n    B = (((ex.bodies[1]).bodies[2]).body.body.rhs.args[2]).tns.bind\n    sugar_1 = size((((ex.bodies[1]).bodies[2]).body.body.rhs.args[2]).tns.bind)\n    B_mode1_stop = sugar_1[1]\n    B_mode2_stop = sugar_1[2]\n    B_mode1_stop == A_lvl_2.shape || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode1_stop) != $(A_lvl_2.shape))\"))\n    B_mode2_stop == A_lvl.shape || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode2_stop) != $(A_lvl.shape))\"))\n    C_val = 0\n    for j_4 = 1:B_mode2_stop\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_4\n        A_lvl_2_q = A_lvl_ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(B_mode1_stop, A_lvl_2_i1)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while true\n                A_lvl_2_i = A_lvl_idx[A_lvl_2_q]\n                if A_lvl_2_i < phase_stop\n                    for i_6 = i:-1 + A_lvl_2_i\n                        val = B[i_6, j_4]\n                        C_val = (Main).f(0.0, val) + C_val\n                    end\n                    A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                    val = B[A_lvl_2_i, j_4]\n                    C_val += (Main).f(A_lvl_3_val, val)\n                    A_lvl_2_q += 1\n                    i = A_lvl_2_i + 1\n                else\n                    phase_stop_3 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_3\n                        for i_8 = i:-1 + phase_stop_3\n                            val = B[i_8, j_4]\n                            C_val += (Main).f(0.0, val)\n                        end\n                        A_lvl_3_val = A_lvl_2_val[A_lvl_2_q]\n                        val = B[phase_stop_3, j_4]\n                        C_val += (Main).f(A_lvl_3_val, val)\n                        A_lvl_2_q += 1\n                    else\n                        for i_10 = i:phase_stop_3\n                            val = B[i_10, j_4]\n                            C_val += (Main).f(0.0, val)\n                        end\n                    end\n                    i = phase_stop_3 + 1\n                    break\n                end\n            end\n        end\n        phase_start_3 = max(1, 1 + A_lvl_2_i1)\n        if B_mode1_stop >= phase_start_3\n            for i_12 = phase_start_3:B_mode1_stop\n                val = B[i_12, j_4]\n                C_val += (Main).f(0.0, val)\n            end\n        end\n    end\n    C.val = C_val\n    (C = C,)\nend\n","category":"page"},{"location":"guides/optimization_tips/#Type-Stability","page":"Optimization Tips","title":"Type Stability","text":"","category":"section"},{"location":"guides/optimization_tips/","page":"Optimization Tips","title":"Optimization Tips","text":"Julia code runs fastest when the compiler can infer the types of all intermediate values.  Finch does not check that the generated code is type-stable. In situations where tensors have nonuniform index or element types, or the computation itself might involve multiple types, one should check that the output of @finch_kernel code is type-stable with @code_warntype.","category":"page"},{"location":"reference/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"CurrentModule = Finch","category":"page"},{"location":"reference/internals/finch_notation/#Finch-Notation-Internals","page":"Finch Notation","title":"Finch Notation Internals","text":"","category":"section"},{"location":"reference/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"Finch IR is a tree structure that represents a finch program. Different types of nodes are delineated by a FinchKind enum, for type stability. There are a few useful functions to be aware of:","category":"page"},{"location":"reference/internals/finch_notation/","page":"Finch Notation","title":"Finch Notation","text":"FinchNode\ncached\nfinch_leaf\nFinchNotation.isstateful\nisliteral\nisvalue\nisconstant\nisvirtual\nisvariable\nisindex","category":"page"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.FinchNode","page":"Finch Notation","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node, used to represent an imperative, physical Finch program.\n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.cached","page":"Finch Notation","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.finch_leaf","page":"Finch Notation","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isstateful","page":"Finch Notation","title":"Finch.FinchNotation.isstateful","text":"isstateful(node)\n\nReturns true if the node is a finch statement, and false if the node is an index expression. Typically, statements specify control flow and expressions describe values.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isliteral","page":"Finch Notation","title":"Finch.FinchNotation.isliteral","text":"isliteral(node)\n\nReturns true if the node is a finch literal\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isvalue","page":"Finch Notation","title":"Finch.FinchNotation.isvalue","text":"isvalue(node)\n\nReturns true if the node is a finch value\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isconstant","page":"Finch Notation","title":"Finch.FinchNotation.isconstant","text":"isconstant(node)\n\nReturns true if the node can be expected to be constant within the current finch context\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isvirtual","page":"Finch Notation","title":"Finch.FinchNotation.isvirtual","text":"isvirtual(node)\n\nReturns true if the node is a finch virtual\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isvariable","page":"Finch Notation","title":"Finch.FinchNotation.isvariable","text":"isvariable(node)\n\nReturns true if the node is a finch variable\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_notation/#Finch.FinchNotation.isindex","page":"Finch Notation","title":"Finch.FinchNotation.isindex","text":"isindex(node)\n\nReturns true if the node is a finch index\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"CurrentModule = Finch","category":"page"},{"location":"reference/internals/tensor_interface/#Tensor-Interface","page":"Tensor Interface","title":"Tensor Interface","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The AbstractTensor interface (defined in src/abstract_tensor.jl) is the interface through which Finch understands tensors. It is a high-level interace which allows tensors to interact with the rest of the Finch system. The interface is designed to be extensible, allowing users to define their own tensor types and behaviors. For a minimal example, read the definitions in /ext/SparseArraysExt.jl and in /src/interface/abstractarray.jl. Once these methods are defined that tell Finch how to generate code for an array, the AbstractTensor interface will also use Finch to generate code for several Julia AbstractArray methods, such as getindex, setindex!, map, and reduce. An important note: getindex and setindex! are not a source of truth for Finch tensors. Search the codebase for ::AbstractTensor for a full list of methods that are implemented for AbstractTensor. Note than most AbstractTensor implement labelled_show and labelled_children methods instead of show(::IO, ::MIME\"text/plain\", t::AbstractTensor) for pretty printed display.","category":"page"},{"location":"reference/internals/tensor_interface/#Tensor-Methods","page":"Tensor Interface","title":"Tensor Methods","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"declare!\ninstantiate\nfreeze!\nthaw!\nunfurl\nfill_value\nvirtual_eltype\nvirtual_fill_value\nvirtual_size\nvirtual_resize!\nmoveto\nvirtual_moveto\nlabelled_show\nlabelled_children\nis_injective\nis_atomic\nis_concurrent","category":"page"},{"location":"reference/internals/tensor_interface/#Finch.declare!","page":"Tensor Interface","title":"Finch.declare!","text":"declare!(ctx, tns, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.instantiate","page":"Tensor Interface","title":"Finch.instantiate","text":"instantiate(ctx, tns, mode, protos)\n\nReturn an object (usually a looplet nest) capable of unfurling the virtual tensor tns. Before executing a statement, each subsequent in-scope access will be initialized with a separate call to instantiate. protos is the list of protocols in each case.\n\nThe fallback for instantiate will iteratively move the last element of protos into the arguments of a function. This allows fibers to specialize on the last arguments of protos rather than the first, as Finch is column major.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.freeze!","page":"Tensor Interface","title":"Finch.freeze!","text":"freeze!(ctx, tns)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. This may involve trimming any excess overallocated memory.  Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.thaw!","page":"Tensor Interface","title":"Finch.thaw!","text":"thaw!(ctx, tns)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.unfurl","page":"Tensor Interface","title":"Finch.unfurl","text":"unfurl(ctx, tns, ext, protos...)\n\nReturn an array object (usually a looplet nest) for lowering the virtual tensor tns. ext is the extent of the looplet. protos is the list of protocols that should be used for each index, but one doesn't need to unfurl all the indices at once.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.fill_value","page":"Tensor Interface","title":"Finch.fill_value","text":"fill_value(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the \"fill\" value becomes the \"background\" value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.virtual_eltype","page":"Tensor Interface","title":"Finch.virtual_eltype","text":"virtual_eltype(arr)\n\nReturn the element type of the virtual tensor arr.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.virtual_fill_value","page":"Tensor Interface","title":"Finch.virtual_fill_value","text":"virtual fill_value(arr)\n\nReturn the initializer for virtual array arr.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.virtual_size","page":"Tensor Interface","title":"Finch.virtual_size","text":"virtual_size(ctx, tns)\n\nReturn a tuple of the dimensions of tns in the context ctx. This is a function similar in spirit to Base.axes.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.virtual_resize!","page":"Tensor Interface","title":"Finch.virtual_resize!","text":"virtual_resize!(ctx, tns, dims...)\n\nResize tns in the context ctx. This is a function similar in spirit to Base.resize!.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.moveto","page":"Tensor Interface","title":"Finch.moveto","text":"moveto(arr, device)\n\nIf the array is not on the given device, it creates a new version of this array on that device and copies the data in to it, according to the device trait.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.virtual_moveto","page":"Tensor Interface","title":"Finch.virtual_moveto","text":"virtual_moveto(device, arr)\n\nIf the virtual array is not on the given device, copy the array to that device. This function may modify underlying data arrays, but cannot change the virtual itself. This function is used to move data to the device before a kernel is launched.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.labelled_show","page":"Tensor Interface","title":"Finch.labelled_show","text":"labelled_show(node)\n\nShow the node in a LabelledTree.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.labelled_children","page":"Tensor Interface","title":"Finch.labelled_children","text":"labelled_children(node)\n\nReturn the children of node in a LabelledTree. You may label the children by returning a LabelledTree(key, value), which will be shown as key: value a.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.is_injective","page":"Tensor Interface","title":"Finch.is_injective","text":"is_injective(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating whether the access is injective in that dimension.  A dimension is injective if each index in that dimension maps to a different memory space in the underlying array.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.is_atomic","page":"Tensor Interface","title":"Finch.is_atomic","text":"is_atomic(ctx, tns)\n\nReturns a tuple (atomicities, overall) where atomicities is a vector, indicating which indices have an atomic that guards them,\nand overall is a boolean that indicates is the last level had an atomic guarding it.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.is_concurrent","page":"Tensor Interface","title":"Finch.is_concurrent","text":"is_concurrent(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating\nwhether the index can be written to without any execution state. So if a matrix returns [true, false],\nthen we can write to A[i, j] and A[i_2, j] without any shared execution state between the two, but\nwe can't write to A[i, j] and A[i, j_2] without carrying over execution state.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Level-Interface","page":"Tensor Interface","title":"Level Interface","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> A_fbr = Tensor(Dense(Dense(Element(0.0))), A)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: Dense [1:4]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: Dense [1:4]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 0.0\n   │  ├─ [3]: 0.0\n   │  └─ [4]: 0.0\n   └─ [:, 3]: Dense [1:4]\n      ├─ [1]: 4.4\n      ├─ [2]: 0.0\n      ├─ [3]: 5.5\n      └─ [4]: 0.0\n","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"We refer to a node in the tree as a subfiber. All of the nodes at the same level are stored in the same datastructure, and disambiguated by an integer position.  in the above example, there are three levels: the rootmost level contains only one subfiber, the root. The middle level has 3 subfibers, one for each column. The leafmost level has 12 subfibers, one for each element of the array.  For example, the first level is A_fbr.lvl, and we can represent it's third position as SubFiber(A_fbr.lvl.lvl, 3). The second level is A_fbr.lvl.lvl, and we can access it's 9th position as SubFiber(A_fbr.lvl.lvl.lvl, 9). For instructional purposes, you can use parentheses to call a subfiber on an index to select among children of a subfiber.","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> Finch.SubFiber(A_fbr.lvl.lvl, 3)\nDense [1:4]\n├─ [1]: 4.4\n├─ [2]: 0.0\n├─ [3]: 5.5\n└─ [4]: 0.0\n\njulia> A_fbr[:, 3]\n4-Tensor\n└─ Dense [1:4]\n   ├─ [1]: 4.4\n   ├─ [2]: 0.0\n   ├─ [3]: 5.5\n   └─ [4]: 0.0\n\njulia> A_fbr(3)\nDense [1:4]\n├─ [1]: 4.4\n├─ [2]: 0.0\n├─ [3]: 5.5\n└─ [4]: 0.0\n\njulia> Finch.SubFiber(A_fbr.lvl.lvl.lvl, 9)\n4.4\n\njulia> A_fbr[1, 3]\n4.4\n\njulia> A_fbr(3)(1)\n4.4\n","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"When we print the tree in text, positions are numbered from top to bottom. However, if we visualize our tree with the root at the top, positions range from left to right:","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"(Image: Dense Format Index Tree)","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Because our array is sparse, (mostly zero, or another fill value), it would be more efficient to store only the nonzero values. In Finch, each level is represented with a different format. A sparse level only stores non-fill values. This time, we'll use a tensor constructor with sl (for \"SparseList of nonzeros\") instead of d (for \"Dense\"):","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), A)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"(Image: CSC Format Index Tree)","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Our Dense(SparseList(Element(0.0))) format is also known as \"CSC\" and is equivalent to SparseMatrixCSC. The Tensor function will perform a zero-cost copy between Finch fibers and sparse matrices, when available.  CSC is an excellent general-purpose representation when we expect most of the columns to have a few nonzeros. However, when most of the columns are entirely fill (a situation known as hypersparsity), it is better to compress the root level as well:","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(SparseList(SparseList(Element(0.0))), A)\n4×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"(Image: DCSC Format Index Tree)","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Here we see that the entirely zero column has also been compressed. The SparseList(SparseList(Element(0.0))) format is also known as \"DCSC\".","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The \"COO\" (or \"Coordinate\") format is often used in practice for ease of interchange between libraries. In an N-dimensional array A, COO stores N lists of indices I_1, ..., I_N where A[I_1[p], ..., I_N[p]] is the p^th stored value in column-major numbering. In Finch, COO is represented as a multi-index level, which can handle more than one index at once. We use curly brackets to declare the number of indices handled by the level:","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"julia> A_fbr = Tensor(SparseCOO{2}(Element(0.0)), A)\n4×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [2, 1]: 1.1\n   ├─ [3, 1]: 2.2\n   ├─ ⋮\n   ├─ [1, 3]: 4.4\n   └─ [3, 3]: 5.5","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"(Image: COO Format Index Tree)","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"The COO format is compact and straightforward, but doesn't support random access. For random access, one should use the SparseDict or SparseBytemap format. A full listing of supported formats is described after a rough description of shared common internals of level, relating to types and storage.","category":"page"},{"location":"reference/internals/tensor_interface/#Types-and-Storage-of-Level","page":"Tensor Interface","title":"Types and Storage of Level","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"All levels have a postype, typically denoted as Tp in the constructors, used for internal pointer types but accessible by the function:","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"postype","category":"page"},{"location":"reference/internals/tensor_interface/#Finch.postype","page":"Tensor Interface","title":"Finch.postype","text":"postype(lvl)\n\nReturn a position type with the same flavor as those used to store the positions of the fibers contained in lvl. The name position descends from the pos or position or pointer arrays found in many definitions of CSR or CSC. In Finch, positions should be data used to access either a subfiber or some other similar auxiliary data. Thus, we often end up iterating over positions.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Additionally, many levels have a Vp or Vi in their constructors; these stand for vector of element type Tp or Ti. More generally, levels are paramterized by the types that they use for storage. By default, all levels use Vector, but a user could could change any or all of the storage types of a tensor so that the tensor would be stored on a GPU or CPU or some combination thereof, or even just via a vector with a different allocation mechanism.  The storage type should behave like AbstractArray and needs to implement the usual abstract array functions and Base.resize!. See the tests for an example.","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"When levels are constructed in short form as in the examples above, the index, position, and storage types are inferred from the level below. All the levels at the bottom of a Tensor (Element, Pattern, Repeater) specify an index type, position type, and storage type even if they don't need them. These are used by levels that take these as parameters.","category":"page"},{"location":"reference/internals/tensor_interface/#Level-Methods","page":"Tensor Interface","title":"Level Methods","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Tensor levels are implemented using the following methods:","category":"page"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"declare_level!\nassemble_level!\nreassemble_level!\nfreeze_level!\nlevel_ndims\nlevel_size\nlevel_axes\nlevel_eltype\nlevel_fill_value","category":"page"},{"location":"reference/internals/tensor_interface/#Finch.declare_level!","page":"Tensor Interface","title":"Finch.declare_level!","text":"declare_level!(ctx, lvl, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.assemble_level!","page":"Tensor Interface","title":"Finch.assemble_level!","text":"assemble_level!(ctx, lvl, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.reassemble_level!","page":"Tensor Interface","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_fill_value(lvl).  Not avaliable on all level types as this presumes updating.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.freeze_level!","page":"Tensor Interface","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, freeze all fibers within lvl assuming that we have potentially updated 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.level_ndims","page":"Tensor Interface","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.level_size","page":"Tensor Interface","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.level_axes","page":"Tensor Interface","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.level_eltype","page":"Tensor Interface","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Finch.level_fill_value","page":"Tensor Interface","title":"Finch.level_fill_value","text":"level_fill_value(::Type{Lvl})\n\nThe result of level_fill_value(Lvl) defines fill_value for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/tensor_interface/#Combinator-Interface","page":"Tensor Interface","title":"Combinator Interface","text":"","category":"section"},{"location":"reference/internals/tensor_interface/","page":"Tensor Interface","title":"Tensor Interface","text":"Tensor Combinators allow us to modify the behavior of tensors. The AbstractCombinator interface (defined in src/tensors/abstract_combinator.jl) is the interface through which Finch understands tensor combinators. The interface requires the combinator to overload all of the tensor methods, as well as the methods used by Looplets when lowering ranges, etc. For a minimal example, read the definitions in /src/tensors/combinators/offset.jl.","category":"page"},{"location":"appendices/faqs/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"CurrentModule = Finch","category":"page"},{"location":"guides/benchmarking_tips/#Benchmarking-Tips","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"","category":"section"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Julia code is nototoriously fussy to benchmark. We'll use BenchmarkTools.jl to automatically follow best practices for getting reliable julia benchmarks. We'll also follow the Julia Performance Tips.","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Finch is even trickier to benchmark, for a few reasons:","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"The first time an @finch function is called, it is compiled, which takes an extra long time. @finch can also incur dynamic dispatch costs if the array types are not type stable. We can remedy this by using @finch_kernel, which simplifies benchmarking by compiling the function ahead of time, so it behaves like a normal Julia function. If you must use @finch, try to ensure that the code is type-stable.\nFinch tensors reuse memory from previous calls, so the first time a tensor is used in a Finch function, it will allocate memory, but subsequent times not so much. If we want to benchmark the memory allocation, we can reconstruct the tensor each time. Otherwise, we can let the repeated executions of the kernel measure the non-allocating runtime.\nRuntime for sparse code often depends on the sparsity pattern, so it's important to benchmark with representative data. Using standard matrices or tensors from MatrixDepot.jl or TensorDepot.jl is a good way to do this.","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"using Finch\nusing BenchmarkTools\nusing SparseArrays\nusing MatrixDepot","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Load a sparse matrix from MatrixDepot.jl and convert it to a Finch tensor","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"A = Tensor(Dense(SparseList(Element(0.0))), matrixdepot(\"HB/west0067\"))\n(m, n) = size(A)\n\nx = Tensor(Dense(Element(0.0)), rand(n))\ny = Tensor(Dense(Element(0.0)))","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Dense [1:0]","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Construct a Finch kernel for sparse matrix-vector multiply","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"eval(@finch_kernel function spmv(y, A, x)\n    y .= 0\n    for j = _, i = _\n        y[i] += A[i, j] * x[j]\n    end\nend)","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"spmv (generic function with 1 method)","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"Benchmark the kernel, ignoring allocation costs for y","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"@benchmark spmv($y, $A, $x)","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"BenchmarkTools.Trial: 10000 samples with 211 evaluations.\n Range (min … max):  355.450 ns … 517.379 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     358.014 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   359.839 ns ±   9.413 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▇▃▇█▃               ▁▂▃                                       ▂\n  █████▁▁▁▁▁▁▁▃▃▁▁▄▃▃████▄▆▄▅▆▄▅▅▄▅▆▅▆▆▇▇▇▆▆▅▆▆▄▆▅▆▅▅▄▆▅▅▆▃▆▄▃▆ █\n  355 ns        Histogram: log(frequency) by time        407 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"The @benchmark macro will benchmark a function in local scope, and it will run the function a few times to estimate the runtime. It will also try to avoid first-time compilation costs by running the function once before benchmarking it. Note the use of $ to interpolate the arrays into the benchmark, bringing them into the local scope.","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"We can also benchmark the memory allocation of the kernel by constructing y in the benchmark kernel","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"@benchmark begin\n    y = Tensor(Dense(Element(0.0)))\n    y = spmv(y, $A, $x).y\nend","category":"page"},{"location":"guides/benchmarking_tips/","page":"Benchmarking Tips","title":"Benchmarking Tips","text":"BenchmarkTools.Trial: 10000 samples with 202 evaluations.\n Range (min … max):  387.168 ns …   3.504 μs  ┊ GC (min … max): 0.00% … 86.82%\n Time  (median):     391.297 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   403.645 ns ± 136.241 ns  ┊ GC (mean ± σ):  1.68% ±  4.34%\n\n   ▂▇█▇▅▂           ▂▃▂▂▂▂▁▁▁                                   ▂\n  ▆███████▅▄▄▃▅▄▄▃▄▇████████████▇▆▆▆▇█▇███▇▇▇▆▇█▇█▇▇▆▅▅▄▄▆▅▅▅▄▅ █\n  387 ns        Histogram: log(frequency) by time        452 ns <\n\n Memory estimate: 608 bytes, allocs estimate: 2.","category":"page"},{"location":"appendices/publications_articles/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/concordization/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/iteration_protocols/#Iteration-Protocols","page":"Iteration Protocols","title":"Iteration Protocols","text":"","category":"section"},{"location":"guides/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"Finch is a flexible tensor compiler with many ways to iterate over the same data. For example, consider the case where we are intersecting two sparse vectors x[i] and y[i]. By default, we would iterate over all of the nonzeros of each vector. However, if we want to skip over the nonzeros in y based on the nonzeros in x, we could declare the tensor x as the leader tensor with an x[gallop(i)] protocol. When x leads the iteration, the generated code uses the nonzeros of x as an outer loop and the nonzeros of y as an inner loop. If we know that the nonzero datastructure of y supports efficient random access, we might ask to iterate over y with a y[follow(i)] protocol, where we look up each value of y[i] only when x[i] is nonzero.","category":"page"},{"location":"guides/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"Finch supports several iteration protocols, documented below. Note that not all formats support all protocols, consult the documentation for each format to figure out which protocols are supported.","category":"page"},{"location":"guides/iteration_protocols/","page":"Iteration Protocols","title":"Iteration Protocols","text":"follow\nwalk\ngallop\nextrude\nlaminate","category":"page"},{"location":"guides/iteration_protocols/#Finch.FinchNotation.follow","page":"Iteration Protocols","title":"Finch.FinchNotation.follow","text":"follow(i)\n\nThe follow protocol ignores the structure of the tensor. By itself, the follow protocol iterates over each value of the tensor in order, looking it up with random access.  The follow protocol may specialize on e.g. the zero value of the tensor, but does not specialize on the structure of the tensor. This enables efficient random access and avoids large code sizes.\n\n\n\n\n\n","category":"function"},{"location":"guides/iteration_protocols/#Finch.FinchNotation.walk","page":"Iteration Protocols","title":"Finch.FinchNotation.walk","text":"walk(i)\n\nThe walk protocol usually iterates over each pattern element of a tensor in order. Note that the walk protocol \"imposes\" the structure of its argument on the kernel, so that we specialize the kernel to the structure of the tensor.\n\n\n\n\n\n","category":"function"},{"location":"guides/iteration_protocols/#Finch.FinchNotation.gallop","page":"Iteration Protocols","title":"Finch.FinchNotation.gallop","text":"gallop(i)\n\nThe gallop protocol iterates over each pattern element of a tensor, leading the iteration and superceding the priority of other tensors. Mutual leading is possible, where we fast-forward to the largest step between either leader.\n\n\n\n\n\n","category":"function"},{"location":"guides/iteration_protocols/#Finch.FinchNotation.extrude","page":"Iteration Protocols","title":"Finch.FinchNotation.extrude","text":"extrude(i)\n\nThe extrude protocol declares that the tensor update happens in order and only once, so that reduction loops occur below the extrude loop. It is not usually necessary to declare an extrude protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"function"},{"location":"guides/iteration_protocols/#Finch.FinchNotation.laminate","page":"Iteration Protocols","title":"Finch.FinchNotation.laminate","text":"laminate(i)\n\nThe laminate protocol declares that the tensor update may happen out of order and multiple times. It is not usually necessary to declare a laminate protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"function"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"CurrentModule = Finch","category":"page"},{"location":"guides/array_api/#High-Level-Array-API","page":"High-Level Array API","title":"High-Level Array API","text":"","category":"section"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch tensors also support many of the basic array operations one might expect, including indexing, slicing, and elementwise maps, broadcast, and reduce. For example:","category":"page"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])\n3×6-Tensor\n└─ SparseCOO{2} (0.0) [:,1:6]\n   ├─ [1, 2]: 1.0\n   ├─ [1, 4]: 2.0\n   └─ [2, 5]: 3.0\n\njulia> A + 0\n3×6-Tensor\n└─ Dense [:,1:6]\n   ├─ [:, 1]: Dense [1:3]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 0.0\n   │  └─ [3]: 0.0\n   ├─ [:, 2]: Dense [1:3]\n   │  ├─ [1]: 1.0\n   │  ├─ [2]: 0.0\n   │  └─ [3]: 0.0\n   ├─ ⋮\n   ├─ [:, 5]: Dense [1:3]\n   │  ├─ [1]: 0.0\n   │  ├─ [2]: 3.0\n   │  └─ [3]: 0.0\n   └─ [:, 6]: Dense [1:3]\n      ├─ [1]: 0.0\n      ├─ [2]: 0.0\n      └─ [3]: 0.0\n\njulia> A + 1\n3×6-Tensor\n└─ Dense [:,1:6]\n   ├─ [:, 1]: Dense [1:3]\n   │  ├─ [1]: 1.0\n   │  ├─ [2]: 1.0\n   │  └─ [3]: 1.0\n   ├─ [:, 2]: Dense [1:3]\n   │  ├─ [1]: 2.0\n   │  ├─ [2]: 1.0\n   │  └─ [3]: 1.0\n   ├─ ⋮\n   ├─ [:, 5]: Dense [1:3]\n   │  ├─ [1]: 1.0\n   │  ├─ [2]: 4.0\n   │  └─ [3]: 1.0\n   └─ [:, 6]: Dense [1:3]\n      ├─ [1]: 1.0\n      ├─ [2]: 1.0\n      └─ [3]: 1.0\n\njulia> B = A .* 2\n3×6-Tensor\n└─ SparseDict (0.0) [:,1:6]\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 2.0\n   ├─ [:, 4]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 4.0\n   └─ [:, 5]: SparseDict (0.0) [1:3]\n      └─ [2]: 6.0\n\njulia> B[1:2, 1:2]\n2×2-Tensor\n└─ SparseDict (0.0) [:,1:2]\n   └─ [:, 2]: SparseDict (0.0) [1:2]\n      └─ [1]: 2.0\n\njulia> map(x -> x^2, B)\n3×6-Tensor\n└─ SparseDict (0.0) [:,1:6]\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 4.0\n   ├─ [:, 4]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 16.0\n   └─ [:, 5]: SparseDict (0.0) [1:3]\n      └─ [2]: 36.0","category":"page"},{"location":"guides/array_api/#Array-Fusion","page":"High-Level Array API","title":"Array Fusion","text":"","category":"section"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch supports array fusion, which allows you to compose multiple array operations into a single kernel. This can be a significant performance optimization, as it allows the compiler to optimize the entire operation at once. The two functions the user needs to know about are lazy and compute. You can use lazy to mark an array as an input to a fused operation, and call compute to execute the entire operation at once. For example:","category":"page"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"julia> C = lazy(A);\n\njulia> D = lazy(B);\n\njulia> E = (C .+ D)/2;\n\njulia> compute(E)\n3×6-Tensor\n└─ SparseDict (0.0) [:,1:6]\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 1.5\n   ├─ [:, 4]: SparseDict (0.0) [1:3]\n   │  └─ [1]: 3.0\n   └─ [:, 5]: SparseDict (0.0) [1:3]\n      └─ [2]: 4.5\n","category":"page"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"In the above example, E is a fused operation that adds C and D together and then divides the result by 2. The compute function examines the entire operation and decides how to execute it in the most efficient way possible. In this case, it would likely generate a single kernel that adds the elements of A and B together and divides each result by 2, without materializing an intermediate.","category":"page"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"lazy\ncompute","category":"page"},{"location":"guides/array_api/#Finch.lazy","page":"High-Level Array API","title":"Finch.lazy","text":"lazy(arg)\n\nCreate a lazy tensor from an argument. All operations on lazy tensors are lazy, and will not be executed until compute is called on their result.\n\nfor example,\n\nx = lazy(rand(10))\ny = lazy(rand(10))\nz = x + y\nz = z + 1\nz = compute(z)\n\nwill not actually compute z until compute(z) is called, so the execution of x + y is fused with the execution of z + 1.\n\n\n\n\n\n","category":"function"},{"location":"guides/array_api/#Finch.compute","page":"High-Level Array API","title":"Finch.compute","text":"compute(args..., ctx=default_scheduler()) -> Any\n\nCompute the value of a lazy tensor. The result is the argument itself, or a tuple of arguments if multiple arguments are passed.\n\n\n\n\n\n","category":"function"},{"location":"guides/array_api/#Einsum","page":"High-Level Array API","title":"Einsum","text":"","category":"section"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"Finch also supports a highly general @einsum macro which supports any reduction over any simple pointwise array expression.","category":"page"},{"location":"guides/array_api/","page":"High-Level Array API","title":"High-Level Array API","text":"@einsum","category":"page"},{"location":"guides/array_api/#Finch.@einsum","page":"High-Level Array API","title":"Finch.@einsum","text":"@einsum tns[idxs...] <<op>>= ex...\n\nConstruct an einsum expression that computes the result of applying op to the tensor tns with the indices idxs and the tensors in the expression ex. The result is stored in the variable tns.\n\nex may be any pointwise expression consisting of function calls and tensor references of the form tns[idxs...], where tns and idxs are symbols.\n\nThe <<op>> operator can be any binary operator that is defined on the element type of the expression ex.\n\nThe einsum will evaluate the pointwise expression tns[idxs...] <<op>>= ex... over all combinations of index values in tns and the tensors in ex.\n\nHere are a few examples:\n\n@einsum C[i, j] += A[i, k] * B[k, j]\n@einsum C[i, j, k] += A[i, j] * B[j, k]\n@einsum D[i, k] += X[i, j] * Y[j, k]\n@einsum J[i, j] = H[i, j] * I[i, j]\n@einsum N[i, j] = K[i, k] * L[k, j] - M[i, j]\n@einsum R[i, j] <<max>>= P[i, k] + Q[k, j]\n@einsum x[i] = A[i, j] * x[j]\n\n\n\n\n\n","category":"macro"},{"location":"guides/mask_sugar/#Mask-Sugar","page":"Mask Sugar","title":"Mask Sugar","text":"","category":"section"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"In Finch, expressions like i == j are treated as a sugar for mask tensors, which can be used to encode fancy iteration patterns. For example, the expression i == j is converted to a diagonal boolean mask tensor DiagMask()[i, j], which allows an expression like","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"    @finch begin\n        for i=_, j=_\n            if i == j\n                s[] += A[i, j]\n            end\n        end\n    end\nend","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"to compile to something like","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"    for i = 1:n\n        s[] += A[i, i]\n    end","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"There are several mask tensors and syntaxes available, summarized in the following table where i, j are indices:","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"Expression Transformed Expression\ni < j UpTriMask()[i, j - 1]\ni <= j UpTriMask()[i, j]\ni > j LoTriMask()[i, j + 1]\ni >= j LoTriMask()[i, j]\ni == j DiagMask()[i, j]\ni != j !(DiagMask()[i, j])","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"Note that either i or j may be expressions, so long as the expression is constant with respect to the loop over the index.","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"The mask tensors are described below:","category":"page"},{"location":"guides/mask_sugar/","page":"Mask Sugar","title":"Mask Sugar","text":"uptrimask\nlotrimask\ndiagmask\nbandmask\nchunkmask","category":"page"},{"location":"guides/mask_sugar/#Finch.uptrimask","page":"Mask Sugar","title":"Finch.uptrimask","text":"uptrimask\n\nA mask for an upper triangular tensor, uptrimask[i, j] = i <= j. Note that this specializes each column for the cases where i <= j and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"guides/mask_sugar/#Finch.lotrimask","page":"Mask Sugar","title":"Finch.lotrimask","text":"lotrimask\n\nA mask for an upper triangular tensor, lotrimask[i, j] = i >= j. Note that this specializes each column for the cases where i < j and i >= j.\n\n\n\n\n\n","category":"constant"},{"location":"guides/mask_sugar/#Finch.diagmask","page":"Mask Sugar","title":"Finch.diagmask","text":"diagmask\n\nA mask for a diagonal tensor, diagmask[i, j] = i == j. Note that this specializes each column for the cases where i < j, i == j, and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"guides/mask_sugar/#Finch.bandmask","page":"Mask Sugar","title":"Finch.bandmask","text":"bandmask\n\nA mask for a banded tensor, bandmask[i, j, k] = j <= i <= k. Note that this specializes each column for the cases where i < j, j <= i <= k, and k < i.\n\n\n\n\n\n","category":"constant"},{"location":"guides/mask_sugar/#Finch.chunkmask","page":"Mask Sugar","title":"Finch.chunkmask","text":"chunkmask(b)\n\nA mask for a chunked tensor, chunkmask[i, j] = b * (j - 1) < i <= b * j. Note that this specializes each column for the cases where i < b * (j - 1), `b * (j\n\n< i <= b * j, andb * j < i`.\n\n\n\n\n\n","category":"function"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"We welcome contributions to Finch, and follow the Julia contributing guidelines.  If you use or want to use Finch and have a question or bug, please do file a Github issue!  If you want to contribute to Finch, please first file an issue to double check that there is interest from a contributor in the feature.","category":"page"},{"location":"CONTRIBUTING/#Versions","page":"Community and Contributions","title":"Versions","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Finch is currently in a pre-release state. The API is not yet stable, and breaking changes may occur between minor versions. We follow semantic versioning and will release 1.0 when the API is stable. The main branch of the Finch repo is the most up-to-date development branch. While it is not stable, it should always pass tests.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Contributors will develop and test Finch from a local directory. Please see the Package documentation for more info, particularly the section on developing.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"To determine which version of Finch you have, run Pkg.status(\"Finch\") in the Julia REPL. If the installed version of Finch tracks a local path, the output will include the path like so:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Status `~/.julia/environments/v1.9/Project.toml`\n  [9177782c] Finch v0.5.4 `~/Projects/Finch.jl`","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If the installed version of Finch tracks a particular version (probably not what you want since it will not reflect local changes), the output will look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Status `~/.julia/environments/v1.8/Project.toml`\n  [9177782c] Finch v0.5.4","category":"page"},{"location":"CONTRIBUTING/#Utilities","page":"Community and Contributions","title":"Utilities","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Finch include several scripts that can be executed directly, e.g. runtests.jl. These scripts are all have local Pkg environments. The scripts include convenience headers to automatically use their respective environments, so you won't need to worry about --project=. flags, etc.","category":"page"},{"location":"CONTRIBUTING/#Testing","page":"Community and Contributions","title":"Testing","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"All pull requests should pass continuous integration testing before merging. The test suite has a few options, which are accessible through running the test suite directly as ./test/runtests.jl.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"Finch compares compiler output against reference versions.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If you have the appropriate permissions, you can run the FixBot github action on your PR branch to automatically generate output for both 32-bit and 64-bit builds.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"If you run the test suite directly you can pass the --overwrite flag to tell the test suite to overwrite the reference.  Because the reference output depends on the system word size, you'll need to generate reference output for 32-bit and 64-bit builds of Julia to get Finch to pass tests. The easiest way to do this is to run each 32-bit or 64-bit build of Julia on a system that supports it. You can Download multiple builds yourself or use juliaup to manage multiple versions. Using juliaup, it might look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"julia +release~x86 test/runtests.jl --overwrite\njulia +release~x64 test/runtests.jl --overwrite","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The test suite takes a while to run. You can filter to only run a selection of test suites by specifying them as positional arguments, e.g.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"./test/runtests.jl constructors conversions representation","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"This information is summarized with ./test/runtests.jl --help","category":"page"},{"location":"CONTRIBUTING/#Python-test-suite","page":"Community and Contributions","title":"Python test suite","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"finch-tensor-python contains a separate Array API compatible test suite written in Python. It requires Python 3.10 or later and Poetry installed.","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"It can be run with:","category":"page"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"git clone https://github.com/finch-tensor/finch-tensor-python.git\ncd finch-tensor-python\npoetry install --with test\nFINCH_REPO_PATH=<PATH_TO_FINCH_REPO> poetry run pytest tests/","category":"page"},{"location":"CONTRIBUTING/#Benchmarking","page":"Community and Contributions","title":"Benchmarking","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The Finch test suite includes a benchmarking script that measures Finch performance on a variety of kernels. It also includes some scripts to help compare Finch performance on the feature branch to the main branch. To run the benchmarking script, run ./benchmarks/runbenchmarks.jl. To run the comparison script, run ./benchmarks/runjudge.jl. Both scripts take a while to run and generate a report at the end.","category":"page"},{"location":"CONTRIBUTING/#Documentation","page":"Community and Contributions","title":"Documentation","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Community and Contributions","title":"Community and Contributions","text":"The /docs directory includes Finch documentation in /src, and a built website in /build. You can build the website with ./docs/make.jl. You can run doctests with ./docs/test.jl, and fix doctests with ./docs/fix.jl, though both are included as part of the test suite.","category":"page"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"CurrentModule = Finch","category":"page"},{"location":"guides/fileio/#Finch-Tensor-File-Input/Output","page":"FileIO","title":"Finch Tensor File Input/Output","text":"","category":"section"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"All of the file formats supported by Finch are listed below. Each format has a corresponding read and write function, and can be selected automatically based on the file extension with the following functions:","category":"page"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"fread\nfwrite","category":"page"},{"location":"guides/fileio/#Finch.fread","page":"FileIO","title":"Finch.fread","text":"fread(filename::AbstractString)\n\nRead the Finch tensor from a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#Finch.fwrite","page":"FileIO","title":"Finch.fwrite","text":"fwrite(filename::AbstractString, tns::Finch.Tensor)\n\nWrite the Finch tensor to a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#Binsparse-Format-(.bsp)","page":"FileIO","title":"Binsparse Format (.bsp)","text":"","category":"section"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the most recent revision of the Binsparse binary sparse tensor format, including the v2.0 tensor extension. This is a good option for those who want an efficient way to transfer sparse tensors between supporting libraries and languages. The Binsparse format represents the tensor format as a JSON string in the underlying data container, which can be either HDF5 or a combination of NPY or JSON files.  Binsparse arrays are stored 0-indexed.","category":"page"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"bspwrite\nbspread","category":"page"},{"location":"guides/fileio/#Finch.bspwrite","page":"FileIO","title":"Finch.bspwrite","text":"bspwrite(::AbstractString, tns)\nbspwrite(::HDF5.File, tns)\nbspwrite(::NPYPath, tns)\n\nWrite the Finch tensor to a file using Binsparse file format.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#Finch.bspread","page":"FileIO","title":"Finch.bspread","text":"bspread(::AbstractString) bspread(::HDF5.File) bspread(::NPYPath)\n\nRead the Binsparse file into a Finch tensor.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\n\n\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#TensorMarket-(.mtx,-.ttx)","page":"FileIO","title":"TensorMarket (.mtx, .ttx)","text":"","category":"section"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the MatrixMarket and TensorMarket formats, which prioritize readability and archiveability, storing matrices and tensors in plaintext.","category":"page"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"fttwrite\nfttread","category":"page"},{"location":"guides/fileio/#Finch.fttwrite","page":"FileIO","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch tensor to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#Finch.fttread","page":"FileIO","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch tensor. The tensor will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#FROSTT-(.tns)","page":"FileIO","title":"FROSTT (.tns)","text":"","category":"section"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"Finch supports the FROSTT format for legacy codes that still use it.","category":"page"},{"location":"guides/fileio/","page":"FileIO","title":"FileIO","text":"ftnswrite\nftnsread","category":"page"},{"location":"guides/fileio/#Finch.ftnswrite","page":"FileIO","title":"Finch.ftnswrite","text":"ftnswrite(filename, tns)\n\nWrite a sparse Finch tensor to a FROSTT .tns file.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnswrite\n\n\n\n\n\n","category":"function"},{"location":"guides/fileio/#Finch.ftnsread","page":"FileIO","title":"Finch.ftnsread","text":"ftnsread(filename)\n\nRead the contents of the FROSTT .tns file 'filename' into a Finch COO Tensor.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnsread\n\n\n\n\n\n","category":"function"},{"location":"appendices/glossary/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"appendices/changelog/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In Finch, all tensors accessed by a particular index must have the same dimension along the corresponding mode. Finch determines the dimension of a loop index i from all of the tensors using i in an access, as well as the bounds in the loop itself.","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"For example, consider the following code","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"A = fsprand(3, 4, 0.5)\nB = fsprand(4, 5, 0.5)\nC = Tensor(Dense(SparseList(Element(0.0))))\n@finch begin\nC .= 0\nfor i = 1:3\n    for j = _\n        for k = _\n            C[i, j] += A[i, k] * B[k, j]\n        end\n    end\nend","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In the above code, the second dimension of A must match the first dimension of B.  Also, the first dimension of A must match the i loop dimension, 1:3. Finch will also resize declared tensors to match indices used in writes, so C is resized to (1:3, 1:5). If no dimensions are specified elsewhere, then Finch will use the dimension of the declared tensor.","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Dimensionalization occurs after wrapper arrays are de-sugared. You can therefore exempt a tensor from dimensionalization by wrapping the corresponding index in ~. For example,","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"@finch begin\ny .= 0\nfor i = 1:3\n    y[~i] += x[i]\nend","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"does not set the dimension of y, and y does not participate in dimensionalization.","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"In summary, the rules of index dimensionalization are as follows:","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Indices have dimensions\nUsing an index in an access “hints” that the index should have the corresponding dimension\nLoop dimensions are equal to the “meet” of all hints in the loop body and the loop bounds\nThe meet usually asserts that dimensions match, but may also e.g. propagate info about parallelization","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"The rules of declaration dimensionalization are as follows:","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Declarations have dimensions\nLeft hand side (updating) tensor access “hint” the size of that tensor\nThe dimensions of a declaration are the “meet” of all hints from the declaration to the first read\nThe new dimensions of the declared tensor are used when the tensor is on the right hand side (reading) access.","category":"page"},{"location":"guides/dimensionalization/","page":"Dimensionalization","title":"Dimensionalization","text":"Finch.FinchNotation.Dimensionless","category":"page"},{"location":"guides/dimensionalization/#Finch.FinchNotation.Dimensionless","page":"Dimensionalization","title":"Finch.FinchNotation.Dimensionless","text":"Dimensionless()\n\nA singleton type representing the lack of a dimension.  This is used in place of a dimension when we want to avoid dimensionality checks. In the @finch macro, you can write Dimensionless() with an underscore as for i = _, allowing finch to pick up the loop bounds from the tensors automatically.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"CurrentModule = Finch","category":"page"},{"location":"reference/internals/compiler_interface/#Compiler-Internals","page":"Compiler Interfaces","title":"Compiler Internals","text":"","category":"section"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"Finch has several compiler modules with separate interfaces.","category":"page"},{"location":"reference/internals/compiler_interface/#SymbolicContexts","page":"Compiler Interfaces","title":"SymbolicContexts","text":"","category":"section"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"SymbolicContexts are used to represent the symbolic information of a program. They are used to reason about the bounds of loops and the symbolic information of the program, and are defined on an algebra","category":"page"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"StaticHash\nSymbolicContext\nget_algebra\nget_static_hash\nprove\nsimplify","category":"page"},{"location":"reference/internals/compiler_interface/#Finch.StaticHash","page":"Compiler Interfaces","title":"Finch.StaticHash","text":"StaticHash\n\nA hash function which is static, i.e. the hashes are the same when objects are hashed in the same order. The hash is used to memoize the results of simplification and proof rules.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.SymbolicContext","page":"Compiler Interfaces","title":"Finch.SymbolicContext","text":"SymbolicContext\n\nA compiler context for symbolic computation, defined on an algebra.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.get_algebra","page":"Compiler Interfaces","title":"Finch.get_algebra","text":"get_algebra(ctx)\n\nget the algebra used in the current context\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.get_static_hash","page":"Compiler Interfaces","title":"Finch.get_static_hash","text":"get_static_hash(ctx)\n\nReturn an object which can be called as a hash function. The hashes are the same when objects are hashed in the same order.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.prove","page":"Compiler Interfaces","title":"Finch.prove","text":"prove(ctx, root; verbose = false)\n\nuse the rules in ctx to attempt to prove that the program root is true. Return false if the program cannot be shown to be true.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.simplify","page":"Compiler Interfaces","title":"Finch.simplify","text":"simplify(ctx, node)\n\nsimplify the program node using the rules in ctx\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#ScopeContexts","page":"Compiler Interfaces","title":"ScopeContexts","text":"","category":"section"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"ScopeContexts are used to represent the scope of a program. They are used to reason about values bound to variables and also the modes of tensor variables.","category":"page"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"ScopeContext\nget_binding\nhas_binding\nset_binding!\nset_declared!\nset_frozen!\nset_thawed!\nget_tensor_mode\nopen_scope","category":"page"},{"location":"reference/internals/compiler_interface/#Finch.ScopeContext","page":"Compiler Interfaces","title":"Finch.ScopeContext","text":"ScopeContext\n\nA context for managing variable bindings and tensor modes.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.get_binding","page":"Compiler Interfaces","title":"Finch.get_binding","text":"get_binding(ctx, var)\n\nGet the binding of a variable in the context.\n\n\n\n\n\nget_binding(ctx, var, val)\n\nGet the binding of a variable in the context, or return a default value.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.has_binding","page":"Compiler Interfaces","title":"Finch.has_binding","text":"has_binding(ctx, var)\n\nCheck if a variable is bound in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.set_binding!","page":"Compiler Interfaces","title":"Finch.set_binding!","text":"set_binding!(ctx, var, val)\n\nSet the binding of a variable in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.set_declared!","page":"Compiler Interfaces","title":"Finch.set_declared!","text":"set_declared!(ctx, var, val)\n\nMark a tensor variable as declared in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.set_frozen!","page":"Compiler Interfaces","title":"Finch.set_frozen!","text":"set_frozen!(ctx, var, val)\n\nMark a tensor variable as frozen in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.set_thawed!","page":"Compiler Interfaces","title":"Finch.set_thawed!","text":"set_thawed!(ctx, var, val)\n\nMark a tensor variable as thawed in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.get_tensor_mode","page":"Compiler Interfaces","title":"Finch.get_tensor_mode","text":"get_tensor_mode(ctx, var)\n\nGet the mode of a tensor variable in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.open_scope","page":"Compiler Interfaces","title":"Finch.open_scope","text":"open_scope(f, ctx)\n\nCall the function f(ctx_2) in a new scope ctx_2.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#JuliaContexts","page":"Compiler Interfaces","title":"JuliaContexts","text":"","category":"section"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"JuliaContexts are used to represent the execution environment of a program, including variables and tasks. They are used to generate code.","category":"page"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"Namespace\nJuliaContext\npush_preamble!\npush_epilogue!\nget_task\nfreshen\ncontain","category":"page"},{"location":"reference/internals/compiler_interface/#Finch.Namespace","page":"Compiler Interfaces","title":"Finch.Namespace","text":"Namespace\n\nA namespace for managing variable names and aesthetic fresh variable generation.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.JuliaContext","page":"Compiler Interfaces","title":"Finch.JuliaContext","text":"JuliaContext\n\nA context for compiling Julia code, managing side effects, parallelism, and variable names in the generated code of the executing environment.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.push_preamble!","page":"Compiler Interfaces","title":"Finch.push_preamble!","text":"push_preamble!(ctx, thunk)\n\nPush the thunk onto the preamble in the currently executing context. The preamble will be evaluated before the code returned by the given function in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.push_epilogue!","page":"Compiler Interfaces","title":"Finch.push_epilogue!","text":"push_epilogue!(ctx, thunk)\n\nPush the thunk onto the epilogue in the currently executing context. The epilogue will be evaluated after the code returned by the given function in the context.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.get_task","page":"Compiler Interfaces","title":"Finch.get_task","text":"get_task(ctx)\n\nGet the task which will execute code in this context\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.freshen","page":"Compiler Interfaces","title":"Finch.freshen","text":"freshen(ctx, tags...)\n\nReturn a fresh variable in the current context named after Symbol(tags...)\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.contain","page":"Compiler Interfaces","title":"Finch.contain","text":"contain(f, ctx)\n\nCall f on a subcontext of ctx and return the result. Variable bindings, preambles, and epilogues defined in the subcontext will not escape the call to contain.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#AbstractCompiler","page":"Compiler Interfaces","title":"AbstractCompiler","text":"","category":"section"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"The AbstractCompiler interface requires all of the functionality of the above contexts, as well as the following two methods:","category":"page"},{"location":"reference/internals/compiler_interface/","page":"Compiler Interfaces","title":"Compiler Interfaces","text":"FinchCompiler\nget_result\nget_mode_flag","category":"page"},{"location":"reference/internals/compiler_interface/#Finch.FinchCompiler","page":"Compiler Interfaces","title":"Finch.FinchCompiler","text":"FinchCompiler\n\nThe core compiler for Finch, lowering canonicalized Finch IR to Julia code.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/compiler_interface/#Finch.get_result","page":"Compiler Interfaces","title":"Finch.get_result","text":"get_result(ctx)\n\nReturn a variable which evaluates to the result of the program which should be returned to the user.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/compiler_interface/#Finch.get_mode_flag","page":"Compiler Interfaces","title":"Finch.get_mode_flag","text":"get_mode_flag(ctx)\n\nReturn the mode flag given in @finch mode = ?.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"CurrentModule = Finch","category":"page"},{"location":"reference/internals/finch_logic/#Finch-Logic-(High-Level-IR)","page":"Finch Logic","title":"Finch Logic (High-Level IR)","text":"","category":"section"},{"location":"reference/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"Finch Logic is an internal high-level intermediate representation (IR) that allows us to fuse and optimize successive calls to array operations such as map, reduce, and broadcast. It is reminiscent to database query notation, representing the a sequence of tensor expressions bound to variables. Values in the program are tensors, with named indices. The order of indices is semantically meaningful.","category":"page"},{"location":"reference/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"The nodes are as follows:","category":"page"},{"location":"reference/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"immediate\ndeferred\nfield\nalias\ntable\nmapjoin\naggregate\nreorder\nrelabel\nreformat\nsubquery\nquery\nproduces\nplan","category":"page"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.immediate","page":"Finch Logic","title":"Finch.FinchLogic.immediate","text":"immediate(val)\n\nLogical AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.deferred","page":"Finch Logic","title":"Finch.FinchLogic.deferred","text":"deferred(ex, [type])\n\nLogical AST expression for an expression ex of type type, yet to be evaluated.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.field","page":"Finch Logic","title":"Finch.FinchLogic.field","text":"field(name)\n\nLogical AST expression for an field named name.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.alias","page":"Finch Logic","title":"Finch.FinchLogic.alias","text":"alias(name)\n\nLogical AST expression for an alias named name.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.table","page":"Finch Logic","title":"Finch.FinchLogic.table","text":"table(tns, idxs...)\n\nLogical AST expression for a tensor object val, indexed by fields idxs....\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.mapjoin","page":"Finch Logic","title":"Finch.FinchLogic.mapjoin","text":"mapjoin(op, args...)\n\nLogical AST expression for mapping the function op across args.... The order of fields in the mapjoin is unique(vcat(map(getfields, args)...))\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.aggregate","page":"Finch Logic","title":"Finch.FinchLogic.aggregate","text":"aggregate(op, init, arg, idxs...)\n\nLogical AST statement that reduces arg using op, starting with init. idxs are the dimensions to reduce. May happen in any order.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.reorder","page":"Finch Logic","title":"Finch.FinchLogic.reorder","text":"reorder(arg, idxs...)\n\nLogical AST statement that reorders the dimensions of arg to be idxs.... Dimensions known to be length 1 may be dropped. Dimensions that do not exist in arg may be added.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.relabel","page":"Finch Logic","title":"Finch.FinchLogic.relabel","text":"relabel(arg, idxs...)\n\nLogical AST statement that relabels the dimensions of arg to be idxs...\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.reformat","page":"Finch Logic","title":"Finch.FinchLogic.reformat","text":"reformat(tns, arg)\n\nLogical AST statement that reformats arg into the tensor tns.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.subquery","page":"Finch Logic","title":"Finch.FinchLogic.subquery","text":"subquery(lhs, arg)\n\nLogical AST statement that evaluates arg, binding the result to lhs, and returns arg.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.query","page":"Finch Logic","title":"Finch.FinchLogic.query","text":"query(lhs, rhs)\n\nLogical AST statement that evaluates rhs, binding the result to lhs.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.produces","page":"Finch Logic","title":"Finch.FinchLogic.produces","text":"produces(args...)\n\nLogical AST statement that returns args... from the current plan. Halts execution of the program.\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.plan","page":"Finch Logic","title":"Finch.FinchLogic.plan","text":"plan(bodies...)\n\nLogical AST statement that executes a sequence of statements bodies....\n\n\n\n\n\n","category":"constant"},{"location":"reference/internals/finch_logic/#Finch-Logic-Internals","page":"Finch Logic","title":"Finch Logic Internals","text":"","category":"section"},{"location":"reference/internals/finch_logic/","page":"Finch Logic","title":"Finch Logic","text":"FinchLogic.LogicNode\nFinchLogic.logic_leaf\nisimmediate\nisdeferred\nisalias\nisfield","category":"page"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.LogicNode","page":"Finch Logic","title":"Finch.FinchLogic.LogicNode","text":"LogicNode\n\nA Finch Logic IR node. Finch uses a variant of Concrete Field Notation as an intermediate representation.\n\nThe LogicNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchLogic.LogicNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.logic_leaf","page":"Finch Logic","title":"Finch.FinchLogic.logic_leaf","text":"logic_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a immediate or value.\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.isimmediate","page":"Finch Logic","title":"Finch.FinchLogic.isimmediate","text":"isimmediate(node)\n\nReturns true if the node is a finch immediate\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.isdeferred","page":"Finch Logic","title":"Finch.FinchLogic.isdeferred","text":"isdeferred(node)\n\nReturns true if the node is a finch immediate\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.isalias","page":"Finch Logic","title":"Finch.FinchLogic.isalias","text":"isalias(node)\n\nReturns true if the node is a finch alias\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/#Finch.FinchLogic.isfield","page":"Finch Logic","title":"Finch.FinchLogic.isfield","text":"isfield(node)\n\nReturns true if the node is a finch field\n\n\n\n\n\n","category":"function"},{"location":"reference/internals/finch_logic/#Executing-FinchLogic","page":"Finch Logic","title":"Executing FinchLogic","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"CurrentModule = Finch","category":"page"},{"location":"guides/user-defined_functions/#User-Defined-Functions","page":"User-Defined Functions","title":"User-Defined Functions","text":"","category":"section"},{"location":"guides/user-defined_functions/#User-Functions","page":"User-Defined Functions","title":"User Functions","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch supports arbitrary Julia Base functions over isbits types.  You can also use your own functions and use them in Finch! Just remember to define any special algebraic properties of your functions so that Finch can optimize them better. You must declare the properties of your functions before you call any Finch functions on them.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch only supports incrementing assignments to arrays such as += or *=. If you would like to increment A[i...] by the value of ex with a custom reduction operator op, you may use the following syntax: A[i...] <<op>>= ex.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Consider the greatest common divisor function gcd. This function is associative and commutative, and the greatest common divisor of 1 and anything else is 1, so 1 is an annihilator.  We declare these properties by overloading trait functions on Finch's default algebra as follows:","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch.isassociative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::Finch.DefaultAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Then, the following code will only call gcd when neither u[i] nor v[i] are 1 (just once!).","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"u = Tensor(SparseList(Element(1)), [3, 1, 6, 1, 9, 1, 4, 1, 8, 1])\nv = Tensor(SparseList(Element(1)), [1, 2, 3, 1, 1, 1, 1, 4, 1, 1])\nw = Tensor(SparseList(Element(1)))\n\n@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end)","category":"page"},{"location":"guides/user-defined_functions/#A-Few-Convenient-Functions","page":"User-Defined Functions","title":"A Few Convenient Functions","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"For your convenience, Finch defines a few useful functions that help express common array operations inside Finch:","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"choose\nminby\nmaxby","category":"page"},{"location":"guides/user-defined_functions/#Finch.choose","page":"User-Defined Functions","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5-Tensor\n└─ SparseList (0.0) [1:5]\n   ├─ [2]: 1.1\n   └─ [4]: 4.4\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<choose(1.1)>>= a[i] end;\n\njulia> x[]\n0.0\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.minby","page":"User-Defined Functions","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch for i=_; x[] <<minby>>= a[i] => i end;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.maxby","page":"User-Defined Functions","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch for i=_; x[] <<maxby>>= a[i] => i end;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Properties","page":"User-Defined Functions","title":"Properties","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"The full list of properties recognized by Finch is as follows (use these to declare the properties of your own functions):","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"isassociative\niscommutative\nisdistributive\nisidempotent\nisidentity\nisannihilator\nisinverse\nisinvolution\nFinch.return_type","category":"page"},{"location":"guides/user-defined_functions/#Finch.isassociative","page":"User-Defined Functions","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.iscommutative","page":"User-Defined Functions","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isdistributive","page":"User-Defined Functions","title":"Finch.isdistributive","text":"isdistributive(algebra, f, g)\n\nReturn true when f(a, g(b, c)) = g(f(a, b), f(a, c)) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isidempotent","page":"User-Defined Functions","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isidentity","page":"User-Defined Functions","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isannihilator","page":"User-Defined Functions","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isinverse","page":"User-Defined Functions","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.isinvolution","page":"User-Defined Functions","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.return_type","page":"User-Defined Functions","title":"Finch.return_type","text":"return_type(algebra, f, arg_types...)\n\nGive the return type of f when applied to arguments of types arg_types... in algebra. Used to determine output types of functions in the high-level interface. This function falls back to Base.promote_op.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch-Kernel-Caching","page":"User-Defined Functions","title":"Finch Kernel Caching","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch code is cached when you first run it. Thus, if you run a Finch function once, then make changes to the Finch compiler (like defining new properties), the cached code will be used and the changes will not be reflected.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"It's best to design your code so that modifications to the Finch compiler occur before any Finch functions are called. However, if you really need to modify a precompiled Finch kernel, you can call Finch.refresh() to invalidate the code cache.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"refresh","category":"page"},{"location":"guides/user-defined_functions/#Finch.refresh","page":"User-Defined Functions","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#(Advanced)-On-World-Age-and-Generated-Functions","page":"User-Defined Functions","title":"(Advanced) On World-Age and Generated Functions","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Julia uses a \"world age\" to describe the set of defined functions at a point in time. Generated functions run in the same world age in which they were defined, so they can't call functions defined after the generated function. This means that if Finch used normal generated functions, users can't define their own functions without first redefining all of Finch's generated functions.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Finch uses special generators that run in the current world age, but do not update with subsequent compiler function invalidations. If two packages modify the behavior of Finch in different ways, and call those Finch functions during precompilation, the resulting behavior is undefined.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"There are several packages that take similar, but different, approaches to allow user participation in staged Julia programming (not to mention Base eval or @generated): StagedFunctions.jl, GeneralizedGenerated.jl, RuntimeGeneratedFunctions.jl, or Zygote.jl.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Our approach is most similar to that of StagedFunctions.jl or Zygote.jl. We chose our approach to be the simple and flexible while keeping the kernel call overhead low.","category":"page"},{"location":"guides/user-defined_functions/#(Advanced)-Separate-Algebras","page":"User-Defined Functions","title":"(Advanced) Separate Algebras","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"If you want to define non-standard properties or custom rewrite rules for some functions in a separate context, you can represent these changes with your own algebra type.  We express this by subtyping AbstractAlgebra and defining properties as follows:","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"struct MyAlgebra <: AbstractAlgebra end\n\nFinch.isassociative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::MyAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"We pass the algebra to Finch as an optional first argument:","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end; return w)","category":"page"},{"location":"guides/user-defined_functions/#Rewriting","page":"User-Defined Functions","title":"Rewriting","text":"","category":"section"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"Define custom rewrite rules by overloading the get_simplify_rules function on your algebra.  Unless you want to write the full rule set from scratch, be sure to append your new rules to the old rules, which can be obtained by calling get_simplify_rules with another algebra. Rules can be specified directly on Finch IR using RewriteTools.jl.","category":"page"},{"location":"guides/user-defined_functions/","page":"User-Defined Functions","title":"User-Defined Functions","text":"get_simplify_rules\nget_prove_rules","category":"page"},{"location":"guides/user-defined_functions/#Finch.get_simplify_rules","page":"User-Defined Functions","title":"Finch.get_simplify_rules","text":"get_simplify_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"function"},{"location":"guides/user-defined_functions/#Finch.get_prove_rules","page":"User-Defined Functions","title":"Finch.get_prove_rules","text":"get_prove_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"function"},{"location":"appendices/directory_structure/#Directory-Structure","page":"Directory Structure","title":"Directory Structure","text":"","category":"section"},{"location":"appendices/directory_structure/","page":"Directory Structure","title":"Directory Structure","text":"Here's a little roadmap to the Finch codebase! Please file an issue if this is not up to date.","category":"page"},{"location":"appendices/directory_structure/","page":"Directory Structure","title":"Directory Structure","text":".\n├── benchmark                  # benchmarks for internal use\n│   ├── runbenchmarks.jl       # run benchmarks\n│   ├── runjudge.jl            # run benchmarks on current branch and compare with main\n│   └── ...\n├── docs                       # documentation\n│   ├── [build]                # rendered docs website\n│   ├── src                    # docs website source\n│   ├── fix.jl                 # fix docstrings\n│   ├── examples               # example applications implemented in Finch!\n│   │   └── ...\n│   ├── make.jl                # build documentation locally\n│   └── ...\n├── ext                        # conditionally-loaded code for interaction with other packages (e.g. SparseArrays)\n├── src                        # Source files\n│   ├── interface              # Implementations of array api functions (e.g. map, reduce, etc.)\n│   │   ├── fileio             # File IO function definitions\n│   │   └── ...\n│   ├── FinchLogic             # SubModule containing the High-Level IR\n│   │   ├── nodes.jl           # defines the High-Level IR\n│   │   └── ...\n│   ├── scheduler              # Auto-Scheduler to compile High-Level IR to Finch IR\n│   │   └── ...\n│   ├── FinchNotation          # SubModule containing the Finch IR\n│   │   ├── nodes.jl           # defines the Finch IR\n│   │   ├── syntax.jl          # defines the @finch frontend syntax\n│   │   └── ...\n│   ├── looplets               # this is where all the Looplets live\n│   ├── symbolic               # term rewriting systems for program and bounds\n│   ├── tensors                # built-in Finch tensor definitions\n│   │   ├── levels             # all of the levels\n│   │   │   └── ...\n│   │   ├── combinators        # tensor combinators which modify tensor behavior\n│   │   │   └── ...\n│   │   ├── fibers.jl          # fibers combine levels to form tensors\n│   │   ├── scalars.jl         # a nice scalar type\n│   │   └── masks.jl           # mask tensors (e.g. upper-triangular mask)\n│   ├── transformations        # global program transformations\n│   │   ├── scopes.jl          # gives unique names to indices\n│   │   ├── lifetimes.jl       # adds freeze and thaw\n│   │   ├── dimensionalize.jl  # computes extents for loops and declarations\n│   │   ├── concordize.jl      # adds loops to ensure all accesses are concordant\n│   │   └── wrapperize.jl      # converts index expressions to array wrappers\n│   ├── abstract_tensor.jl     # finch array interface functions\n│   ├── execute.jl             # global compiler calls\n│   ├── lower.jl               # inner compiler definition\n│   ├── util                   # shims and julia codegen utils (Dead code elimination, etc...)\n│   │   └── ...\n│   └── ...\n├── test                       # tests\n│   ├──  reference32           # reference output for 32-bit systems\n│   ├──  reference64           # reference output for 64-bit systems\n│   ├──  runtests.jl           # runs the test suite. (pass -h for options and more info!)\n│   └── ...\n├── Project.toml               # julia-readable listing of project dependencies\n├── [Manifest.toml]            # local listing of installed dependencies (don't commit this)\n├── LICENSE\n├── CONTRIBUTING.md\n└── README.md","category":"page"},{"location":"reference/internals/looplets_coiteration/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"getting_started/#TODO","page":"TODO","title":"TODO","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"CurrentModule = Finch","category":"page"},{"location":"guides/finch_language/#Finch-Notation","page":"The Finch Language","title":"Finch Notation","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are written in Julia, but they are not Julia programs. Instead, they are an abstraction description of a tensor computation.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are blocks of tensor operations, joined by control flow. Finch is an imperative language. The AST is separated into statements and expressions, where statements can modify the state of the program but expressions cannot.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"The core Finch expressions are:","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"literal e.g. 1, 1.0, nothing\nvalue e.g. x, y\nindex e.g. i, inside of for i = _; ... end\nvariable e.g. x, inside of (x = y; ...)\ncall e.g. op(args...)\naccess e.g. tns[idxs...]","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"And the core Finch statements are:","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"declare e.g. tns .= init\nassign e.g. lhs[idxs...] <<op>>= rhs\nloop e.g. for i = _; ... end\ndefine e.g. let var = val; ... end\nsieve e.g. if cond; ... end\nblock e.g. begin ... end","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"literal\nvalue\nindex\nvariable\ncall\naccess\ndefine\nassign\nloop\nsieve\nblock","category":"page"},{"location":"guides/finch_language/#Finch.FinchNotation.literal","page":"The Finch Language","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.value","page":"The Finch Language","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.index","page":"The Finch Language","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.variable","page":"The Finch Language","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.call","page":"The Finch Language","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.access","page":"The Finch Language","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.define","page":"The Finch Language","title":"Finch.FinchNotation.define","text":"define(lhs, rhs, body)\n\nFinch AST statement that defines lhs as having the value rhs in body. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.assign","page":"The Finch Language","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). Overwriting is accomplished with the function overwrite(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.loop","page":"The Finch Language","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.sieve","page":"The Finch Language","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.block","page":"The Finch Language","title":"Finch.FinchNotation.block","text":"block(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn. If the body is not a block, replaces accesses to tensors in the body with instantiate.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Scoping","page":"The Finch Language","title":"Scoping","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch programs are scoped. Scopes contain variable definitions and tensor declarations.  Loops and sieves introduce new scopes. The following program has four scopes, each of which is numbered to the left of the statements it contains.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"@finch begin\n1   y .= 0\n1   for j = _\n1   2   t .= 0\n1   2   for i = _\n1   2   3   t[] += A[i, j] * x[i]\n1   2   end\n1   2   for i = _\n1   2   4   y[i] += A[i, j] * t[]\n1   2   end\n1   end\nend","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Variables refer to their defined values in the innermost containing scope. If variables are undefined, they are assumed to have global scope (they may come from the surrounding program).","category":"page"},{"location":"guides/finch_language/#Tensor-Lifecycle","page":"The Finch Language","title":"Tensor Lifecycle","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensors have two modes: Read and Update. Tensors in read mode may be read, but not updated. Tensors in update mode may be updated, but not read. A tensor declaration initializes and possibly resizes the tensor, setting it to update mode. Also, Finch will automatically change the mode of tensors as they are used. However, tensors may only change their mode within scopes that contain their declaration. If a tensor has not been declared, it is assumed to have global scope.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensor declaration is different than variable definition. Declaring a tensor initializes the memory (usually to zero) and sets the tensor to update mode. Defining a tensor simply gives a name to that memory. A tensor may be declared multiple times, but it may only be defined once.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensors are assumed to be in read mode when they are defined. Tensors must enter and exit scope in read mode. Finch inserts freeze and thaw statements to ensure that tensors are in the correct mode. Freezing a tensor prevents further updates and allows reads. Thawing a tensor allows further updates and prevents reads.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Tensor lifecycle statements consist of:","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"declare\nfreeze\nthaw","category":"page"},{"location":"guides/finch_language/#Finch.FinchNotation.declare","page":"The Finch Language","title":"Finch.FinchNotation.declare","text":"declare(tns, init)\n\nFinch AST statement that declares tns with an initial value init in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.freeze","page":"The Finch Language","title":"Finch.FinchNotation.freeze","text":"freeze(tns)\n\nFinch AST statement that freezes tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Finch.FinchNotation.thaw","page":"The Finch Language","title":"Finch.FinchNotation.thaw","text":"thaw(tns)\n\nFinch AST statement that thaws tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"guides/finch_language/#Dimensionalization","page":"The Finch Language","title":"Dimensionalization","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch loops have dimensions. Accessing a tensor with an unmodified loop index \"hints\" that the loop should have the same dimension as the corresponding axis of the tensor. Finch will automatically dimensionalize loops that are hinted by tensor accesses. One may refer to the automatically determined dimension using a variable named _ or :.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Similarly, tensor declarations also set the dimensions of a tensor. Accessing a tensor with an unmodified loop index \"hints\" that the tensor axis should have the same dimension as the corresponding loop. Finch will automatically dimensionalize declarations based on all updates up to the first read.","category":"page"},{"location":"guides/finch_language/#Array-Combinators","page":"The Finch Language","title":"Array Combinators","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch includes several array combinators that modify the behavior of arrays. For example, the OffsetArray type wraps an existing array, but shifts its indices. The PermissiveArray type wraps an existing array, but allows out-of-bounds reads and writes. When an array is accessed out of bounds, it produces Missing.","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Array combinators introduce some complexity to the tensor lifecycle, as wrappers may contain multiple or different arrays that could potentially be in different modes. Any array combinators used in a tensor access must reference a single global variable which holds the root array. The root array is the single array that gets declared, and changes modes from read to update, or vice versa.","category":"page"},{"location":"guides/finch_language/#Fancy-Indexing","page":"The Finch Language","title":"Fancy Indexing","text":"","category":"section"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Finch supports arbitrary indexing of arrays, but certain indexing operations have first class support through array combinators. Before dimensionalization, the following transformations are performed:","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"    A[i + c] =>        OffsetArray(A, c)[i]\n    A[i + j] =>      ToeplitzArray(A, 1)[i, j]\n       A[~i] => PermissiveArray(A, true)[i]","category":"page"},{"location":"guides/finch_language/","page":"The Finch Language","title":"The Finch Language","text":"Note that these transformations may change the behavior of dimensionalization, since they often result in unmodified loop indices (the index i will participate in dimensionalization, but an index expression like i + 1 will not).","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"CurrentModule = Finch","category":"page"},{"location":"guides/tensor_formats/#Constructing-Tensors","page":"Tensor Formats","title":"Constructing Tensors","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"You can build a finch tensor with the Tensor constructor. In general, the Tensor constructor mirrors Julia's Array constructor, but with an additional prefixed argument which specifies the formatted storage for the tensor.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"For example, to construct an empty sparse matrix:","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"julia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), 4, 3)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"To initialize a sparse matrix with some values:","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\n\njulia> A_fbr = Tensor(Dense(SparseList(Element(0.0))), A)\n4×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:4]\n   │  ├─ [2]: 1.1\n   │  ├─ [3]: 2.2\n   │  └─ [4]: 3.3\n   ├─ [:, 2]: SparseList (0.0) [1:4]\n   └─ [:, 3]: SparseList (0.0) [1:4]\n      ├─ [1]: 4.4\n      └─ [3]: 5.5","category":"page"},{"location":"guides/tensor_formats/#Storage-Tree-Level-Formats","page":"Tensor Formats","title":"Storage Tree Level Formats","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"This section describes the formatted storage for Finch tensors, the first argument to the Tensor constructor. Level storage types holds all of the tensor data, and can be nested hierarchichally.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch represents tensors hierarchically in a tree, where each node in the tree is a vector of subtensors and the leaves are the elements.  Thus, a matrix is analogous to a vector of vectors, and a 3-tensor is analogous to a vector of vectors of vectors.  The vectors at each level of the tensor all have the same structure, which can be selected by the user.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"In a Finch tensor tree, the child of each node is selected by an array index. All of the children at the same level will use the same format and share the same storage. Finch is column major, so in an expression A[i_1, ..., i_N], the rightmost dimension i_N corresponds to the root level of the tree, and the leftmost dimension i_1 corresponds to the leaf level.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Our example could be visualized as follows:","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"(Image: CSC Format Index Tree)","category":"page"},{"location":"guides/tensor_formats/#Types-of-Level-Storage","page":"Tensor Formats","title":"Types of Level Storage","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch supports a variety of storage formats for each level of the tensor tree, each with advantages and disadvantages. Some storage formats support in-order access, while others support random access. Some storage formats must be written to in column-major order, while others support out-of-order writes. The capabilities of each level are summarized in the following tables along with some general descriptions.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Level Format Name Group Data Characteristic Column-Major Reads Random Reads Column-Major Bulk Update Random Bulk Update Random Updates Status\nDense Core Dense ✅ ✅ ✅ ✅ ✅ ✅\nSparseTree Core Sparse ✅ ✅ ✅ ✅ ✅ ⚙️\nSparseRLETree Core Sparse Runs ✅ ✅ ✅ ✅ ✅ ⚙️\nElement Core Leaf ✅ ✅ ✅ ✅ ✅ ✅\nPattern Core Leaf ✅ ✅ ✅ ✅ ✅ ✅\nSparseList Advanced Sparse ✅ ❌ ✅ ❌ ❌ ✅\nSparseRLE Advanced Sparse Runs ✅ ❌ ✅ ❌ ❌ ✅\nSparseVBL Advanced Sparse Blocks ✅ ❌ ✅ ❌ ❌ ✅\nSparsePoint Advanced Single Sparse ✅ ✅ ✅ ❌ ❌ ✅\nSparseInterval Advanced Single Sparse Run ✅ ✅ ✅ ❌ ❌ ✅\nSparseBand Advanced Single Sparse Block ✅ ✅ ✅ ❌ ❌ ⚙️\nDenseRLE Advanced Dense Runs ✅ ❌ ✅ ❌ ❌ ⚙️\nSparseBytemap Advanced Sparse ✅ ✅ ✅ ✅ ❌ ✅\nSparseDict Advanced Sparse ✅ ✅ ✅ ✅ ❌ ✅️\nAtomicLevel Modifier No Data ✅ ✅ ✅ ✅ ✅ ⚙️\nSeperationLevel Modifier No Data ✅ ✅ ✅ ✅ ✅ ⚙️\nSparseCOO Legacy Sparse ✅ ✅ ✅ ❌ ✅ ✅️","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"The \"Level Format Name\" is the name of the level datatype. Other columns have descriptions below.","category":"page"},{"location":"guides/tensor_formats/#Status","page":"Tensor Formats","title":"Status","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Symbol Meaning\n✅ Indicates the level is ready for serious use.\n⚙️ Indicates the level is experimental and under development.\n🕸️ Indicates the level is deprecated, and may be removed in a future release.","category":"page"},{"location":"guides/tensor_formats/#Groups","page":"Tensor Formats","title":"Groups","text":"","category":"section"},{"location":"guides/tensor_formats/#Core-Group","page":"Tensor Formats","title":"Core Group","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains the basic, minimal set of levels one should use to build and manipulate tensors.  These levels can be efficiently read and written to in any order.","category":"page"},{"location":"guides/tensor_formats/#Advanced-Group","page":"Tensor Formats","title":"Advanced Group","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are more specialized, and geared towards bulk updates. These levels may be more efficient in certain cases, but are also more restrictive about access orders and intended for more advanced usage.","category":"page"},{"location":"guides/tensor_formats/#Modifier-Group","page":"Tensor Formats","title":"Modifier Group","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are also more specialized, but not towards a sparsity pattern. These levels modify other levels in a variety of ways, but don't store novel sparsity patterns. Typically, they modify how levels are stored or attach data to levels to support the utilization of various hardware features.","category":"page"},{"location":"guides/tensor_formats/#Legacy-Group","page":"Tensor Formats","title":"Legacy Group","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Contains levels which are not recommended for new code, but are included for compatibility with older code.","category":"page"},{"location":"guides/tensor_formats/#Data-Characteristics","page":"Tensor Formats","title":"Data Characteristics","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Level Type Description\nDense Levels which store every subtensor.\nLeaf Levels which store only scalars, used for the leaf level of the tree.\nSparse Levels which store only non-fill values, used for levels with few nonzeros.\nSparse Runs Levels which store runs of repeated non-fill values.\nSparse Blocks Levels which store Blocks of repeated non-fill values.\nDense Runs Levels which store runs of repeated values, and no compile-time zero annihilation.\nNo Data Levels which don't store data but which alter the storage pattern or attach additional meta-data.","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Note that the Single sparse levels store a single instance of each nonzero, run, or block. These are useful with a parent level to represent IDs.","category":"page"},{"location":"guides/tensor_formats/#Access-Characteristics","page":"Tensor Formats","title":"Access Characteristics","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Operation Type Description\nColumn-Major Reads Indicates efficient reading of data in column-major order.\nRandom Reads Indicates efficient reading of data in random-access order.\nColumn-Major Bulk Update Indicates efficient writing of data in column-major order, the total time roughly linear to the size of the tensor.\nColumn-Major Random Update Indicates efficient writing of data in random-access order, the total time roughly linear to the size of the tensor.\nRandom Update Indicates efficient writing of data in random-access order, the total time roughly linear to the number of updates.","category":"page"},{"location":"guides/tensor_formats/#Examples-of-Popular-Formats-in-Finch","page":"Tensor Formats","title":"Examples of Popular Formats in Finch","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Finch levels can be used to construct a variety of popular sparse formats. A few examples follow:","category":"page"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Format Type Syntax\nSparse Vector Tensor(SparseList(Element(0.0)), args...)\nCSC Matrix Tensor(Dense(SparseList(Element(0.0))), args...)\nCSF 3-Tensor Tensor(Dense(SparseList(SparseList(Element(0.0)))), args...)\nDCSC (Hypersparse) Matrix Tensor(SparseList(SparseList(Element(0.0))), args...)\nCOO Matrix Tensor(SparseCOO{2}(Element(0.0)), args...)\nCOO 3-Tensor Tensor(SparseCOO{3}(Element(0.0)), args...)\nRun-Length-Encoded Image Tensor(Dense(DenseRLE(Element(0.0))), args...)","category":"page"},{"location":"guides/tensor_formats/#Tensor-Constructors","page":"Tensor Formats","title":"Tensor Constructors","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"Tensor\nTensor(lvl::AbstractLevel)\nTensor(lvl::AbstractLevel, dims::Number...)\nTensor(lvl::AbstractLevel, init::UndefInitializer)\nTensor(lvl::AbstractLevel, arr)\nTensor(arr)","category":"page"},{"location":"guides/tensor_formats/#Finch.Tensor","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor{Lvl} <: AbstractFiber{Lvl}\n\nThe multidimensional array type used by Finch. Tensor is a thin wrapper around the hierarchical level storage of type Lvl.\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl)\n\nConstruct a Tensor using the tensor level storage lvl. No initialization of storage is performed, it is assumed that position 1 of lvl corresponds to a valid tensor, and lvl will be wrapped as-is. Call a different constructor to initialize the storage.\n\n\n\n\n\n","category":"method"},{"location":"guides/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, [undef], dims...)\n\nConstruct a Tensor of size dims, and initialize to undef, potentially allocating memory.  Here undef is the UndefInitializer singleton type. dims... may be a variable number of dimensions or a tuple of dimensions, but it must correspond to the number of dimensions in lvl.\n\n\n\n\n\n","category":"method"},{"location":"guides/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, UndefInitializer}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"guides/tensor_formats/#Finch.Tensor-Tuple{Finch.AbstractLevel, Any}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"guides/tensor_formats/#Finch.Tensor-Tuple{Any}","page":"Tensor Formats","title":"Finch.Tensor","text":"Tensor(arr, [init = zero(eltype(arr))])\n\nCopy an array-like object arr into a corresponding, similar Tensor datastructure. Uses init as an initial value. May reuse memory when possible. To explicitly copy into a tensor, use @ref[copyto!].\n\nExamples\n\njulia> println(summary(Tensor(sparse([1 0; 0 1]))))\n2×2 Tensor(Dense(SparseList(Element(0))))\n\njulia> println(summary(Tensor(ones(3, 2, 4))))\n3×2×4 Tensor(Dense(Dense(Dense(Element(0.0)))))\n\n\n\n\n\n","category":"method"},{"location":"guides/tensor_formats/#Level-Constructors","page":"Tensor Formats","title":"Level Constructors","text":"","category":"section"},{"location":"guides/tensor_formats/#Core-Levels","page":"Tensor Formats","title":"Core Levels","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"DenseLevel\nElementLevel\nPatternLevel","category":"page"},{"location":"guides/tensor_formats/#Finch.DenseLevel","page":"Tensor Formats","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\njulia> ndims(Tensor(Dense(Element(0.0))))\n1\n\njulia> ndims(Tensor(Dense(Dense(Element(0.0)))))\n2\n\njulia> Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4])\n2×2-Tensor\n└─ Dense [:,1:2]\n   ├─ [:, 1]: Dense [1:2]\n   │  ├─ [1]: 1.0\n   │  └─ [2]: 3.0\n   └─ [:, 2]: Dense [1:2]\n      ├─ [1]: 2.0\n      └─ [2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.ElementLevel","page":"Tensor Formats","title":"Finch.ElementLevel","text":"ElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to Vf. Vf may optionally be given as the first argument.\n\nThe data is stored in a vector of type Val with eltype(Val) = Tv. The type Tp is the index type used to access Val.\n\njulia> Tensor(Dense(Element(0.0)), [1, 2, 3])\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.PatternLevel","page":"Tensor Formats","title":"Finch.PatternLevel","text":"PatternLevel{[Tp=Int]}()\n\nA subfiber of a pattern level is the Boolean value true, but it's fill_value is false. PatternLevels are used to create tensors that represent which values are stored by other fibers. See pattern! for usage examples.\n\njulia> Tensor(Dense(Pattern()), 3)\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: true\n   ├─ [2]: true\n   └─ [3]: true\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Advanced-Levels","page":"Tensor Formats","title":"Advanced Levels","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"SparseListLevel\nDenseRLELevel\nSparseRLELevel\nSparseVBLLevel\nSparseBandLevel\nSparsePointLevel\nSparseIntervalLevel\nSparseByteMapLevel\nSparseDictLevel","category":"page"},{"location":"guides/tensor_formats/#Finch.SparseListLevel","page":"Tensor Formats","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseList (0.0) [1:3]\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.DenseRLELevel","page":"Tensor Formats","title":"Finch.DenseRLELevel","text":"DenseRLELevel{[Ti=Int], [Ptr, Right]}(lvl, [dim], [merge = true])\n\nThe dense RLE level represent runs of equivalent slices A[:, ..., :, i]. A sorted list is used to record the right endpoint of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> Tensor(Dense(DenseRLELevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: DenseRLE (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  ├─ [2:2]: 30.0\n   │  └─ [3:3]: 0.0\n   ├─ [:, 2]: DenseRLE (0.0) [1:3]\n   │  └─ [1:3]: 0.0\n   └─ [:, 3]: DenseRLE (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      ├─ [2:2]: 0.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseRLELevel","page":"Tensor Formats","title":"Finch.SparseRLELevel","text":"SparseRLELevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim]; [merge = true])\n\nThe sparse RLE level represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A sorted list is used to record the left and right endpoints of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr, Left, and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> Tensor(Dense(SparseRLELevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseRLE (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  └─ [2:2]: 30.0\n   ├─ [:, 2]: SparseRLE (0.0) [1:3]\n   └─ [:, 3]: SparseRLE (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseVBLLevel","page":"Tensor Formats","title":"Finch.SparseVBLLevel","text":"SparseVBLLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseListLevel, but contiguous subfibers are stored together in blocks.\n\n```jldoctest julia> Tensor(Dense(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\njulia> Tensor(SparseVBL(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) SparseList (0.0) [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseBandLevel","page":"Tensor Formats","title":"Finch.SparseBandLevel","text":"SparseBandLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseVBLLevel, but stores only a single block, and fills in zeros.\n\n```jldoctest julia> Tensor(Dense(SparseBand(Element(0.0))), [10 0 20; 30 40 0; 0 0 50]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparsePointLevel","page":"Tensor Formats","title":"Finch.SparsePointLevel","text":"SparsePointLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a SparsePoint level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. A main difference compared to SparseList level is that SparsePoint level only stores a 'single' non-fill slice. It emits an error if the program tries to write multiple (>=2) coordinates into SparsePoint.\n\nTi is the type of the last tensor index. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparsePoint(Element(0.0))), [10 0 0; 0 20 0; 0 0 30])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparsePoint (0.0) [1:3]\n   │  └─ 10.0\n   ├─ [:, 2]: SparsePoint (0.0) [1:3]\n   │  └─ 20.0\n   └─ [:, 3]: SparsePoint (0.0) [1:3]\n      └─ 30.0\n\njulia> Tensor(SparsePoint(Dense(Element(0.0))), [0 0 0; 0 0 30; 0 0 30])\n3×3-Tensor\n└─ SparsePoint (0.0) [:,1:3]\n   └─ Dense [1:3]\n      ├─ [1]: 0.0\n      ├─ [2]: 30.0\n      └─ [3]: 30.0\n\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseIntervalLevel","page":"Tensor Formats","title":"Finch.SparseIntervalLevel","text":"SparseIntervalLevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim])\n\nThe single RLE level represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A main difference compared to SparseRLE level is that SparseInterval level only stores a 'single' non-fill run. It emits an error if the program tries to write multiple (>=2) runs into SparseInterval.\n\nTi is the type of the last tensor index. The types Ptr, Left, and 'Right' are the types of the arrays used to store positions and endpoints.\n\njulia> Tensor(SparseInterval(Element(0)), [0, 10, 0])\n3-Tensor\n└─ SparseInterval (0) [1:3]\n   └─ [2:2]: 10\n\njulia> x = Tensor(SparseInterval(Element(0)), 10);\n\njulia> @finch begin for i = extent(3,6); x[~i] = 1 end end;\n\njulia> x\n10-Tensor\n└─ SparseInterval (0) [1:10]\n   └─ [3:6]: 1\n\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseByteMapLevel","page":"Tensor Formats","title":"Finch.SparseByteMapLevel","text":"SparseByteMapLevel{[Ti=Int], [Ptr, Tbl]}(lvl, [dims])\n\nLike the SparseListLevel, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level.\n\njulia> Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseByteMap (0.0) [1:3]\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n      ├─ [1]: 0.0\n      └─ [3]: 0.0\n\njulia> Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseByteMap (0.0) [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Finch.SparseDictLevel","page":"Tensor Formats","title":"Finch.SparseDictLevel","text":"SparseDictLevel{[Ti=Int], [Tp=Int], [Ptr, Idx, Val, Tbl, Pool=Dict]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A datastructure specified by Tbl is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseDict(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseDict (0.0) [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Modifier-Levels","page":"Tensor Formats","title":"Modifier Levels","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"SeparateLevel","category":"page"},{"location":"guides/tensor_formats/#Finch.SeparateLevel","page":"Tensor Formats","title":"Finch.SeparateLevel","text":"SeparateLevel{Lvl, [Val]}()\n\nA subfiber of a Separate level is a separate tensor of type Lvl, in it's own memory space.\n\nEach sublevel is stored in a vector of type Val with eltype(Val) = Lvl.\n\njulia> Tensor(Dense(Separate(Element(0.0))), [1, 2, 3])\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Pointer ->\n   │  └─ 1.0\n   ├─ [2]: Pointer ->\n   │  └─ 2.0\n   └─ [3]: Pointer ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"guides/tensor_formats/#Legacy-Levels","page":"Tensor Formats","title":"Legacy Levels","text":"","category":"section"},{"location":"guides/tensor_formats/","page":"Tensor Formats","title":"Tensor Formats","text":"SparseCOOLevel","category":"page"},{"location":"guides/tensor_formats/#Finch.SparseCOOLevel","page":"Tensor Formats","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr, Tbl]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTI is the type of the last N tensor indices, and Tp is the type used for positions in the level.\n\nThe type Tbl is an NTuple type where each entry k is a subtype AbstractVector{TI[k]}.\n\nThe type Ptr is the type for the pointer array.\n\njulia> Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseCOO{1} (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseCOO{1} (0.0) [1:3]\n   └─ [:, 3]: SparseCOO{1} (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [1, 1]: 10.0\n   ├─ [2, 1]: 30.0\n   ├─ [1, 3]: 20.0\n   └─ [3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Documentation-Listing","page":"Documentation Listing","title":"Documentation Listing","text":"","category":"section"},{"location":"reference/listing/","page":"Documentation Listing","title":"Documentation Listing","text":"Modules = [Finch, Finch.FinchNotation]","category":"page"},{"location":"reference/listing/#Finch.bandmask-reference-listing","page":"Documentation Listing","title":"Finch.bandmask","text":"bandmask\n\nA mask for a banded tensor, bandmask[i, j, k] = j <= i <= k. Note that this specializes each column for the cases where i < j, j <= i <= k, and k < i.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.diagmask-reference-listing","page":"Documentation Listing","title":"Finch.diagmask","text":"diagmask\n\nA mask for a diagonal tensor, diagmask[i, j] = i == j. Note that this specializes each column for the cases where i < j, i == j, and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.lotrimask-reference-listing","page":"Documentation Listing","title":"Finch.lotrimask","text":"lotrimask\n\nA mask for an upper triangular tensor, lotrimask[i, j] = i >= j. Note that this specializes each column for the cases where i < j and i >= j.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.uptrimask-reference-listing","page":"Documentation Listing","title":"Finch.uptrimask","text":"uptrimask\n\nA mask for an upper triangular tensor, uptrimask[i, j] = i <= j. Note that this specializes each column for the cases where i <= j and i > j.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Core.Array-Tuple{Union{Finch.SwizzleArray, Tensor}}-reference-listing","page":"Documentation Listing","title":"Core.Array","text":"Array(arr::Union{Tensor, SwizzleArray})\n\nConstruct an array from a tensor or swizzle. May reuse memory, will usually densify the tensor.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.DefaultLogicOptimizer-reference-listing","page":"Documentation Listing","title":"Finch.DefaultLogicOptimizer","text":"DefaultLogicOptimizer(ctx)\n\nThe default optimizer for finch logic programs. Optimizes to a structure suitable for the LogicCompiler or LogicInterpreter, then calls ctx on the resulting program.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.DenseData-reference-listing","page":"Documentation Listing","title":"Finch.DenseData","text":"DenseData(lvl)\n\nRepresents a tensor A where each A[:, ..., :, i] is represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.DenseLevel-reference-listing","page":"Documentation Listing","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\njulia> ndims(Tensor(Dense(Element(0.0))))\n1\n\njulia> ndims(Tensor(Dense(Dense(Element(0.0)))))\n2\n\njulia> Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4])\n2×2-Tensor\n└─ Dense [:,1:2]\n   ├─ [:, 1]: Dense [1:2]\n   │  ├─ [1]: 1.0\n   │  └─ [2]: 3.0\n   └─ [:, 2]: Dense [1:2]\n      ├─ [1]: 2.0\n      └─ [2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.DenseRLELevel-reference-listing","page":"Documentation Listing","title":"Finch.DenseRLELevel","text":"DenseRLELevel{[Ti=Int], [Ptr, Right]}(lvl, [dim], [merge = true])\n\nThe dense RLE level represent runs of equivalent slices A[:, ..., :, i]. A sorted list is used to record the right endpoint of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> Tensor(Dense(DenseRLELevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: DenseRLE (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  ├─ [2:2]: 30.0\n   │  └─ [3:3]: 0.0\n   ├─ [:, 2]: DenseRLE (0.0) [1:3]\n   │  └─ [1:3]: 0.0\n   └─ [:, 3]: DenseRLE (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      ├─ [2:2]: 0.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.ElementData-reference-listing","page":"Documentation Listing","title":"Finch.ElementData","text":"ElementData(fill_value, eltype)\n\nRepresents a scalar element of type eltype and fillvalue `fillvalue`.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.ElementLevel-reference-listing","page":"Documentation Listing","title":"Finch.ElementLevel","text":"ElementLevel{Vf, [Tv=typeof(Vf)], [Tp=Int], [Val]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to Vf. Vf may optionally be given as the first argument.\n\nThe data is stored in a vector of type Val with eltype(Val) = Tv. The type Tp is the index type used to access Val.\n\njulia> Tensor(Dense(Element(0.0)), [1, 2, 3])\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: 1.0\n   ├─ [2]: 2.0\n   └─ [3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.ExtrudeData-reference-listing","page":"Documentation Listing","title":"Finch.ExtrudeData","text":"ExtrudeData(lvl)\n\nRepresents a tensor A where A[:, ..., :, 1] is the only slice, and is represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.FinchCompiler-reference-listing","page":"Documentation Listing","title":"Finch.FinchCompiler","text":"FinchCompiler\n\nThe core compiler for Finch, lowering canonicalized Finch IR to Julia code.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.HollowData-reference-listing","page":"Documentation Listing","title":"Finch.HollowData","text":"HollowData(lvl)\n\nRepresents a tensor which is represented by lvl but is sometimes entirely fill_value(lvl).\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.Infinitesimal-reference-listing","page":"Documentation Listing","title":"Finch.Infinitesimal","text":"Infintesimal(s)\n\nThe Infintesimal type represents an infinitestimal number.  The sign field is used to represent positive, negative, or zero in this number system.\n\n```jl-doctest julia> tiny() +0\n\njulia> positive_tiny() +ϵ\n\njulia> negative_tiny() -ϵ\n\njulia> positivetiny() + negativetiny() +0\n\njulia> positive_tiny() * 2 +ϵ\n\njulia> positivetiny() * negativetiny() -ϵ\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.JuliaContext-reference-listing","page":"Documentation Listing","title":"Finch.JuliaContext","text":"JuliaContext\n\nA context for compiling Julia code, managing side effects, parallelism, and variable names in the generated code of the executing environment.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.Limit-reference-listing","page":"Documentation Listing","title":"Finch.Limit","text":"Limit{T}(x, s)\n\nThe Limit type represents endpoints of closed and open intervals.  The val field is the value of the endpoint.  The sign field is used to represent the openness/closedness of the interval endpoint, using an Infinitesmal.\n\n```jl-doctest julia> limit(1.0) 1.0+0\n\njulia> plus_eps(1.0) 1.0+ϵ\n\njulia> minus_eps(1.0) 1.0-ϵ\n\njulia> pluseps(1.0) + minuseps(1.0) 2.0+0.0\n\njulia> plus_eps(1.0) * 2 2.0+2.0ϵ\n\njulia> pluseps(1.0) * minuseps(1.0) 1.0-1.0ϵ\n\njulia> pluseps(-1.0) * minuseps(1.0) -1.0+2.0ϵ\n\njulia> 1.0 < plus_eps(1.0) true\n\njulia> 1.0 < minus_eps(1.0) false\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.LogicCompiler-reference-listing","page":"Documentation Listing","title":"Finch.LogicCompiler","text":"LogicCompiler\n\nThe LogicCompiler is a simple compiler for finch logic programs. The interpreter is only capable of executing programs of the form:       REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)        ACCESS := reorder(relabel(ALIAS, idxs1::FIELD...), idxs2::FIELD...) where issubsequence(idxs1, idxs2)     POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE     MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)        TABLE  := table(IMMEDIATE | DEFERRED, FIELD...) COMPUTEQUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))   INPUTQUERY := query(ALIAS, TABLE)          STEP := COMPUTEQUERY | INPUTQUERY | produces(ALIAS...)          ROOT := PLAN(STEP...)\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.LogicExecutor-reference-listing","page":"Documentation Listing","title":"Finch.LogicExecutor","text":"LogicExecutor(ctx, verbose=false)\n\nExecutes a logic program by compiling it with the given compiler ctx. Compiled codes are cached, and are only compiled once for each program with the same structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.LogicExecutorCode-reference-listing","page":"Documentation Listing","title":"Finch.LogicExecutorCode","text":"LogicExecutorCode(ctx)\n\nReturn the code that would normally be used by the LogicExecutor to run a program.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.LogicInterpreter-reference-listing","page":"Documentation Listing","title":"Finch.LogicInterpreter","text":"LogicInterpreter(scope = Dict(), verbose = false, mode = :fast)\n\nThe LogicInterpreter is a simple interpreter for finch logic programs. The interpreter is only capable of executing programs of the form:       REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)        ACCESS := reorder(relabel(ALIAS, idxs1::FIELD...), idxs2::FIELD...) where issubsequence(idxs1, idxs2)     POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE     MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)        TABLE  := table(IMMEDIATE, FIELD...) COMPUTEQUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))   INPUTQUERY := query(ALIAS, TABLE)          STEP := COMPUTEQUERY | INPUTQUERY | produces(ALIAS...)          ROOT := PLAN(STEP...)\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.Namespace-reference-listing","page":"Documentation Listing","title":"Finch.Namespace","text":"Namespace\n\nA namespace for managing variable names and aesthetic fresh variable generation.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.PatternLevel-reference-listing","page":"Documentation Listing","title":"Finch.PatternLevel","text":"PatternLevel{[Tp=Int]}()\n\nA subfiber of a pattern level is the Boolean value true, but it's fill_value is false. PatternLevels are used to create tensors that represent which values are stored by other fibers. See pattern! for usage examples.\n\njulia> Tensor(Dense(Pattern()), 3)\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: true\n   ├─ [2]: true\n   └─ [3]: true\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.RepeatData-reference-listing","page":"Documentation Listing","title":"Finch.RepeatData","text":"RepeatData(lvl)\n\nRepresents a tensor A where A[:, ..., :, i] is sometimes entirely fill_value(lvl) and is sometimes represented by repeated runs of lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.ScopeContext-reference-listing","page":"Documentation Listing","title":"Finch.ScopeContext","text":"ScopeContext\n\nA context for managing variable bindings and tensor modes.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SeparateLevel-reference-listing","page":"Documentation Listing","title":"Finch.SeparateLevel","text":"SeparateLevel{Lvl, [Val]}()\n\nA subfiber of a Separate level is a separate tensor of type Lvl, in it's own memory space.\n\nEach sublevel is stored in a vector of type Val with eltype(Val) = Lvl.\n\njulia> Tensor(Dense(Separate(Element(0.0))), [1, 2, 3])\n3-Tensor\n└─ Dense [1:3]\n   ├─ [1]: Pointer ->\n   │  └─ 1.0\n   ├─ [2]: Pointer ->\n   │  └─ 2.0\n   └─ [3]: Pointer ->\n      └─ 3.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseBandLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseBandLevel","text":"SparseBandLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseVBLLevel, but stores only a single block, and fills in zeros.\n\n```jldoctest julia> Tensor(Dense(SparseBand(Element(0.0))), [10 0 20; 30 40 0; 0 0 50]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseByteMapLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseByteMapLevel","text":"SparseByteMapLevel{[Ti=Int], [Ptr, Tbl]}(lvl, [dims])\n\nLike the SparseListLevel, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level.\n\njulia> Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseByteMap (0.0) [1:3]\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n      ├─ [1]: 0.0\n      └─ [3]: 0.0\n\njulia> Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseByteMap (0.0) [:,1:3]\n   ├─ [:, 1]: SparseByteMap (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseByteMap (0.0) [1:3]\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseCOOLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr, Tbl]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTI is the type of the last N tensor indices, and Tp is the type used for positions in the level.\n\nThe type Tbl is an NTuple type where each entry k is a subtype AbstractVector{TI[k]}.\n\nThe type Ptr is the type for the pointer array.\n\njulia> Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseCOO{1} (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseCOO{1} (0.0) [1:3]\n   └─ [:, 3]: SparseCOO{1} (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseCOO{2} (0.0) [:,1:3]\n   ├─ [1, 1]: 10.0\n   ├─ [2, 1]: 30.0\n   ├─ [1, 3]: 20.0\n   └─ [3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseData-reference-listing","page":"Documentation Listing","title":"Finch.SparseData","text":"SparseData(lvl)\n\nRepresents a tensor A where A[:, ..., :, i] is sometimes entirely fill_value(lvl) and is sometimes represented by lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseDictLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseDictLevel","text":"SparseDictLevel{[Ti=Int], [Tp=Int], [Ptr, Idx, Val, Tbl, Pool=Dict]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A datastructure specified by Tbl is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseDict (0.0) [1:3]\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseDict(SparseDict(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseDict (0.0) [:,1:3]\n   ├─ [:, 1]: SparseDict (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseDict (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseIntervalLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseIntervalLevel","text":"SparseIntervalLevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim])\n\nThe single RLE level represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A main difference compared to SparseRLE level is that SparseInterval level only stores a 'single' non-fill run. It emits an error if the program tries to write multiple (>=2) runs into SparseInterval.\n\nTi is the type of the last tensor index. The types Ptr, Left, and 'Right' are the types of the arrays used to store positions and endpoints.\n\njulia> Tensor(SparseInterval(Element(0)), [0, 10, 0])\n3-Tensor\n└─ SparseInterval (0) [1:3]\n   └─ [2:2]: 10\n\njulia> x = Tensor(SparseInterval(Element(0)), 10);\n\njulia> @finch begin for i = extent(3,6); x[~i] = 1 end end;\n\njulia> x\n10-Tensor\n└─ SparseInterval (0) [1:10]\n   └─ [3:6]: 1\n\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseListLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   ├─ [:, 2]: SparseList (0.0) [1:3]\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\njulia> Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ SparseList (0.0) [:,1:3]\n   ├─ [:, 1]: SparseList (0.0) [1:3]\n   │  ├─ [1]: 10.0\n   │  └─ [2]: 30.0\n   └─ [:, 3]: SparseList (0.0) [1:3]\n      ├─ [1]: 20.0\n      └─ [3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparsePointLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparsePointLevel","text":"SparsePointLevel{[Ti=Int], [Ptr, Idx]}(lvl, [dim])\n\nA subfiber of a SparsePoint level does not need to represent slices A[:, ..., :, i] which are entirely fill_value. Instead, only potentially non-fill slices are stored as subfibers in lvl. A main difference compared to SparseList level is that SparsePoint level only stores a 'single' non-fill slice. It emits an error if the program tries to write multiple (>=2) coordinates into SparsePoint.\n\nTi is the type of the last tensor index. The types Ptr and Idx are the types of the arrays used to store positions and indicies.\n\njulia> Tensor(Dense(SparsePoint(Element(0.0))), [10 0 0; 0 20 0; 0 0 30])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparsePoint (0.0) [1:3]\n   │  └─ 10.0\n   ├─ [:, 2]: SparsePoint (0.0) [1:3]\n   │  └─ 20.0\n   └─ [:, 3]: SparsePoint (0.0) [1:3]\n      └─ 30.0\n\njulia> Tensor(SparsePoint(Dense(Element(0.0))), [0 0 0; 0 0 30; 0 0 30])\n3×3-Tensor\n└─ SparsePoint (0.0) [:,1:3]\n   └─ Dense [1:3]\n      ├─ [1]: 0.0\n      ├─ [2]: 30.0\n      └─ [3]: 30.0\n\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseRLELevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseRLELevel","text":"SparseRLELevel{[Ti=Int], [Ptr, Left, Right]}(lvl, [dim]; [merge = true])\n\nThe sparse RLE level represent runs of equivalent slices A[:, ..., :, i] which are not entirely fill_value. A sorted list is used to record the left and right endpoints of each run. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last tensor index, and Tp is the type used for positions in the level. The types Ptr, Left, and Right are the types of the arrays used to store positions and endpoints.\n\nThe merge keyword argument is used to specify whether the level should merge duplicate consecutive runs.\n\njulia> Tensor(Dense(SparseRLELevel(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\n3×3-Tensor\n└─ Dense [:,1:3]\n   ├─ [:, 1]: SparseRLE (0.0) [1:3]\n   │  ├─ [1:1]: 10.0\n   │  └─ [2:2]: 30.0\n   ├─ [:, 2]: SparseRLE (0.0) [1:3]\n   └─ [:, 3]: SparseRLE (0.0) [1:3]\n      ├─ [1:1]: 20.0\n      └─ [3:3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SparseVBLLevel-reference-listing","page":"Documentation Listing","title":"Finch.SparseVBLLevel","text":"SparseVBLLevel{[Ti=Int], [Ptr, Idx, Ofs]}(lvl, [dim])\n\nLike the SparseListLevel, but contiguous subfibers are stored together in blocks.\n\n```jldoctest julia> Tensor(Dense(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) Dense [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,2]: SparseList (0.0) [1:3] ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\njulia> Tensor(SparseVBL(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40]) SparseList (0.0) [:,1:3] ├─[:,1]: SparseList (0.0) [1:3] │ ├─[1]: 10.0 │ ├─[2]: 30.0 ├─[:,3]: SparseList (0.0) [1:3] │ ├─[1]: 20.0 │ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.StaticHash-reference-listing","page":"Documentation Listing","title":"Finch.StaticHash","text":"StaticHash\n\nA hash function which is static, i.e. the hashes are the same when objects are hashed in the same order. The hash is used to memoize the results of simplification and proof rules.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SubFiber-reference-listing","page":"Documentation Listing","title":"Finch.SubFiber","text":"SubFiber(lvl, pos)\n\nSubFiber represents a tensor at position pos within lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.SymbolicContext-reference-listing","page":"Documentation Listing","title":"Finch.SymbolicContext","text":"SymbolicContext\n\nA compiler context for symbolic computation, defined on an algebra.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.Tensor-Tuple{Finch.AbstractLevel, Any}-reference-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl, arr)\n\nConstruct a Tensor and initialize it to the contents of arr. To explicitly copy into a tensor, use @ref[copyto!]\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.Tensor-Tuple{Finch.AbstractLevel, Vararg{Number}}-reference-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl, [undef], dims...)\n\nConstruct a Tensor of size dims, and initialize to undef, potentially allocating memory.  Here undef is the UndefInitializer singleton type. dims... may be a variable number of dimensions or a tuple of dimensions, but it must correspond to the number of dimensions in lvl.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.Tensor-Tuple{Lvl} where Lvl<:Finch.AbstractLevel-reference-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(lvl)\n\nConstruct a Tensor using the tensor level storage lvl. No initialization of storage is performed, it is assumed that position 1 of lvl corresponds to a valid tensor, and lvl will be wrapped as-is. Call a different constructor to initialize the storage.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.Tensor-Union{Tuple{AbstractArray{Tv, N}}, Tuple{N}, Tuple{Tv}, Tuple{AbstractArray{Tv, N}, Tv}} where {Tv, N}-reference-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor(arr, [init = zero(eltype(arr))])\n\nCopy an array-like object arr into a corresponding, similar Tensor datastructure. Uses init as an initial value. May reuse memory when possible. To explicitly copy into a tensor, use @ref[copyto!].\n\nExamples\n\njulia> println(summary(Tensor(sparse([1 0; 0 1]))))\n2×2 Tensor(Dense(SparseList(Element(0))))\n\njulia> println(summary(Tensor(ones(3, 2, 4))))\n3×2×4 Tensor(Dense(Dense(Dense(Element(0.0)))))\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.Tensor-reference-listing","page":"Documentation Listing","title":"Finch.Tensor","text":"Tensor{Lvl} <: AbstractFiber{Lvl}\n\nThe multidimensional array type used by Finch. Tensor is a thin wrapper around the hierarchical level storage of type Lvl.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Base.resize!-Tuple{Tensor, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Base.resize!","text":"resize!(fbr, dims...)\n\nSet the shape of fbr equal to dims. May reuse memory and render the original tensor unusable when modified.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.aggregate_rep-NTuple{4, Any}-reference-listing","page":"Documentation Listing","title":"Finch.aggregate_rep","text":"aggregate_rep(op, init, tns, dims)\n\nReturn a trait object representing the result of reducing a tensor represented by tns on dims by op starting at init.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.aquire_lock!-Tuple{Finch.AbstractDevice, Any}-reference-listing","page":"Documentation Listing","title":"Finch.aquire_lock!","text":"aquire_lock!(dev::AbstractDevice, val)\n\nLock the lock, val, on the device dev, waiting until it can acquire lock.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.assemble_level!-reference-listing","page":"Documentation Listing","title":"Finch.assemble_level!","text":"assemble_level!(ctx, lvl, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.bspread-reference-listing","page":"Documentation Listing","title":"Finch.bspread","text":"bspread(::AbstractString) bspread(::HDF5.File) bspread(::NPYPath)\n\nRead the Binsparse file into a Finch tensor.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\n\n\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.bspwrite-reference-listing","page":"Documentation Listing","title":"Finch.bspwrite","text":"bspwrite(::AbstractString, tns)\nbspwrite(::HDF5.File, tns)\nbspwrite(::NPYPath, tns)\n\nWrite the Finch tensor to a file using Binsparse file format.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.cache_deferred!-Tuple{Any, Finch.FinchLogic.LogicNode}-reference-listing","page":"Documentation Listing","title":"Finch.cache_deferred!","text":"cache_deferred(ctx, root::LogicNode, seen)\n\nReplace deferred expressions with simpler expressions, and cache their evaluation in the preamble.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.choose-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5-Tensor\n└─ SparseList (0.0) [1:5]\n   ├─ [2]: 1.1\n   └─ [4]: 4.4\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<choose(1.1)>>= a[i] end;\n\njulia> x[]\n0.0\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.chunkmask-reference-listing","page":"Documentation Listing","title":"Finch.chunkmask","text":"chunkmask(b)\n\nA mask for a chunked tensor, chunkmask[i, j] = b * (j - 1) < i <= b * j. Note that this specializes each column for the cases where i < b * (j - 1), `b * (j\n\n< i <= b * j, andb * j < i`.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.cld_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.cld_nothrow","text":"cld_nothrow(x, y)\n\nReturns cld(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.collapse_rep-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.collapse_rep","text":"collapse_rep(tns)\n\nNormalize a trait object to collapse subfiber information into the parent tensor.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.collapsed-NTuple{6, Any}-reference-listing","page":"Documentation Listing","title":"Finch.collapsed","text":"collapsed(algebra, f, idx, ext, node)\n\nReturn collapsed expression with respect to f.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.combinedim-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.combinedim","text":"combinedim(ctx, a, b)\n\nCombine the two dimensions a and b.  To avoid ambiguity, only define one of\n\ncombinedim(ctx, ::A, ::B)\ncombinedim(ctx, ::B, ::A)\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.compute-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.compute","text":"compute(args..., ctx=default_scheduler()) -> Any\n\nCompute the value of a lazy tensor. The result is the argument itself, or a tuple of arguments if multiple arguments are passed.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.concordize-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.concordize","text":"concordize(root)\n\nAccepts a program of the following form:\n\n        TABLE := table(IMMEDIATE, FIELD...)\n       ACCESS := reorder(relabel(ALIAS, FIELD...), FIELD...)\n      COMPUTE := ACCESS |\n                 mapjoin(IMMEDIATE, COMPUTE...) |\n                 aggregate(IMMEDIATE, IMMEDIATE, COMPUTE, FIELD...) |\n                 reformat(IMMEDIATE, COMPUTE) |\n                 IMMEDIATE\nCOMPUTE_QUERY := query(ALIAS, COMPUTE)\n  INPUT_QUERY := query(ALIAS, TABLE)\n         STEP := COMPUTE_QUERY | INPUT_QUERY | produces((ALIAS | ACCESS)...)\n         ROOT := PLAN(STEP...)\n\nInserts permutation statements of the form query(ALIAS, reorder(ALIAS, FIELD...)) and updates relabels so that they match their containing reorders. Modified ACCESS statements match the form:\n\nACCESS := reorder(relabel(ALIAS, idxs_1::FIELD...), idxs_2::FIELD...) where issubsequence(idxs_1, idxs_2)\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.concordize-Tuple{Finch.AbstractCompiler, Any}-reference-listing","page":"Documentation Listing","title":"Finch.concordize","text":"concordize(ctx, root)\n\nA raw index is an index expression consisting of a single index node (i.e. A[i] as opposed to A[i + 1]). A Finch program is concordant when all indices are raw and column major with respect to the program loop ordering.  The concordize transformation ensures that tensor indices are concordant by inserting loops and lifting index expressions or transposed indices into the loop bounds.\n\nFor example,\n\n@finch for i = :\n    b[] += A[f(i)]\nend\n\nbecomes\n\n@finch for i = :\n    t = f(i)\n    for s = t:t\n        b[] += A[s]\n    end\nend\n\nand\n\n@finch for i = :, j = :\n    b[] += A[i, j]\nend\n\nbecomes\n\n@finch for i = :, j = :, s = i:i\n    b[] += A[s, j]\nend\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.contain-Tuple{Any, Finch.JuliaContext}-reference-listing","page":"Documentation Listing","title":"Finch.contain","text":"contain(f, ctx)\n\nCall f on a subcontext of ctx and return the result. Variable bindings, preambles, and epilogues defined in the subcontext will not escape the call to contain.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.countstored-Tuple{Tensor}-reference-listing","page":"Documentation Listing","title":"Finch.countstored","text":"countstored(arr)\n\nReturn the number of stored elements in arr. If there are explicitly stored fill elements, they are counted too.\n\nSee also: (SparseArrays.nnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.nnz) and (Base.summarysize)(https://docs.julialang.org/en/v1/base/base/#Base.summarysize)\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.data_rep-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.data_rep","text":"data_rep(tns)\n\nReturn a trait object representing everything that can be learned about the data based on the storage format (type) of the tensor\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.dataflow-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.dataflow","text":"dataflow(ex)\n\nRun dead code elimination and constant propagation. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.declare!-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.declare!","text":"declare!(ctx, tns, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.declare_level!-reference-listing","page":"Documentation Listing","title":"Finch.declare_level!","text":"declare_level!(ctx, lvl, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.defer_tables-Tuple{Any, Finch.FinchLogic.LogicNode}-reference-listing","page":"Documentation Listing","title":"Finch.defer_tables","text":"defer_tables(root::LogicNode)\n\nReplace immediate tensors with deferred expressions assuming the original program structure is given as input to the program.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.dimensionalize!-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.dimensionalize!","text":"dimensionalize!(prgm, ctx)\n\nA program traversal which coordinates dimensions based on shared indices. In particular, loops and declaration statements have dimensions. Accessing a tensor with a raw index hints that the loop should have a dimension corresponding to the tensor axis. Accessing a tensor on the left hand side with a raw index also hints that the tensor declaration should have a dimension corresponding to the loop axis.  All hints inside a loop body are used to evaluate loop dimensions, and all hints after a declaration until the first freeze are used to evaluate declaration dimensions. One may refer to the automatically determined dimension using a variable named _ or :. Index sharing is transitive, so A[i] = B[i] and B[j] = C[j] will induce a gathering of the dimensions of A, B, and C into one.\n\nThe dimensions are semantically evaluated just before the corresponding loop or declaration statement.  The program is assumed to be scoped, so that all loops have unique index names.\n\nSee also: virtual_size, virtual_resize!, combinedim\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.dropfills!-Tuple{Finch.AbstractTensor, Finch.AbstractTensor}-reference-listing","page":"Documentation Listing","title":"Finch.dropfills!","text":"dropfills!(dst, src)\n\nCopy only the non-fill values from src into dst.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.dropfills-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.dropfills","text":"dropfills(src)\n\nDrop the fill values from src and return a new tensor with the same shape and format.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.enforce_lifecycles-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.enforce_lifecycles","text":"enforce_lifecycles(prgm)\n\nA transformation which adds freeze and thaw statements automatically to tensor roots, depending on whether they appear on the left or right hand side.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.enforce_scopes-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.enforce_scopes","text":"enforce_scopes(prgm)\n\nA transformation which gives all loops unique index names and enforces that tensor roots are declared in a containing scope and enforces that variables are declared once within their scope. Note that loop and sieve both introduce new scopes.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.ensure_concurrent-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.ensure_concurrent","text":"ensure_concurrent(root, ctx)\n\nEnsures that all nonlocal assignments to the tensor root are consistently accessed with the same indices and associative operator.  Also ensures that the tensor is either atomic, or accessed by i and concurrent and injective on i.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.evaluate_partial-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.evaluate_partial","text":"evaluate_partial(ctx, root)\n\nThis pass evaluates tags, global variable definitions, and foldable functions into the context bindings.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.exit_on_yieldbind-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.exit_on_yieldbind","text":"exit_on_yieldbind(prgm)\n\nThis pass rewrites the program so that yieldbind expressions are only present at the end of a block. It also adds a yieldbind if not present already.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.expanddims-Tuple{Finch.AbstractTensor, Any}-reference-listing","page":"Documentation Listing","title":"Finch.expanddims","text":"expanddims(arr::AbstractTensor, dims)\n\nExpand the dimensions of an array by inserting a new singleton axis or axes that will appear at the dims position in the expanded array shape.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.expanddims_rep-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.expanddims_rep","text":"expanddims_rep(tns, dims)\n\nExpand the representation of tns by inserting singleton dimensions dims.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.ffindnz-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.ffindnz","text":"ffindnz(arr)\n\nReturn the nonzero elements of arr, as Finch understands arr. Returns (I..., V), where I are the coordinate vectors, one for each mode of arr, and V is a vector of corresponding nonzero values, which can be passed to fsparse.\n\nSee also: (findnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.findnz)\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fiber_ctr-reference-listing","page":"Documentation Listing","title":"Finch.fiber_ctr","text":"fiber_ctr(tns, protos...)\n\nReturn an expression that would construct a tensor suitable to hold data with a representation described by tns. Assumes representation is collapsed.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.fill_value-reference-listing","page":"Documentation Listing","title":"Finch.fill_value","text":"fill_value(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the \"fill\" value becomes the \"background\" value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.filterop-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.filterop","text":"filterop(z)(cond, arg)\n\nfilterop(z) is a function which returns ifelse(cond, arg, z). This operation is handy for filtering out values based on a mask or a predicate. map(filterop(0), cond, arg) is analogous to filter(x -> cond ? x: z, arg).\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5-Tensor\n└─ SparseList (0.0) [1:5]\n   ├─ [2]: 1.1\n   └─ [4]: 4.4\n\njulia> x = Tensor(SparseList(Element(0.0)));\n\njulia> c = Tensor(SparseList(Element(false)), [false, false, false, true, false]);\n\njulia> @finch (x .= 0; for i=_; x[i] = filterop(0)(c[i], a[i]) end)\n(x = Tensor(SparseList{Int64}(Element{0.0, Float64, Int64}([4.4]), 5, [1, 2], [4])),)\n\njulia> x\n5-Tensor\n└─ SparseList (0.0) [1:5]\n   └─ [4]: 4.4\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.finch_kernel-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.finch_kernel","text":"finch_kernel(fname, args, prgm; options...)\n\nReturn a function definition for which can execute a Finch program of type prgm. Here, fname is the name of the function and args is a iterable of argument name => type pairs.\n\nSee also: @finch\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fld1_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.fld1_nothrow","text":"fld1_nothrow(x, y)\n\nReturns fld1(x, y) normally, returns one and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fld_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.fld_nothrow","text":"fld_nothrow(x, y)\n\nReturns fld(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fread-Tuple{AbstractString}-reference-listing","page":"Documentation Listing","title":"Finch.fread","text":"fread(filename::AbstractString)\n\nRead the Finch tensor from a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.freeze!-reference-listing","page":"Documentation Listing","title":"Finch.freeze!","text":"freeze!(ctx, tns)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. This may involve trimming any excess overallocated memory.  Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.freeze_level!-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos)\n\nFreeze all fibers in lvl. Positions 1:pos need freezing.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.freeze_level!-reference-listing","page":"Documentation Listing","title":"Finch.freeze_level!","text":"freeze_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, freeze all fibers within lvl assuming that we have potentially updated 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.freshen-Tuple{Finch.Namespace, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.freshen","text":"freshen(ctx, tags...)\n\nReturn a fresh variable in the current context named after Symbol(tags...)\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fsparse!-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.fsparse!","text":"fsparse!(I..., V,[ M::Tuple])\n\nLike fsparse, but the coordinates must be sorted and unique, and memory is reused.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fsparse-Tuple{AbstractVector, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.fsparse","text":"fsparse(I::Tuple, V,[ M::Tuple, combine]; fill_value=zero(eltype(V)))\n\nCreate a sparse COO tensor S such that size(S) == M and S[(i[q] for i = I)...] = V[q]. The combine function is used to combine duplicates. If M is not specified, it is set to map(maximum, I). If the combine function is not supplied, combine defaults to + unless the elements of V are Booleans in which case combine defaults to |. All elements of I must satisfy 1 <= I[n][q] <= M[n].  Numerical zeros are retained as structural nonzeros; to drop numerical zeros, use dropzeros!.\n\nSee also: sparse\n\nExamples\n\njulia> I = (     [1, 2, 3],     [1, 2, 3],     [1, 2, 3]);\n\njulia> V = [1.0; 2.0; 3.0];\n\njulia> fsparse(I, V) SparseCOO (0.0) [1:3×1:3×1:3] │ │ │ └─└─└─[1, 1, 1] [2, 2, 2] [3, 3, 3]       1.0       2.0       3.0\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fsprand-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.fsprand","text":"fsprand([rng],[type], M..., p, [rfn])\n\nCreate a random sparse tensor of size m in COO format. There are two cases:     - If p is floating point, the probability of any element being nonzero is     independently given by p (and hence the expected density of nonzeros is     also p).     - If p is an integer, exactly p nonzeros are distributed uniformly at     random throughout the tensor (and hence the density of nonzeros is exactly     p / prod(M)). Nonzero values are sampled from the distribution specified by rfn and have the type type. The uniform distribution is used in case rfn is not specified. The optional rng argument specifies a random number generator.\n\nSee also: (sprand)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.sprand)\n\nExamples\n\njulia> fsprand(Bool, 3, 3, 0.5)\nSparseCOO (false) [1:3,1:3]\n├─├─[1, 1]: true\n├─├─[3, 1]: true\n├─├─[2, 2]: true\n├─├─[3, 2]: true\n├─├─[3, 3]: true\n\njulia> fsprand(Float64, 2, 2, 2, 0.5)\nSparseCOO (0.0) [1:2,1:2,1:2]\n├─├─├─[2, 2, 1]: 0.6478553157718558\n├─├─├─[1, 1, 2]: 0.996665291437684\n├─├─├─[2, 1, 2]: 0.7491940599574348\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fspzeros-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.fspzeros","text":"fspzeros([type], M...)\n\nCreate a random zero tensor of size M, with elements of type type. The tensor is in COO format.\n\nSee also: (spzeros)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.spzeros)\n\nExamples\n\njulia> fspzeros(Bool, 3, 3)\n3×3-Tensor\n└─ SparseCOO{2} (false) [:,1:3]\n\njulia> fspzeros(Float64, 2, 2, 2)\n2×2×2-Tensor\n└─ SparseCOO{3} (0.0) [:,:,1:2]\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.ftnsread-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.ftnsread","text":"ftnsread(filename)\n\nRead the contents of the FROSTT .tns file 'filename' into a Finch COO Tensor.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnsread\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.ftnswrite-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.ftnswrite","text":"ftnswrite(filename, tns)\n\nWrite a sparse Finch tensor to a FROSTT .tns file.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnswrite\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fttread-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch tensor. The tensor will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fttwrite-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch tensor to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fused-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.fused","text":"fused(f, args...; kwargs...)\n\nThis function decorator modifies f to fuse the contained array operations and optimize the resulting program. The function must return a single array or tuple of arrays. kwargs are passed to compute\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.fwrite-Tuple{AbstractString, Any}-reference-listing","page":"Documentation Listing","title":"Finch.fwrite","text":"fwrite(filename::AbstractString, tns::Finch.Tensor)\n\nWrite the Finch tensor to a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_algebra-Tuple{Finch.SymbolicContext}-reference-listing","page":"Documentation Listing","title":"Finch.get_algebra","text":"get_algebra(ctx)\n\nget the algebra used in the current context\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_binding!-Tuple{Finch.AbstractCompiler, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_binding!","text":"get_binding!(ctx, var, val)\n\nGet the binding of a variable in the context, or set it to a default value.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_binding-Tuple{Finch.AbstractCompiler, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_binding","text":"get_binding(ctx, var, val)\n\nGet the binding of a variable in the context, or return a default value.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_binding-Tuple{Finch.ScopeContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_binding","text":"get_binding(ctx, var)\n\nGet the binding of a variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_lock-Tuple{Finch.AbstractDevice, Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_lock","text":"get_lock(dev::AbstractDevice, arr, idx, ty)\n\nGiven a device, an array of elements of type ty, and an index to the array, idx, gets a lock of type ty associated to arr[idx] on dev.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_mode_flag-Tuple{Finch.FinchCompiler}-reference-listing","page":"Documentation Listing","title":"Finch.get_mode_flag","text":"get_mode_flag(ctx)\n\nReturn the mode flag given in @finch mode = ?.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_prove_rules-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_prove_rules","text":"get_prove_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_result-Tuple{Finch.FinchCompiler}-reference-listing","page":"Documentation Listing","title":"Finch.get_result","text":"get_result(ctx)\n\nReturn a variable which evaluates to the result of the program which should be returned to the user.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_scheduler-Tuple{}-reference-listing","page":"Documentation Listing","title":"Finch.get_scheduler","text":"get_scheduler()\n\nGet the current Finch scheduler used by compute to execute lazy tensor programs.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_simplify_rules-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_simplify_rules","text":"get_simplify_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_static_hash-Tuple{Finch.SymbolicContext}-reference-listing","page":"Documentation Listing","title":"Finch.get_static_hash","text":"get_static_hash(ctx)\n\nReturn an object which can be called as a hash function. The hashes are the same when objects are hashed in the same order.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_structure-reference-listing","page":"Documentation Listing","title":"Finch.get_structure","text":"get_structure(root::LogicNode)\n\nQuickly produce a normalized structure for a logic program. Note: the result will not be a runnable logic program, but can be hashed and compared for equality. Two programs will have equal structure if their tensors have the same type and their program structure is equivalent up to renaming.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.get_style-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_style","text":"get_style(ctx, root)\n\nreturn the style to use for lowering root in ctx. This method is used to determine which pass should be used to lower a given node. The default implementation returns DefaultStyle(). Overload the three argument form of this method, get_style(ctx, node, root) and specialize on node.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_task-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_task","text":"get_task(ctx)\n\nGet the task which will execute code in this context\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_tensor_mode-Tuple{Finch.ScopeContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_tensor_mode","text":"get_tensor_mode(ctx, var)\n\nGet the mode of a tensor variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.get_wrapper_rules-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.get_wrapper_rules","text":"get_wrapper_rules(ctx, depth, alg)\n\nReturn the wrapperizing rule set for Finch, which converts expressions like `A[i\n\n1]to array combinator expressions likeOffsetArray(A, (1,))`. The rules have\n\naccess to the algebra alg and the depth lookup depthOne can dispatch on thealg` trait to specialize the rule set for different algebras. These rules run after simplification so one can expect constants to be folded.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.getindex_rep-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.getindex_rep","text":"getindex_rep(tns, idxs...)\n\nReturn a trait object representing the result of calling getindex(tns, idxs...) on the tensor represented by tns. Assumes traits are in collapsed form.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.getunbound-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.getunbound","text":"getunbound(stmt)\n\nReturn an iterator over the indices in a Finch program that have yet to be bound.\n\njulia> getunbound(@finch_program for i=_; :a[i, j] += 2 end)\n[j]\njulia> getunbound(@finch_program i + j * 2 * i)\n[i, j]\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.has_binding-Tuple{Finch.ScopeContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.has_binding","text":"has_binding(ctx, var)\n\nCheck if a variable is bound in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.instantiate!-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.instantiate!","text":"instantiate!(ctx, prgm)\n\nA transformation to instantiate readers and updaters before executing an expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.instantiate-Tuple{Any, Any, Any, Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.instantiate","text":"instantiate(ctx, tns, mode, protos)\n\nReturn an object (usually a looplet nest) capable of unfurling the virtual tensor tns. Before executing a statement, each subsequent in-scope access will be initialized with a separate call to instantiate. protos is the list of protocols in each case.\n\nThe fallback for instantiate will iteratively move the last element of protos into the arguments of a function. This allows fibers to specialize on the last arguments of protos rather than the first, as Finch is column major.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.is_atomic-reference-listing","page":"Documentation Listing","title":"Finch.is_atomic","text":"is_atomic(ctx, tns)\n\nReturns a tuple (atomicities, overall) where atomicities is a vector, indicating which indices have an atomic that guards them,\nand overall is a boolean that indicates is the last level had an atomic guarding it.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.is_concurrent-reference-listing","page":"Documentation Listing","title":"Finch.is_concurrent","text":"is_concurrent(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating\nwhether the index can be written to without any execution state. So if a matrix returns [true, false],\nthen we can write to A[i, j] and A[i_2, j] without any shared execution state between the two, but\nwe can't write to A[i, j] and A[i, j_2] without carrying over execution state.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.is_injective-reference-listing","page":"Documentation Listing","title":"Finch.is_injective","text":"is_injective(ctx, tns)\n\nReturns a vector of booleans, one for each dimension of the tensor, indicating whether the access is injective in that dimension.  A dimension is injective if each index in that dimension maps to a different memory space in the underlying array.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.isannihilator-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isassociative-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.iscommutative-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isdistributive-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isdistributive","text":"isdistributive(algebra, f, g)\n\nReturn true when f(a, g(b, c)) = g(f(a, b), f(a, c)) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isidempotent-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isidentity-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isinverse-Tuple{Any, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.isinvolution-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.labelled_children-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.labelled_children","text":"labelled_children(node)\n\nReturn the children of node in a LabelledTree. You may label the children by returning a LabelledTree(key, value), which will be shown as key: value a.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.labelled_show-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.labelled_show","text":"labelled_show(node)\n\nShow the node in a LabelledTree.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.lazy-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.lazy","text":"lazy(arg)\n\nCreate a lazy tensor from an argument. All operations on lazy tensors are lazy, and will not be executed until compute is called on their result.\n\nfor example,\n\nx = lazy(rand(10))\ny = lazy(rand(10))\nz = x + y\nz = z + 1\nz = compute(z)\n\nwill not actually compute z until compute(z) is called, so the execution of x + y is fused with the execution of z + 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.level_axes-reference-listing","page":"Documentation Listing","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.level_eltype-reference-listing","page":"Documentation Listing","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.level_fill_value-reference-listing","page":"Documentation Listing","title":"Finch.level_fill_value","text":"level_fill_value(::Type{Lvl})\n\nThe result of level_fill_value(Lvl) defines fill_value for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.level_ndims-reference-listing","page":"Documentation Listing","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.level_size-reference-listing","page":"Documentation Listing","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.lift_fields-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.lift_fields","text":"This one is a placeholder that places reorder statements inside aggregate and mapjoin query nodes. only works on the output of propagatefields(pushfields(prgm))\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.lift_subqueries-Tuple{Finch.FinchLogic.LogicNode}-reference-listing","page":"Documentation Listing","title":"Finch.lift_subqueries","text":"lift_subqueries\n\nCreates a plan that lifts all subqueries to the top level of the program, with unique queries for each distinct subquery alias. This function processes the rhs of each subquery once, to carefully extract SSA form from any nested pointer structure. After calling lift_subqueries, it is safe to map over the program (recursive pointers to subquery structures will not incur exponential overhead).\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.lower_global-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.lower_global","text":"lower_global(ctx, prgm)\n\nlower the program prgm at global scope in the context ctx.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.make_lock-reference-listing","page":"Documentation Listing","title":"Finch.make_lock","text":"make_lock(ty)\n\nMakes a lock of type ty.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.map_rep-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.map_rep","text":"map_rep(f, args...)\n\nReturn a storage trait object representing the result of mapping f over storage traits args. Assumes representation is collapsed.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.materialize_squeeze_expand_productions-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.materialize_squeeze_expand_productions","text":"materializesqueezeexpand_productions(root)\n\nMakes separate kernels for squeeze and expand operations in produces statements, since swizzle does not support this.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.maxby-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch for i=_; x[] <<maxby>>= a[i] => i end;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.minby-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch for i=_; x[] <<minby>>= a[i] => i end;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.mod1_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.mod1_nothrow","text":"mod1_nothrow(x, y)\n\nReturns mod1(x, y) normally, returns one and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.mod_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.mod_nothrow","text":"mod_nothrow(x, y)\n\nReturns mod(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.moveto-reference-listing","page":"Documentation Listing","title":"Finch.moveto","text":"moveto(arr, device)\n\nIf the array is not on the given device, it creates a new version of this array on that device and copies the data in to it, according to the device trait.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.offset-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.offset","text":"offset(tns, delta...)\n\nCreate an OffsetArray such that offset(tns, delta...)[i...] == tns[i .+ delta...]. The dimensions declared by an OffsetArray are shifted, so that size(offset(tns, delta...)) == size(tns) .+ delta.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.open_scope-Union{Tuple{F}, Tuple{F, Finch.ScopeContext}} where F-reference-listing","page":"Documentation Listing","title":"Finch.open_scope","text":"open_scope(f, ctx)\n\nCall the function f(ctx_2) in a new scope ctx_2.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.pattern!-Tuple{Tensor}-reference-listing","page":"Documentation Listing","title":"Finch.pattern!","text":"pattern!(fbr)\n\nReturn the pattern of fbr. That is, return a tensor which is true wherever fbr is structurally unequal to its fill_value. May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10-Tensor\n└─ SparseList (0.0) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\njulia> pattern!(A)\n10-Tensor\n└─ SparseList (false) [1:10]\n   ├─ [1]: true\n   ├─ [3]: true\n   ├─ ⋮\n   ├─ [7]: true\n   └─ [9]: true\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.permissive-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.permissive","text":"permissive(tns, dims...)\n\nCreate an PermissiveArray where permissive(tns, dims...)[i...] is missing if i[n] is not in the bounds of tns when dims[n] is true.  This wrapper allows all permissive dimensions to be exempt from dimension checks, and is useful when we need to access an array out of bounds, or for padding. More formally,\n\n    permissive(tns, dims...)[i...] =\n        if any(n -> dims[n] && !(i[n] in axes(tns)[n]))\n            missing\n        else\n            tns[i...]\n        end\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.permutedims_rep-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.permutedims_rep","text":"permutedims_rep(tns, perm)\n\nReturn a trait object representing the result of permuting a tensor represented by tns to the permutation perm.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.postype-reference-listing","page":"Documentation Listing","title":"Finch.postype","text":"postype(lvl)\n\nReturn a position type with the same flavor as those used to store the positions of the fibers contained in lvl. The name position descends from the pos or position or pointer arrays found in many definitions of CSR or CSC. In Finch, positions should be data used to access either a subfiber or some other similar auxiliary data. Thus, we often end up iterating over positions.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.pretty-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.pretty","text":"pretty(ex)\n\nMake ex prettier. Shorthand for ex |> unblock |> striplines |> regensym.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.products-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.products","text":"products(tns, dim)\n\nCreate a ProductArray such that\n\n    products(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] * i[dim + 1], i[dim + 2:end]...]\n\nThis is like toeplitz but with times instead of plus.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.propagate_transpose_queries-reference-listing","page":"Documentation Listing","title":"Finch.propagate_transpose_queries","text":"propagatetransposequeries(root)\n\nRemoves non-materializing permutation queries by propagating them to the expressions they contain. Pushes fields and also removes copies. Removes queries of the form:\n\n    query(ALIAS, reorder(relabel(ALIAS, FIELD...), FIELD...))\n\nDoes not remove queries which define production aliases.\n\nAccepts programs of the form:\n\n       TABLE  := table(IMMEDIATE, FIELD...)\n       ACCESS := reorder(relabel(ALIAS, FIELD...), FIELD...)\n    POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE\n    MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)\n  INPUT_QUERY := query(ALIAS, TABLE)\nCOMPUTE_QUERY := query(ALIAS, reformat(IMMEDIATE, MAPREDUCE)) | query(ALIAS, MAPREDUCE))\n         PLAN := plan(STEP...)\n         STEP := COMPUTE_QUERY | INPUT_QUERY | PLAN | produces((ALIAS | ACCESS)...)\n         ROOT := STEP\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.protocolize-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.protocolize","text":"protocolize(tns, protos...)\n\nCreate a ProtocolizedArray that accesses dimension n with protocol protos[n], if protos[n] is not nothing. See the documention for Iteration Protocols for more information. For example, to gallop along the inner dimension of a matrix A, we write A[gallop(i), j], which becomes protocolize(A, gallop, nothing)[i, j].\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.prove-Tuple{Finch.SymbolicContext, Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.prove","text":"prove(ctx, root; verbose = false)\n\nuse the rules in ctx to attempt to prove that the program root is true. Return false if the program cannot be shown to be true.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.push_epilogue!-Tuple{Finch.JuliaContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.push_epilogue!","text":"push_epilogue!(ctx, thunk)\n\nPush the thunk onto the epilogue in the currently executing context. The epilogue will be evaluated after the code returned by the given function in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.push_fields-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.push_fields","text":"push_fields(node)\n\nThis program modifies all EXPR statements in the program, as defined in the following grammar:\n\n    LEAF := relabel(ALIAS, FIELD...) |\n            table(IMMEDIATE, FIELD...) |\n            IMMEDIATE\n    EXPR := LEAF |\n            reorder(EXPR, FIELD...) |\n            relabel(EXPR, FIELD...) |\n            mapjoin(IMMEDIATE, EXPR...)\n\nPushes all reorder and relabel statements down to LEAF nodes of each EXPR. Output LEAF nodes will match the form reorder(relabel(LEAF, FIELD...), FIELD...), omitting reorder or relabel if not present as an ancestor of the LEAF in the original EXPR. Tables and immediates will absorb relabels.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.push_preamble!-Tuple{Finch.JuliaContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.push_preamble!","text":"push_preamble!(ctx, thunk)\n\nPush the thunk onto the preamble in the currently executing context. The preamble will be evaluated before the code returned by the given function in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.reassemble_level!-reference-listing","page":"Documentation Listing","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_fill_value(lvl).  Not avaliable on all level types as this presumes updating.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.refresh-Tuple{}-reference-listing","page":"Documentation Listing","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.regensym-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.regensym","text":"regensym(ex)\n\nGive gensyms prettier names by renumbering them. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.release_lock!-Tuple{Finch.AbstractDevice, Any}-reference-listing","page":"Documentation Listing","title":"Finch.release_lock!","text":"release_lock!(dev::AbstractDevice, val)\n\nRelease the lock, val, on the device dev.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.rem_nothrow-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.rem_nothrow","text":"rem_nothrow(x, y)\n\nReturns rem(x, y) normally, returns zero and issues a warning if y is zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.rep_construct-reference-listing","page":"Documentation Listing","title":"Finch.rep_construct","text":"rep_construct(tns, protos...)\n\nConstruct a tensor suitable to hold data with a representation described by tns. Assumes representation is collapsed.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.return_type-Tuple{Any, Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.return_type","text":"return_type(algebra, f, arg_types...)\n\nGive the return type of f when applied to arguments of types arg_types... in algebra. Used to determine output types of functions in the high-level interface. This function falls back to Base.promote_op.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.scale-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.scale","text":"scale(tns, delta...)\n\nCreate a ScaleArray such that scale(tns, delta...)[i...] == tns[i .* delta...].  The dimensions declared by an OffsetArray are shifted, so that size(scale(tns, delta...)) == size(tns) .* delta.  This is only supported on tensors with real-valued dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.scansearch-Union{Tuple{T2}, Tuple{T1}, Tuple{Any, Any, T1, T2}} where {T1<:Integer, T2<:Integer}-reference-listing","page":"Documentation Listing","title":"Finch.scansearch","text":"scansearch(v, x, lo, hi)\n\nreturn the first value of v greater than or equal to x, within the range lo:hi. Return hi+1 if all values are less than x. This implemantation uses an exponential search strategy which involves two steps: 1) searching for binary search bounds via exponential steps rightward 2) binary searching within those bounds.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_binding!-Tuple{Finch.ScopeContext, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_binding!","text":"set_binding!(ctx, var, val)\n\nSet the binding of a variable in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_declared!-Tuple{Finch.ScopeContext, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_declared!","text":"set_declared!(ctx, var, val)\n\nMark a tensor variable as declared in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_fill_value!-Tuple{Tensor, Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_fill_value!","text":"set_fill_value!(fbr, init)\n\nReturn a tensor which is equal to fbr, but with the fill (implicit) value set to init.  May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10-Tensor\n└─ SparseList (0.0) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\njulia> set_fill_value!(A, Inf)\n10-Tensor\n└─ SparseList (Inf) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_frozen!-Tuple{Finch.ScopeContext, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_frozen!","text":"set_frozen!(ctx, var, val)\n\nMark a tensor variable as frozen in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_loop_order-reference-listing","page":"Documentation Listing","title":"Finch.set_loop_order","text":"setlooporder(root)\n\nHeuristically chooses a total order for all loops in the program by inserting reorder statments inside reformat, query, and aggregate nodes.\n\nAccepts programs of the form:\n\n      REORDER := reorder(relabel(ALIAS, FIELD...), FIELD...)\n       ACCESS := reorder(relabel(ALIAS, idxs_1::FIELD...), idxs_2::FIELD...) where issubsequence(idxs_1, idxs_2)\n    POINTWISE := ACCESS | mapjoin(IMMEDIATE, POINTWISE...) | reorder(IMMEDIATE, FIELD...) | IMMEDIATE\n    MAPREDUCE := POINTWISE | aggregate(IMMEDIATE, IMMEDIATE, POINTWISE, FIELD...)\n       TABLE  := table(IMMEDIATE, FIELD...)\nCOMPUTE_QUERY := query(ALIAS, reformat(IMMEDIATE, arg::(REORDER | MAPREDUCE)))\n  INPUT_QUERY := query(ALIAS, TABLE)\n         STEP := COMPUTE_QUERY | INPUT_QUERY\n         ROOT := PLAN(STEP..., produces(ALIAS...))\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.set_scheduler!-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_scheduler!","text":"set_scheduler!(scheduler)\n\nSet the current scheduler to scheduler. The scheduler is used by compute to execute lazy tensor programs.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.set_thawed!-Tuple{Finch.ScopeContext, Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.set_thawed!","text":"set_thawed!(ctx, var, val)\n\nMark a tensor variable as thawed in the context.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.simplify-Tuple{Finch.SymbolicContext, Any}-reference-listing","page":"Documentation Listing","title":"Finch.simplify","text":"simplify(ctx, node)\n\nsimplify the program node using the rules in ctx\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.striplines-Tuple{Expr}-reference-listing","page":"Documentation Listing","title":"Finch.striplines","text":"striplines(ex)\n\nRemove line numbers. ex is the target Julia expression\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.swizzle-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.swizzle","text":"swizzle(tns, dims)\n\nCreate a SwizzleArray to transpose any tensor tns such that\n\n    swizzle(tns, dims)[i...] == tns[i[dims]]\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.thaw!-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.thaw!","text":"thaw!(ctx, tns)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.thaw_level!-reference-listing","page":"Documentation Listing","title":"Finch.thaw_level!","text":"thaw_level!(ctx, lvl, pos, init)\n\nGiven the last reference position, pos, thaw all fibers within lvl assuming that we have previously assembled and frozen 1:pos.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.toeplitz-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.toeplitz","text":"toeplitz(tns, dim)\n\nCreate a ToeplitzArray such that\n\n    Toeplitz(tns, dim)[i...] == tns[i[1:dim-1]..., i[dim] + i[dim + 1], i[dim + 2:end]...]\n\nThe ToplitzArray can be thought of as adding a dimension that shifts another dimension of the original tensor.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.unblock-Tuple{Expr}-reference-listing","page":"Documentation Listing","title":"Finch.unblock","text":"unblock(ex)\n\nFlatten any redundant blocks into a single block, over the whole expression. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.unfurl-Tuple{Any, Finch.Furlable, Any, Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.unfurl","text":"unfurl(ctx, tns, ext, protos...)\n\nReturn an array object (usually a looplet nest) for lowering the virtual tensor tns. ext is the extent of the looplet. protos is the list of protocols that should be used for each index, but one doesn't need to unfurl all the indices at once.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.unquote_literals-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.unquote_literals","text":"unquote_literals(ex)\n\nunquote QuoteNodes when this doesn't change the semantic meaning. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.unresolve-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.unresolve","text":"unresolve(ex)\n\nUnresolve function literals into function symbols. ex is the target Julia expression.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.virtual_call-Tuple{Any, Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.virtual_call","text":"virtual_call(ctx, f, a...)\n\nGiven the virtual arguments a..., and a literal function f, return a virtual object representing the result of the function call. If the function is not foldable, return nothing. This function is used so that we can call e.g. tensor constructors in finch code.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.virtual_eltype-reference-listing","page":"Documentation Listing","title":"Finch.virtual_eltype","text":"virtual_eltype(arr)\n\nReturn the element type of the virtual tensor arr.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.virtual_fill_value-reference-listing","page":"Documentation Listing","title":"Finch.virtual_fill_value","text":"virtual fill_value(arr)\n\nReturn the initializer for virtual array arr.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.virtual_moveto-reference-listing","page":"Documentation Listing","title":"Finch.virtual_moveto","text":"virtual_moveto(device, arr)\n\nIf the virtual array is not on the given device, copy the array to that device. This function may modify underlying data arrays, but cannot change the virtual itself. This function is used to move data to the device before a kernel is launched.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.virtual_resize!-reference-listing","page":"Documentation Listing","title":"Finch.virtual_resize!","text":"virtual_resize!(ctx, tns, dims...)\n\nResize tns in the context ctx. This is a function similar in spirit to Base.resize!.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.virtual_size-reference-listing","page":"Documentation Listing","title":"Finch.virtual_size","text":"virtual_size(ctx, tns)\n\nReturn a tuple of the dimensions of tns in the context ctx. This is a function similar in spirit to Base.axes.\n\n\n\n\n\n","category":"function"},{"location":"reference/listing/#Finch.virtualize-NTuple{4, Any}-reference-listing","page":"Documentation Listing","title":"Finch.virtualize","text":"virtualize(ctx, ex, T, [tag])\n\nReturn the virtual program corresponding to the Julia expression ex of type T in the JuliaContext ctx. Implementaters may support the optional tag argument is used to name the resulting virtual variable.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.window-Tuple{Any, Vararg{Any}}-reference-listing","page":"Documentation Listing","title":"Finch.window","text":"window(tns, dims)\n\nCreate a WindowedArray which represents a view into another tensor\n\n    window(tns, dims)[i...] == tns[dim[1][i], dim[2][i], ...]\n\nThe windowed array restricts the new dimension to the dimension of valid indices of each dim. The dims may also be nothing to represent a full view of the underlying dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.with_scheduler-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.with_scheduler","text":"with_scheduler(f, scheduler)\n\nExecute f with the current scheduler set to scheduler.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.wrapperize-Tuple{Finch.AbstractCompiler, Any}-reference-listing","page":"Documentation Listing","title":"Finch.wrapperize","text":"wrapperize(ctx, root)\n\nConvert index expressions in the program root to wrapper arrays, according to the rules in get_wrapper_rules. By default, the following transformations are performed:\n\nA[i - j] => A[i + (-j)]\nA[3 * i] => ScaleArray(A, (3,))[i]\nA[i * j] => ProductArray(A, 1)[i, j]\nA[i + 1] => OffsetArray(A, (1,))[i]\nA[i + j] => ToeplitzArray(A, 1)[i, j]\nA[~i] => PermissiveArray(A, 1)[i]\n\nThe loop binding order may be used to determine which index comes first in an expression like A[i + j]. Thus, for i=:,j=:; ... A[i + j] will result in ToeplitzArray(A, 1)[j, i], but for j=:,i=:; ... A[i + j] results in ToeplitzArray(A, 1)[i, j]. wrapperize runs before dimensionalization, so resulting raw indices may participate in dimensionalization according to the semantics of the wrapper.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.@barrier-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.@barrier","text":"@barrier args... ex\n\nWrap ex in a let block that captures all free variables in ex that are bound in the arguments. This is useful for ensuring that the variables in ex are not mutated by the arguments.\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@closure-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.@closure","text":"@closure closure_expression\n\nWrap the closure definition closure_expression in a let block to encourage the julia compiler to generate improved type information.  For example:\n\ncallfunc(f) = f()\n\nfunction foo(n)\n   for i=1:n\n       if i >= n\n           # Unlikely event - should be fast.  However, capture of `i` inside\n           # the closure confuses the julia-0.6 compiler and causes it to box\n           # the variable `i`, leading to a 100x performance hit if you remove\n           # the `@closure`.\n           callfunc(@closure ()->println(\"Hello $i\"))\n       end\n   end\nend\n\nThere's nothing nice about this - it's a heuristic workaround for some inefficiencies in the type information inferred by the julia 0.6 compiler. However, it can result in large speedups in many cases, without the need to restructure the code to avoid the closure.\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@einsum-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.@einsum","text":"@einsum tns[idxs...] <<op>>= ex...\n\nConstruct an einsum expression that computes the result of applying op to the tensor tns with the indices idxs and the tensors in the expression ex. The result is stored in the variable tns.\n\nex may be any pointwise expression consisting of function calls and tensor references of the form tns[idxs...], where tns and idxs are symbols.\n\nThe <<op>> operator can be any binary operator that is defined on the element type of the expression ex.\n\nThe einsum will evaluate the pointwise expression tns[idxs...] <<op>>= ex... over all combinations of index values in tns and the tensors in ex.\n\nHere are a few examples:\n\n@einsum C[i, j] += A[i, k] * B[k, j]\n@einsum C[i, j, k] += A[i, j] * B[j, k]\n@einsum D[i, k] += X[i, j] * Y[j, k]\n@einsum J[i, j] = H[i, j] * I[i, j]\n@einsum N[i, j] = K[i, k] * L[k, j] - M[i, j]\n@einsum R[i, j] <<max>>= P[i, k] + Q[k, j]\n@einsum x[i] = A[i, j] * x[j]\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@finch-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.@finch","text":"@finch [options...] prgm\n\nRun a finch program prgm. The syntax for a finch program is a set of nested loops, statements, and branches over pointwise array assignments. For example, the following program computes the sum of two arrays A = B + C:\n\n@finch begin\n    A .= 0\n    for i = _\n        A[i] = B[i] + C[i]\n    end\n    return A\nend\n\nFinch programs are composed using the following syntax:\n\narr .= 0: an array declaration initializing arr to zero.\narr[inds...]: an array access, the array must be a variable and each index may be another finch expression.\nx + y, f(x, y): function calls, where x and y are finch expressions.\narr[inds...] = ex: an array assignment expression, setting arr[inds] to the value of ex.\narr[inds...] += ex: an incrementing array expression, adding ex to arr[inds]. *, &, |, are supported.\narr[inds...] <<min>>= ex: a incrementing array expression with a custom operator, e.g. <<min>> is the minimum operator.\nfor i = _ body end: a loop over the index i, where _ is computed from array access with i in body.\nif cond body end: a conditional branch that executes only iterations where cond is true.\nreturn (tnss...,): at global scope, exit the program and return the tensors tnss with their new dimensions. By default, any tensor declared in global scope is returned.\n\nSymbols are used to represent variables, and their values are taken from the environment. Loops introduce index variables into the scope of their bodies.\n\nFinch uses the types of the arrays and symbolic analysis to discover program optimizations. If B and C are sparse array types, the program will only run over the nonzeros of either.\n\nSemantically, Finch programs execute every iteration. However, Finch can use sparsity information to reliably skip iterations when possible.\n\noptions are optional keyword arguments:\n\nalgebra: the algebra to use for the program. The default is DefaultAlgebra().\nmode: the optimization mode to use for the program. Possible modes are:\n:debug: run the program in debug mode, with bounds checking and better error handling.\n:safe: run the program in safe mode, with modest checks for performance and correctness.\n:fast: run the program in fast mode, with no checks or warnings, this mode is for power users.\nThe default is :safe.\n\nSee also: @finch_code\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@finch_code-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.@finch_code","text":"@finch_code [options...] prgm\n\nReturn the code that would be executed in order to run a finch program prgm.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@finch_kernel-Tuple-reference-listing","page":"Documentation Listing","title":"Finch.@finch_kernel","text":"@finch_kernel [options...] fname(args...) = prgm\n\nReturn a definition for a function named fname which executes @finch prgm on the arguments args. args should be a list of variables holding representative argument instances or types.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.@staged-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.@staged","text":"Finch.@staged\n\nThis function is used internally in Finch in lieu of @generated functions. It ensures the first Finch invocation runs in the latest world, and leaves hooks so that subsequent calls to Finch.refresh can update the world and invalidate old versions. If the body contains closures, this macro uses an eval and invokelatest strategy. Otherwise, it uses a generated function. This macro does not support type parameters, varargs, or keyword arguments.\n\n\n\n\n\n","category":"macro"},{"location":"reference/listing/#Finch.FinchNotation.access-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.assign-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). Overwriting is accomplished with the function overwrite(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.block-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.block","text":"block(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn. If the body is not a block, replaces accesses to tensors in the body with instantiate.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.cached-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.call-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.declare-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.declare","text":"declare(tns, init)\n\nFinch AST statement that declares tns with an initial value init in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.define-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.define","text":"define(lhs, rhs, body)\n\nFinch AST statement that defines lhs as having the value rhs in body. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.freeze-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.freeze","text":"freeze(tns)\n\nFinch AST statement that freezes tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.index-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.literal-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.loop-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.sieve-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true. A new scope is introduced to evaluate body.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.tag-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.tag","text":"tag(var, bind)\n\nFinch AST expression for a global variable var with the value bind. Because the finch compiler cannot pass variable state from the program domain to the type domain directly, the tag type represents a value bind referred to by a variable named bind. All tag in the same program must agree on the value of variables, and only one value will be virtualized.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.thaw-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.thaw","text":"thaw(tns)\n\nFinch AST statement that thaws tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.value-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.variable-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.virtual-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.virtual","text":"virtual(val)\n\nFinch AST expression for an object val which has special meaning to the compiler. This type is typically used for tensors, as it allows users to specify the tensor's shape and data type.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.yieldbind-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.yieldbind","text":"yieldbind(args...)\n\nFinch AST statement that sets the result of the program to the values of variables args.... Subsequent statements will not affect the result of the program.\n\n\n\n\n\n","category":"constant"},{"location":"reference/listing/#Finch.FinchNotation.Dimensionless-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.Dimensionless","text":"Dimensionless()\n\nA singleton type representing the lack of a dimension.  This is used in place of a dimension when we want to avoid dimensionality checks. In the @finch macro, you can write Dimensionless() with an underscore as for i = _, allowing finch to pick up the loop bounds from the tensors automatically.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.FinchNotation.FinchNode-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node, used to represent an imperative, physical Finch program.\n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"reference/listing/#Finch.FinchNotation.extrude-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.extrude","text":"extrude(i)\n\nThe extrude protocol declares that the tensor update happens in order and only once, so that reduction loops occur below the extrude loop. It is not usually necessary to declare an extrude protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.finch_leaf-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.follow-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.follow","text":"follow(i)\n\nThe follow protocol ignores the structure of the tensor. By itself, the follow protocol iterates over each value of the tensor in order, looking it up with random access.  The follow protocol may specialize on e.g. the zero value of the tensor, but does not specialize on the structure of the tensor. This enables efficient random access and avoids large code sizes.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.gallop-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.gallop","text":"gallop(i)\n\nThe gallop protocol iterates over each pattern element of a tensor, leading the iteration and superceding the priority of other tensors. Mutual leading is possible, where we fast-forward to the largest step between either leader.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.initwrite-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.initwrite","text":"initwrite(z)(a, b)\n\ninitwrite(z) is a function which may assert that a isequal to z, and returnsb.  By default,lhs[] = rhsis equivalent tolhs[] <<initwrite(fill_value(lhs))>>= rhs`.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isconstant-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isconstant","text":"isconstant(node)\n\nReturns true if the node can be expected to be constant within the current finch context\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isindex-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isindex","text":"isindex(node)\n\nReturns true if the node is a finch index\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isliteral-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isliteral","text":"isliteral(node)\n\nReturns true if the node is a finch literal\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isstateful-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isstateful","text":"isstateful(node)\n\nReturns true if the node is a finch statement, and false if the node is an index expression. Typically, statements specify control flow and expressions describe values.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isvalue-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvalue","text":"isvalue(node)\n\nReturns true if the node is a finch value\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isvariable-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvariable","text":"isvariable(node)\n\nReturns true if the node is a finch variable\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.isvirtual-Tuple{Finch.FinchNotation.FinchNode}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.isvirtual","text":"isvirtual(node)\n\nReturns true if the node is a finch virtual\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.laminate-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.laminate","text":"laminate(i)\n\nThe laminate protocol declares that the tensor update may happen out of order and multiple times. It is not usually necessary to declare a laminate protocol, but it is used internally to reason about tensor format requirements.\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.overwrite-Tuple{Any, Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.overwrite","text":"overwrite(z)(a, b)\n\noverwrite(z) is a function which returns b always. lhs[] := rhs is equivalent to lhs[] <<overwrite>>= rhs.\n\njulia> a = Tensor(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\n5-Tensor\n└─ SparseList (0.0) [1:5]\n   ├─ [2]: 1.1\n   └─ [4]: 4.4\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<overwrite>>= a[i] end;\n\njulia> x[]\n0.0\n\n\n\n\n\n","category":"method"},{"location":"reference/listing/#Finch.FinchNotation.walk-Tuple{Any}-reference-listing","page":"Documentation Listing","title":"Finch.FinchNotation.walk","text":"walk(i)\n\nThe walk protocol usually iterates over each pattern element of a tensor in order. Note that the walk protocol \"imposes\" the structure of its argument on the kernel, so that we specialize the kernel to the structure of the tensor.\n\n\n\n\n\n","category":"method"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"CurrentModule = Finch","category":"page"},{"location":"guides/calling_finch/#Usage","page":"Calling Finch","title":"Usage","text":"","category":"section"},{"location":"guides/calling_finch/#General-Purpose-(@finch)","page":"Calling Finch","title":"General Purpose (@finch)","text":"","category":"section"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Most users will want to use the @finch macro, which executes the given program immediately in the given scope. The program will be JIT-compiled on the first call to @finch with the given array argument types. If the array arguments to @finch are type stable, the program will be JIT-compiled when the surrounding function is compiled.","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"Very often, the best way to inspect Finch compiler behavior is through the @finch_code macro, which prints the generated code instead of executing it.","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"@finch\n@finch_code","category":"page"},{"location":"guides/calling_finch/#Finch.@finch","page":"Calling Finch","title":"Finch.@finch","text":"@finch [options...] prgm\n\nRun a finch program prgm. The syntax for a finch program is a set of nested loops, statements, and branches over pointwise array assignments. For example, the following program computes the sum of two arrays A = B + C:\n\n@finch begin\n    A .= 0\n    for i = _\n        A[i] = B[i] + C[i]\n    end\n    return A\nend\n\nFinch programs are composed using the following syntax:\n\narr .= 0: an array declaration initializing arr to zero.\narr[inds...]: an array access, the array must be a variable and each index may be another finch expression.\nx + y, f(x, y): function calls, where x and y are finch expressions.\narr[inds...] = ex: an array assignment expression, setting arr[inds] to the value of ex.\narr[inds...] += ex: an incrementing array expression, adding ex to arr[inds]. *, &, |, are supported.\narr[inds...] <<min>>= ex: a incrementing array expression with a custom operator, e.g. <<min>> is the minimum operator.\nfor i = _ body end: a loop over the index i, where _ is computed from array access with i in body.\nif cond body end: a conditional branch that executes only iterations where cond is true.\nreturn (tnss...,): at global scope, exit the program and return the tensors tnss with their new dimensions. By default, any tensor declared in global scope is returned.\n\nSymbols are used to represent variables, and their values are taken from the environment. Loops introduce index variables into the scope of their bodies.\n\nFinch uses the types of the arrays and symbolic analysis to discover program optimizations. If B and C are sparse array types, the program will only run over the nonzeros of either.\n\nSemantically, Finch programs execute every iteration. However, Finch can use sparsity information to reliably skip iterations when possible.\n\noptions are optional keyword arguments:\n\nalgebra: the algebra to use for the program. The default is DefaultAlgebra().\nmode: the optimization mode to use for the program. Possible modes are:\n:debug: run the program in debug mode, with bounds checking and better error handling.\n:safe: run the program in safe mode, with modest checks for performance and correctness.\n:fast: run the program in fast mode, with no checks or warnings, this mode is for power users.\nThe default is :safe.\n\nSee also: @finch_code\n\n\n\n\n\n","category":"macro"},{"location":"guides/calling_finch/#Finch.@finch_code","page":"Calling Finch","title":"Finch.@finch_code","text":"@finch_code [options...] prgm\n\nReturn the code that would be executed in order to run a finch program prgm.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"guides/calling_finch/#Ahead-Of-Time-(@finch_kernel)","page":"Calling Finch","title":"Ahead Of Time (@finch_kernel)","text":"","category":"section"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"While @finch is the recommended way to use Finch, it is also possible to run finch ahead-of-time. The @finch_kernel macro generates a function definition ahead-of-time, which can be evaluated and then called later.","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"There are several reasons one might want to do this:","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"If we want to make tweaks to the Finch implementation, we can directly modify the source code of the resulting function.\nWhen benchmarking Finch functions, we can easily and reliably ensure the benchmarked code is inferrable.\nIf we want to use Finch to generate code but don't want to include Finch as a dependency in our project, we can use @finch_kernel to generate the functions ahead of time and copy and paste the generated code into our project.  Consider automating this workflow to keep the kernels up to date!","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"    @finch_kernel","category":"page"},{"location":"guides/calling_finch/#Finch.@finch_kernel","page":"Calling Finch","title":"Finch.@finch_kernel","text":"@finch_kernel [options...] fname(args...) = prgm\n\nReturn a definition for a function named fname which executes @finch prgm on the arguments args. args should be a list of variables holding representative argument instances or types.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"As an example, the following code generates an spmv kernel definition, evaluates the definition, and then calls the kernel several times.","category":"page"},{"location":"guides/calling_finch/","page":"Calling Finch","title":"Calling Finch","text":"let\n    A = Tensor(Dense(SparseList(Element(0.0))))\n    x = Tensor(Dense(Element(0.0)))\n    y = Tensor(Dense(Element(0.0)))\n    def = @finch_kernel function spmv(y, A, x)\n        y .= 0.0\n        for j = _, i = _\n            y[i] += A[i, j] * x[j]\n        end\n        return y\n    end\n    eval(def)\nend\n\nfunction main()\n    for i = 1:10\n        A2 = Tensor(Dense(SparseList(Element(0.0))), fsprand(10, 10, 0.1))\n        x2 = Tensor(Dense(Element(0.0)), rand(10))\n        y2 = Tensor(Dense(Element(0.0)))\n        spmv(y2, A2, x2)\n    end\nend\n\nmain()","category":"page"},{"location":"guides/sparse_utils/#Sparse-Array-Utilities","page":"Sparse and Structured Utilities","title":"Sparse Array Utilities","text":"","category":"section"},{"location":"guides/sparse_utils/#Sparse-Constructors","page":"Sparse and Structured Utilities","title":"Sparse Constructors","text":"","category":"section"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"In addition to the Tensor constructor, Finch provides a number of convenience constructors for common tensor types. For example, the spzeros and sprand functions have fspzeros and fsprand counterparts that return Finch tensors. We can also construct a sparse COO Tensor from a list of indices and values using the fsparse function.","category":"page"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"fsparse\nfsparse!\nfsprand\nfspzeros\nffindnz","category":"page"},{"location":"guides/sparse_utils/#Finch.fsparse","page":"Sparse and Structured Utilities","title":"Finch.fsparse","text":"fsparse(I::Tuple, V,[ M::Tuple, combine]; fill_value=zero(eltype(V)))\n\nCreate a sparse COO tensor S such that size(S) == M and S[(i[q] for i = I)...] = V[q]. The combine function is used to combine duplicates. If M is not specified, it is set to map(maximum, I). If the combine function is not supplied, combine defaults to + unless the elements of V are Booleans in which case combine defaults to |. All elements of I must satisfy 1 <= I[n][q] <= M[n].  Numerical zeros are retained as structural nonzeros; to drop numerical zeros, use dropzeros!.\n\nSee also: sparse\n\nExamples\n\njulia> I = (     [1, 2, 3],     [1, 2, 3],     [1, 2, 3]);\n\njulia> V = [1.0; 2.0; 3.0];\n\njulia> fsparse(I, V) SparseCOO (0.0) [1:3×1:3×1:3] │ │ │ └─└─└─[1, 1, 1] [2, 2, 2] [3, 3, 3]       1.0       2.0       3.0\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.fsparse!","page":"Sparse and Structured Utilities","title":"Finch.fsparse!","text":"fsparse!(I..., V,[ M::Tuple])\n\nLike fsparse, but the coordinates must be sorted and unique, and memory is reused.\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.fsprand","page":"Sparse and Structured Utilities","title":"Finch.fsprand","text":"fsprand([rng],[type], M..., p, [rfn])\n\nCreate a random sparse tensor of size m in COO format. There are two cases:     - If p is floating point, the probability of any element being nonzero is     independently given by p (and hence the expected density of nonzeros is     also p).     - If p is an integer, exactly p nonzeros are distributed uniformly at     random throughout the tensor (and hence the density of nonzeros is exactly     p / prod(M)). Nonzero values are sampled from the distribution specified by rfn and have the type type. The uniform distribution is used in case rfn is not specified. The optional rng argument specifies a random number generator.\n\nSee also: (sprand)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.sprand)\n\nExamples\n\njulia> fsprand(Bool, 3, 3, 0.5)\nSparseCOO (false) [1:3,1:3]\n├─├─[1, 1]: true\n├─├─[3, 1]: true\n├─├─[2, 2]: true\n├─├─[3, 2]: true\n├─├─[3, 3]: true\n\njulia> fsprand(Float64, 2, 2, 2, 0.5)\nSparseCOO (0.0) [1:2,1:2,1:2]\n├─├─├─[2, 2, 1]: 0.6478553157718558\n├─├─├─[1, 1, 2]: 0.996665291437684\n├─├─├─[2, 1, 2]: 0.7491940599574348\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.fspzeros","page":"Sparse and Structured Utilities","title":"Finch.fspzeros","text":"fspzeros([type], M...)\n\nCreate a random zero tensor of size M, with elements of type type. The tensor is in COO format.\n\nSee also: (spzeros)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.spzeros)\n\nExamples\n\njulia> fspzeros(Bool, 3, 3)\n3×3-Tensor\n└─ SparseCOO{2} (false) [:,1:3]\n\njulia> fspzeros(Float64, 2, 2, 2)\n2×2×2-Tensor\n└─ SparseCOO{3} (0.0) [:,:,1:2]\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.ffindnz","page":"Sparse and Structured Utilities","title":"Finch.ffindnz","text":"ffindnz(arr)\n\nReturn the nonzero elements of arr, as Finch understands arr. Returns (I..., V), where I are the coordinate vectors, one for each mode of arr, and V is a vector of corresponding nonzero values, which can be passed to fsparse.\n\nSee also: (findnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.findnz)\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Fill-Values","page":"Sparse and Structured Utilities","title":"Fill Values","text":"","category":"section"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"Finch tensors support an arbitrary \"background\" value for sparse arrays. While most arrays use 0 as the background value, this is not always the case. For example, a sparse array of Int might use typemin(Int) as the background value. The default function returns the background value of a tensor. If you ever want to change the background value of an existing array, you can use the set_fill_value! function. The countstored function returns the number of stored elements in a tensor, and calling pattern! on a tensor returns tensor which is true whereever the original tensor stores a value. Note that countstored doesn't always return the number of non-zero elements in a tensor, as it counts the number of stored elements, and stored elements may include the background value. You can call dropfills! to remove explicitly stored background values from a tensor.","category":"page"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])\n3×6-Tensor\n└─ SparseCOO{2} (0.0) [:,1:6]\n   ├─ [1, 2]: 1.0\n   ├─ [1, 4]: 2.0\n   └─ [2, 5]: 3.0\n\njulia> min.(A, -1)\n3×6-Tensor\n└─ Dense [:,1:6]\n   ├─ [:, 1]: Dense [1:3]\n   │  ├─ [1]: -1.0\n   │  ├─ [2]: -1.0\n   │  └─ [3]: -1.0\n   ├─ [:, 2]: Dense [1:3]\n   │  ├─ [1]: -1.0\n   │  ├─ [2]: -1.0\n   │  └─ [3]: -1.0\n   ├─ ⋮\n   ├─ [:, 5]: Dense [1:3]\n   │  ├─ [1]: -1.0\n   │  ├─ [2]: -1.0\n   │  └─ [3]: -1.0\n   └─ [:, 6]: Dense [1:3]\n      ├─ [1]: -1.0\n      ├─ [2]: -1.0\n      └─ [3]: -1.0\n\njulia> fill_value(A)\n0.0\n\njulia> B = set_fill_value!(A, -Inf)\n3×6-Tensor\n└─ SparseCOO{2} (-Inf) [:,1:6]\n   ├─ [1, 2]: 1.0\n   ├─ [1, 4]: 2.0\n   └─ [2, 5]: 3.0\n\njulia> min.(B, -1)\n3×6-Tensor\n└─ SparseDict (-Inf) [:,1:6]\n   ├─ [:, 2]: SparseDict (-Inf) [1:3]\n   │  └─ [1]: -1.0\n   ├─ [:, 4]: SparseDict (-Inf) [1:3]\n   │  └─ [1]: -1.0\n   └─ [:, 5]: SparseDict (-Inf) [1:3]\n      └─ [2]: -1.0\n\njulia> countstored(A)\n3\n\njulia> pattern!(A)\n3×6-Tensor\n└─ SparseCOO{2} (false) [:,1:6]\n   ├─ [1, 2]: true\n   ├─ [1, 4]: true\n   └─ [2, 5]: true\n","category":"page"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"set_fill_value!\npattern!\ncountstored\ndropfills\ndropfills!","category":"page"},{"location":"guides/sparse_utils/#Finch.set_fill_value!","page":"Sparse and Structured Utilities","title":"Finch.set_fill_value!","text":"set_fill_value!(fbr, init)\n\nReturn a tensor which is equal to fbr, but with the fill (implicit) value set to init.  May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10-Tensor\n└─ SparseList (0.0) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\njulia> set_fill_value!(A, Inf)\n10-Tensor\n└─ SparseList (Inf) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.pattern!","page":"Sparse and Structured Utilities","title":"Finch.pattern!","text":"pattern!(fbr)\n\nReturn the pattern of fbr. That is, return a tensor which is true wherever fbr is structurally unequal to its fill_value. May reuse memory and render the original tensor unusable when modified.\n\njulia> A = Tensor(SparseList(Element(0.0), 10), [2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 0.0, 6.0, 0.0])\n10-Tensor\n└─ SparseList (0.0) [1:10]\n   ├─ [1]: 2.0\n   ├─ [3]: 3.0\n   ├─ ⋮\n   ├─ [7]: 5.0\n   └─ [9]: 6.0\n\njulia> pattern!(A)\n10-Tensor\n└─ SparseList (false) [1:10]\n   ├─ [1]: true\n   ├─ [3]: true\n   ├─ ⋮\n   ├─ [7]: true\n   └─ [9]: true\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.countstored","page":"Sparse and Structured Utilities","title":"Finch.countstored","text":"countstored(arr)\n\nReturn the number of stored elements in arr. If there are explicitly stored fill elements, they are counted too.\n\nSee also: (SparseArrays.nnz)(https://docs.julialang.org/en/v1/stdlib/SparseArrays/#SparseArrays.nnz) and (Base.summarysize)(https://docs.julialang.org/en/v1/base/base/#Base.summarysize)\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.dropfills","page":"Sparse and Structured Utilities","title":"Finch.dropfills","text":"dropfills(src)\n\nDrop the fill values from src and return a new tensor with the same shape and format.\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#Finch.dropfills!","page":"Sparse and Structured Utilities","title":"Finch.dropfills!","text":"dropfills!(dst, src)\n\nCopy only the non-fill values from src into dst.\n\n\n\n\n\n","category":"function"},{"location":"guides/sparse_utils/#How-to-tell-whether-an-entry-is-\"fill\"","page":"Sparse and Structured Utilities","title":"How to tell whether an entry is \"fill\"","text":"","category":"section"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"In the sparse world, a semantic distinction is sometimes made between \"explicitly stored\" values and \"implicit\" or \"fill\" values (usually zero). However, the formats in the Finch compiler represent a diverse set of structures beyond sparsity, and it is often unclear whether any of the values in the tensor are \"explicit\" (consider a mask matrix, which can be represented with a constant number of bits). Thus, Finch makes no semantic distinction between values which are stored explicitly or not. If users wish to make this distinction, they should instead store a tensor of tuples of the form (value, is_fill). For example,","category":"page"},{"location":"guides/sparse_utils/","page":"Sparse and Structured Utilities","title":"Sparse and Structured Utilities","text":"julia> A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [(1.0, false), (0.0, true), (3.0, false)]; fill_value=(0.0, true))\n3×6-Tensor\n└─ SparseCOO{2} ((0.0, true)) [:,1:6]\n   ├─ [1, 2]: (1.0, false)\n   ├─ [1, 4]: (0.0, true)\n   └─ [2, 5]: (3.0, false)\n\njulia> B = Tensor(Dense(SparseList(Element((0.0, true)))), A)\n3×6-Tensor\n└─ Dense [:,1:6]\n   ├─ [:, 1]: SparseList ((0.0, true)) [1:3]\n   ├─ [:, 2]: SparseList ((0.0, true)) [1:3]\n   │  └─ [1]: (1.0, false)\n   ├─ ⋮\n   ├─ [:, 5]: SparseList ((0.0, true)) [1:3]\n   │  └─ [2]: (3.0, false)\n   └─ [:, 6]: SparseList ((0.0, true)) [1:3]\n\njulia> sum(map(last, B))\n16\n\njulia> sum(map(first, B))\n4.0\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Finch","category":"page"},{"location":"#Finch","page":"Home","title":"Finch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch is an adaptable compiler for loop nests over sparse or otherwise structured arrays. Finch supports general sparsity as well as many specialized sparsity patterns, like clustered nonzeros, diagonals, or triangles.  In addition to zero, Finch supports optimizations over arbitrary fill values and operators, even run-length-compression.","category":"page"},{"location":"","page":"Home","title":"Home","text":"At it's heart, Finch is powered by a domain specific language for coiteration, breaking structured iterators into units we call Looplets. The Looplets are lowered progressively, leaving several opportunities to rewrite and simplify intermediate expressions.","category":"page"},{"location":"#Installation:","page":"Home","title":"Installation:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"Finch\")","category":"page"},{"location":"#Basics:","page":"Home","title":"Basics:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To begin, the following program sums the rows of a sparse matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Finch\nA = sprand(5, 5, 0.5)\ny = zeros(5)\n@finch begin\n    y .= 0\n    for i=_, j=_\n        y[i] += A[i, j]\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The @finch macro takes a block of code, and compiles it using the sparsity attributes of the arguments. In this case, A is a sparse matrix, so the compiler generates a sparse loop nest. The compiler takes care of applying rules like x * 0 => 0 during compilation to make the code more efficient.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can call @finch on any loop program, but it will only generate sparse code if the arguments are sparse. For example, the following program calculates the sum of the elements of a dense matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Finch\nA = rand(5, 5)\ns = Scalar(0.0)\n@finch begin\n    s .= 0\n    for i=_, j=_\n        s[] += A[i, j]\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can call @finch_code to see the generated code (since A is dense, the code is dense):","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> @finch_code for i=_, j=_ ; s[] += A[i, j] end\nquote\n    s = (ex.bodies[1]).body.body.lhs.tns.bind\n    s_val = s.val\n    A = (ex.bodies[1]).body.body.rhs.tns.bind\n    sugar_1 = size((ex.bodies[1]).body.body.rhs.tns.bind)\n    A_mode1_stop = sugar_1[1]\n    A_mode2_stop = sugar_1[2]\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_mode1_stop\n        for j_3 = 1:A_mode2_stop\n            val = A[i_3, j_3]\n            s_val = val + s_val\n        end\n    end\n    result = ()\n    s.val = s_val\n    result\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"We're working on adding more documentation, for now take a look at the examples!","category":"page"}]
}
