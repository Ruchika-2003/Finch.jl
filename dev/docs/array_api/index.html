<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>High-Level Array API · Finch.jl</title><meta name="title" content="High-Level Array API · Finch.jl"/><meta property="og:title" content="High-Level Array API · Finch.jl"/><meta property="twitter:title" content="High-Level Array API · Finch.jl"/><meta name="description" content="Documentation for Finch.jl."/><meta property="og:description" content="Documentation for Finch.jl."/><meta property="twitter:description" content="Documentation for Finch.jl."/><meta property="og:url" content="https://finch-tensor.github.io/Finch.jl/docs/array_api/"/><meta property="twitter:url" content="https://finch-tensor.github.io/Finch.jl/docs/array_api/"/><link rel="canonical" href="https://finch-tensor.github.io/Finch.jl/docs/array_api/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Finch.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Finch.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><span class="tocitem">Documentation</span><ul><li><a class="tocitem" href="../tensor_formats/">Tensor Formats</a></li><li class="is-active"><a class="tocitem" href>High-Level Array API</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Einsum"><span>Einsum</span></a></li><li class="toplevel"><a class="tocitem" href="#Array-Fusion"><span>Array Fusion</span></a></li><li><a class="tocitem" href="#Optimizers"><span>Optimizers</span></a></li><li><a class="tocitem" href="#Fusable-Functions"><span>Fusable Functions</span></a></li></ul></li><li><a class="tocitem" href="../sparse_utils/">Sparse and Structured Utilities</a></li><li><a class="tocitem" href="../user-defined_functions/">User-Defined Functions</a></li><li><a class="tocitem" href="../fileio/">FileIO</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox"/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Advanced: Finch Language</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../language/calling_finch/">Calling Finch</a></li><li><a class="tocitem" href="../language/finch_language/">The Finch Language</a></li><li><a class="tocitem" href="../language/dimensionalization/">Dimensionalization</a></li><li><a class="tocitem" href="../language/index_sugar/">Index Sugar</a></li><li><a class="tocitem" href="../language/mask_sugar/">Mask Sugar</a></li><li><a class="tocitem" href="../language/iteration_protocols/">Iteration Protocols</a></li><li><a class="tocitem" href="../language/parallelization/">Parallelization</a></li><li><a class="tocitem" href="../language/interoperability/">Interoperability</a></li><li><a class="tocitem" href="../language/optimization_tips/">Optimization Tips</a></li><li><a class="tocitem" href="../language/benchmarking_tips/">Benchmarking Tips</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-7" type="checkbox"/><label class="tocitem" for="menuitem-3-7"><span class="docs-label">Developers: Internal Details</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../internals/virtualization/">Virtualization</a></li><li><a class="tocitem" href="../internals/tensor_interface/">Tensor Interface</a></li><li><a class="tocitem" href="../internals/compiler_interface/">Compiler Interfaces</a></li><li><a class="tocitem" href="../internals/finch_notation/">Finch Notation</a></li><li><a class="tocitem" href="../internals/finch_logic/">Finch Logic</a></li></ul></li></ul></li><li><a class="tocitem" href="../../CONTRIBUTING/">Community and Contributions</a></li><li><span class="tocitem">Appendices and Additional Resources</span><ul><li><a class="tocitem" href="../../appendices/directory_structure/">Directory Structure</a></li><li><a class="tocitem" href="../../appendices/directory_structure/">Code Listing</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Documentation</a></li><li class="is-active"><a href>High-Level Array API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>High-Level Array API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/finch-tensor/Finch.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/finch-tensor/Finch.jl/blob/main/docs/src/docs/array_api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="High-Level-Array-API"><a class="docs-heading-anchor" href="#High-Level-Array-API">High-Level Array API</a><a id="High-Level-Array-API-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-Array-API" title="Permalink"></a></h1><p>Finch tensors also support many of the basic array operations one might expect, including indexing, slicing, and elementwise maps, broadcast, and reduce. For example:</p><pre><code class="language-julia-repl hljs">julia&gt; A = fsparse([1, 1, 2, 3], [2, 4, 5, 6], [1.0, 2.0, 3.0])
3×6 Tensor{SparseCOOLevel{2, Tuple{Int64, Int64}, Vector{Int64}, Tuple{Vector{Int64}, Vector{Int64}}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}:
 0.0  1.0  0.0  2.0  0.0  0.0
 0.0  0.0  0.0  0.0  3.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0

julia&gt; A + 0
3×6 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  1.0  0.0  2.0  0.0  0.0
 0.0  0.0  0.0  0.0  3.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0

julia&gt; A + 1
3×6 Tensor{DenseLevel{Int64, DenseLevel{Int64, ElementLevel{1.0, Float64, Int64, Vector{Float64}}}}}:
 1.0  2.0  1.0  3.0  1.0  1.0
 1.0  1.0  1.0  1.0  4.0  1.0
 1.0  1.0  1.0  1.0  1.0  1.0

julia&gt; B = A .* 2
3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  2.0  0.0  4.0  0.0  0.0
 0.0  0.0  0.0  0.0  6.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0

julia&gt; B[1:2, 1:2]
2×2 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  2.0
 0.0  0.0

julia&gt; map(x -&gt; x^2, B)
3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  4.0  0.0  16.0   0.0  0.0
 0.0  0.0  0.0   0.0  36.0  0.0
 0.0  0.0  0.0   0.0   0.0  0.0</code></pre><h1 id="Einsum"><a class="docs-heading-anchor" href="#Einsum">Einsum</a><a id="Einsum-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum" title="Permalink"></a></h1><p>Finch also supports a highly general <code>@einsum</code> macro which supports any reduction over any simple pointwise array expression.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.@einsum" href="#Finch.@einsum"><code>Finch.@einsum</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@einsum tns[idxs...] &lt;&lt;op&gt;&gt;= ex...</code></pre><p>Construct an einsum expression that computes the result of applying <code>op</code> to the tensor <code>tns</code> with the indices <code>idxs</code> and the tensors in the expression <code>ex</code>. The result is stored in the variable <code>tns</code>.</p><p><code>ex</code> may be any pointwise expression consisting of function calls and tensor references of the form <code>tns[idxs...]</code>, where <code>tns</code> and <code>idxs</code> are symbols.</p><p>The <code>&lt;&lt;op&gt;&gt;</code> operator can be any binary operator that is defined on the element type of the expression <code>ex</code>.</p><p>The einsum will evaluate the pointwise expression <code>tns[idxs...] &lt;&lt;op&gt;&gt;= ex...</code> over all combinations of index values in <code>tns</code> and the tensors in <code>ex</code>.</p><p>Here are a few examples:</p><pre><code class="nohighlight hljs">@einsum C[i, j] += A[i, k] * B[k, j]
@einsum C[i, j, k] += A[i, j] * B[j, k]
@einsum D[i, k] += X[i, j] * Y[j, k]
@einsum J[i, j] = H[i, j] * I[i, j]
@einsum N[i, j] = K[i, k] * L[k, j] - M[i, j]
@einsum R[i, j] &lt;&lt;max&gt;&gt;= P[i, k] + Q[k, j]
@einsum x[i] = A[i, j] * x[j]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/einsum.jl#L163-L189">source</a></section></article><h1 id="Array-Fusion"><a class="docs-heading-anchor" href="#Array-Fusion">Array Fusion</a><a id="Array-Fusion-1"></a><a class="docs-heading-anchor-permalink" href="#Array-Fusion" title="Permalink"></a></h1><p>Finch supports array fusion, which allows you to compose multiple array operations into a single kernel. This can be a significant performance optimization, as it allows the compiler to optimize the entire operation at once. The two functions the user needs to know about are <code>lazy</code> and <code>compute</code>. You can use <code>lazy</code> to mark an array as an input to a fused operation, and call <code>compute</code> to execute the entire operation at once. For example:</p><pre><code class="language-julia-repl hljs">julia&gt; C = lazy(A);

julia&gt; D = lazy(B);

julia&gt; E = (C .+ D) / 2;

julia&gt; compute(E)
3×6 Tensor{SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, SparseDictLevel{Int64, Vector{Int64}, Vector{Int64}, Vector{Int64}, Dict{Tuple{Int64, Int64}, Int64}, Vector{Int64}, ElementLevel{0.0, Float64, Int64, Vector{Float64}}}}}:
 0.0  1.5  0.0  3.0  0.0  0.0
 0.0  0.0  0.0  0.0  4.5  0.0
 0.0  0.0  0.0  0.0  0.0  0.0</code></pre><p>In the above example, <code>E</code> is a fused operation that adds <code>C</code> and <code>D</code> together and then divides the result by 2. The <code>compute</code> function examines the entire operation and decides how to execute it in the most efficient way possible. In this case, it would likely generate a single kernel that adds the elements of <code>A</code> and <code>B</code> together and divides each result by 2, without materializing an intermediate.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.lazy" href="#Finch.lazy"><code>Finch.lazy</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">lazy(arg)</code></pre><p>Create a lazy tensor from an argument. All operations on lazy tensors are lazy, and will not be executed until <code>compute</code> is called on their result.</p><p>for example,</p><pre><code class="language-julia hljs">x = lazy(rand(10))
y = lazy(rand(10))
z = x + y
z = z + 1
z = compute(z)</code></pre><p>will not actually compute <code>z</code> until <code>compute(z)</code> is called, so the execution of <code>x + y</code> is fused with the execution of <code>z + 1</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L623-L639">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.compute" href="#Finch.compute"><code>Finch.compute</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">compute(args...; ctx=default_scheduler(), kwargs...) -&gt; Any</code></pre><p>Compute the value of a lazy tensor. The result is the argument itself, or a tuple of arguments if multiple arguments are passed. Some keyword arguments can be passed to control the execution of the program:     - <code>verbose=false</code>: Print the generated code before execution     - <code>tag=:global</code>: A tag to distinguish between different classes of inputs for the same program.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L698-L706">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.fused" href="#Finch.fused"><code>Finch.fused</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fused(f, args...; kwargs...)</code></pre><p>This function decorator modifies <code>f</code> to fuse the contained array operations and optimize the resulting program. The function must return a single array or tuple of arrays.  Some keyword arguments can be passed to control the execution of the program:     - <code>verbose=false</code>: Print the generated code before execution     - <code>tag=:global</code>: A tag to distinguish between different classes of inputs for the same program.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L652-L661">source</a></section></article><p>The <code>lazy</code> and <code>compute</code> functions allow the compiler to fuse operations together, resulting in asymptotically more efficient code.</p><pre><code class="language-julia hljs">julia&gt; using BenchmarkTools

julia&gt; A = fsprand(1000, 1000, 100);
       B = Tensor(rand(1000, 1000));
       C = Tensor(rand(1000, 1000));

julia&gt; @btime A .* (B * C);
  145.940 ms (859 allocations: 7.69 MiB)

julia&gt; @btime compute(lazy(A) .* (lazy(B) * lazy(C)));
  694.666 μs (712 allocations: 60.86 KiB)</code></pre><h2 id="Optimizers"><a class="docs-heading-anchor" href="#Optimizers">Optimizers</a><a id="Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizers" title="Permalink"></a></h2><p>Different optimizers can be used with <code>compute</code>, such as the state-of-the-art Galley optimizer, which can adapt to the sparsity patterns of the inputs. The optimizer can be set as an argument <code>ctx</code> to the <code>compute</code> function, or using <code>set_scheduler</code> or <code>with_scheduler</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.set_scheduler!" href="#Finch.set_scheduler!"><code>Finch.set_scheduler!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">set_scheduler!(scheduler)</code></pre><p>Set the current scheduler to <code>scheduler</code>. The scheduler is used by <code>compute</code> to execute lazy tensor programs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L668-L673">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.with_scheduler" href="#Finch.with_scheduler"><code>Finch.with_scheduler</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">with_scheduler(f, scheduler)</code></pre><p>Execute <code>f</code> with the current scheduler set to <code>scheduler</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L683-L687">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.default_scheduler" href="#Finch.default_scheduler"><code>Finch.default_scheduler</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">default_scheduler(;verbose=false)</code></pre><p>The default scheduler used by <code>compute</code> to execute lazy tensor programs. Fuses all pointwise expresions into reductions. Only fuses reductions into pointwise expressions when they are the only usage of the reduction.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/interface/lazy.jl#L642-L648">source</a></section></article><h3 id="The-Galley-Optimizer"><a class="docs-heading-anchor" href="#The-Galley-Optimizer">The Galley Optimizer</a><a id="The-Galley-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-Galley-Optimizer" title="Permalink"></a></h3><p>Galley is a cost-based optimizer for Finch&#39;s lazy evaluation interface based on techniques from database query optimization. To use Galley, you just add the parameter <code>ctx=galley_optimizer()</code> to the <code>compute</code> function. While the default optimizer (<code>ctx=default_scheduler()</code>) makes decisions entirely based on the types of the inputs, Galley gathers statistics on their sparsity to make cost-based based optimization decisions.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Finch.Galley.galley_scheduler" href="#Finch.Galley.galley_scheduler"><code>Finch.Galley.galley_scheduler</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">galley_scheduler(verbose = false, estimator=DCStats)</code></pre><p>The galley scheduler uses the sparsity patterns of the inputs to optimize the computation. The first set of inputs given to galley is used to optimize, and the <code>estimator</code> is used to estimate the sparsity of intermediate computations during optimization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/finch-tensor/Finch.jl/blob/c067a29a3f76631ae1d7277fcad82f7c31bbc1a0/src/Galley/FinchCompat/executor.jl#L172-L178">source</a></section></article><pre><code class="language-julia hljs">julia&gt; A = fsprand(1000, 1000, 0.1);
       B = fsprand(1000, 1000, 0.1);
       C = fsprand(1000, 1000, 0.0001);

julia&gt; A = lazy(A);
       B = lazy(B);
       C = lazy(C);

julia&gt; @btime compute(sum(A * B * C));
  282.503 ms (1018 allocations: 184.43 MiB)

julia&gt; @btime compute(sum(A * B * C), ctx=galley_scheduler());
  152.792 μs (672 allocations: 28.81 KiB)</code></pre><p>By taking advantage of the fact that C is highly sparse, Galley can better structure the computation. In the matrix chain multiplication, it always starts with the C,B matmul before multiplying with A. In the summation, it takes advantage of distributivity to pushing the reduction down to the inputs. It first sums over A and C, then multiplies those vectors with B.</p><p>Because Galley adapts to the sparsity patterns of the first input tensor, it can be useful to distinguish between different uses of the same function using the <code>tag</code> keyword argument to <code>compute</code> or <code>fuse</code>.  For example, we may wish to distinguish one spmv from another, as follows:</p><pre><code class="language-julia-repl hljs">julia&gt; A = rand(1000, 1000);
       B = rand(1000, 1000);
       C = fsprand(1000, 1000, 0.0001);

julia&gt; fused((A, B, C) -&gt; C .* (A * B), A, B, C; tag=:very_sparse_sddmm);

julia&gt; C = fsprand(1000, 1000, 0.9);

julia&gt; fused((A, B, C) -&gt; C .* (A * B), A, B, C; tag=:very_dense_sddmm);
</code></pre><p>By distinguishing between the two uses of the same function, Galley can make better decisions about how to optimize each computation separately.</p><h2 id="Fusable-Functions"><a class="docs-heading-anchor" href="#Fusable-Functions">Fusable Functions</a><a id="Fusable-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Fusable-Functions" title="Permalink"></a></h2><p>The following functions can be fused:</p><p><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>broadcast</code>, any of the <code>Base.Broadcast</code> operators (e.g. <code>.+</code>, <code>.&amp;</code>, <code>.min</code>,), <code>reduce</code>, <code>map</code>, <code>sum</code>, <code>prod</code>, <code>any</code>, <code>all</code>, <code>mapreduce</code>, <code>maximum</code>, <code>minimum</code>, <code>extrema</code>, <code>Statistics.mean</code>, <code>Statistics.std</code>, <code>Statistics.var</code>, <code>Statistics.stdm</code>, <code>Statistics.varm</code>, <code>LinearAlgebra.norm</code>, <code>argmax</code>, <code>argmin</code>, <code>transpose</code>, <code>permutedims</code>, <code>Finch.tensordot</code>, <code>Finch.@einsum</code>, <code>dropdims</code>, <code>Finch.expanddims</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tensor_formats/">« Tensor Formats</a><a class="docs-footer-nextpage" href="../sparse_utils/">Sparse and Structured Utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.10.1 on <span class="colophon-date" title="Friday 18 April 2025 14:32">Friday 18 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
